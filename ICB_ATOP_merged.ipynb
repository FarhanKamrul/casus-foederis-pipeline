{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e135f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: c:\\Users\\Farhan\\Desktop\\PPTP\\casus-foederis-pipeline\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"Current working dir:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9aa3fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atop_5.1__.csv_\\\\ATOP 5.1 (.csv)\\\\atop5_1a.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob, pprint\n",
    "pprint.pp(glob.glob(\"**/atop5_1a.csv\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5037cc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATOP file exists: True\n",
      "(789, 134) (1131, 95)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "root = Path.cwd()                          # c:\\Users\\Farhan\\Desktop\\PPTP\\casus-foederis-pipeline\n",
    "atop_path = root / \"atop_5.1__.csv_\" / \"ATOP 5.1 (.csv)\" / \"atop5_1a.csv\"\n",
    "icb_path  = root / \"icb2v16.csv\"\n",
    "\n",
    "print(\"ATOP file exists:\", atop_path.exists())   # should print True\n",
    "\n",
    "atop_df = pd.read_csv(atop_path)\n",
    "icb_df  = pd.read_csv(icb_path)\n",
    "\n",
    "print(atop_df.shape, icb_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941ef3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== cracid values with >1 actor label ===\n",
      "345: YUG, SER\n",
      "\n",
      "=== cracid values with >1 actloc label ===\n",
      "630: 13.0, 15.0\n",
      "\n",
      "Saved 147 actor mappings and 147 actloc mappings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_values(series):\n",
    "    \"\"\"Remove NaN/None values and convert to strings\"\"\"\n",
    "    cleaned = []\n",
    "    for val in series.dropna().unique():\n",
    "        if pd.notna(val) and val is not None:\n",
    "            cleaned.append(str(val))\n",
    "    return cleaned\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Build cracid → [actor1, actor2, …]  (allow many)\n",
    "# ------------------------------------------------------------------\n",
    "cracid_actor_series = (\n",
    "    icb_df.groupby(\"cracid\")[\"actor\"]\n",
    "          .apply(clean_values)\n",
    ")\n",
    "\n",
    "# Warn about multiple labels\n",
    "multi_actor = cracid_actor_series[cracid_actor_series.str.len() > 1]\n",
    "print(\"=== cracid values with >1 actor label ===\")\n",
    "if multi_actor.empty:\n",
    "    print(\"None\")\n",
    "else:\n",
    "    for cid, labels in multi_actor.items():\n",
    "        print(f\"{cid}: {', '.join(labels)}\")\n",
    "\n",
    "# Save to CSV\n",
    "(cracid_actor_series\n",
    "     .apply(\"; \".join)\n",
    "     .to_csv(\"cracid_to_actor.csv\", header=[\"actor_labels\"])\n",
    ")\n",
    "\n",
    "# Dict for later use\n",
    "cracid_actor = cracid_actor_series.to_dict()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Build cracid → [actloc1, actloc2, …]  (allow many)\n",
    "# ------------------------------------------------------------------\n",
    "cracid_actloc_series = (\n",
    "    icb_df.groupby(\"cracid\")[\"actloc\"]\n",
    "          .apply(clean_values)\n",
    ")\n",
    "\n",
    "# Warn about multiple locations\n",
    "multi_loc = cracid_actloc_series[cracid_actloc_series.str.len() > 1]\n",
    "print(\"\\n=== cracid values with >1 actloc label ===\")\n",
    "if multi_loc.empty:\n",
    "    print(\"None\")\n",
    "else:\n",
    "    for cid, locs in multi_loc.items():\n",
    "        print(f\"{cid}: {', '.join(locs)}\")\n",
    "\n",
    "# Save to CSV\n",
    "(cracid_actloc_series\n",
    "     .apply(\"; \".join)\n",
    "     .to_csv(\"cracid_to_actloc.csv\", header=[\"actloc_labels\"])\n",
    ")\n",
    "\n",
    "# Dict for later use\n",
    "cracid_actloc = cracid_actloc_series.to_dict()\n",
    "\n",
    "print(f\"\\nSaved {len(cracid_actor)} actor mappings and {len(cracid_actloc)} actloc mappings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52dcd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Investigating cracid 630 (Iran) conflicts ===\n",
      "Total rows for Iran: 20\n",
      "Unique actloc values: [13.0, 15.0]\n",
      "Unique actor values: ['IRN']\n",
      "\n",
      "=== Iran conflict details ===\n",
      " crisno actor  actloc                        crisname  yrtrig  motrig  triggr  geog  period  syslev  outcom\n",
      "     14   IRN    13.0                  PERSIAN BORDER  1920.0     5.0     9.0  13.0     1.0     1.0     1.0\n",
      "     87   IRN    13.0              OCCUPATION OF IRAN  1941.0     8.0     9.0  13.0     2.0     2.0     2.0\n",
      "     96   IRN    13.0            IRAN-OIL CONCESSIONS  1944.0     9.0     3.0  13.0     2.0     1.0     1.0\n",
      "    108   IRN    13.0                      AZERBAIJAN  1945.0     8.0     6.0  10.0     3.0     2.0     1.0\n",
      "    172   IRN    13.0                 SHATT-AL-ARAB I  1959.0    12.0     2.0  15.0     3.0     1.0     3.0\n",
      "    234   IRN    13.0                SHATT-AL-ARAB II  1969.0     4.0     2.0  15.0     4.0     1.0     1.0\n",
      "    309   IRN    13.0             US HOSTAGES IN IRAN  1980.0     4.0     9.0  13.0     4.0     1.0     2.0\n",
      "    317   IRN    13.0             ONSET IRAN/IRAQ WAR  1980.0     9.0     2.0  15.0     4.0     1.0     3.0\n",
      "    348   IRN    13.0              BASRA-KHARG ISLAND  1984.0     3.0     9.0  15.0     4.0     1.0     3.0\n",
      "    361   IRN    13.0               CAPTURE OF AL-FAW  1986.0     2.0     9.0  15.0     4.0     1.0     1.0\n",
      "    379   IRN    13.0                MECCA PILGRIMAGE  1987.0     7.0     9.0  15.0     4.0     1.0     4.0\n",
      "    385   IRN    13.0           IRAQ RECAPTURE-AL-FAW  1988.0     4.0     9.0  13.0     4.0     1.0     4.0\n",
      "    442   IRN    13.0                  IRAN NUCLEAR I  2003.0     6.0     4.0  13.0     5.0     2.0     2.0\n",
      "    448   IRN    13.0                 IRAN NUCLEAR II  2006.0     1.0     1.0  13.0     5.0     2.0     2.0\n",
      "    488   IRN    13.0    ISRAEL-IRAN CLASHES IN SYRIA  2018.0     2.0     9.0  15.0     6.0     1.0     3.0\n",
      "    492   IRN    15.0 ISRAEL-IRAN CLASHES IN SYRIA II  2019.0     1.0     9.0  15.0     6.0     1.0     3.0\n",
      "    495   IRN    15.0         SOLEIMANI ASSASSINATION  2019.0     6.0     7.0  15.0     6.0     1.0     3.0\n",
      "    496   IRN    15.0               ABQAIQ OIL STRIKE  2019.0     9.0     9.0  15.0     6.0     1.0     2.0\n",
      "    504   IRN    15.0       FAKHRIZADEH ASSASSINATION  2020.0    11.0     9.0  15.0     6.0     1.0     3.0\n",
      "    508   IRN    15.0 ISRAEL-IRAN CLASHES IN SYRIA II  2021.0     1.0     9.0  15.0     6.0     1.0     3.0\n",
      "\n",
      "=== Breakdown by actloc ===\n",
      "\n",
      "Actloc 13.0:\n",
      "  Crisis 14: PERSIAN BORDER (1920)\n",
      "  Crisis 87: OCCUPATION OF IRAN (1941)\n",
      "  Crisis 96: IRAN-OIL CONCESSIONS (1944)\n",
      "  Crisis 108: AZERBAIJAN (1945)\n",
      "  Crisis 172: SHATT-AL-ARAB I (1959)\n",
      "  Crisis 234: SHATT-AL-ARAB II (1969)\n",
      "  Crisis 309: US HOSTAGES IN IRAN (1980)\n",
      "  Crisis 317: ONSET IRAN/IRAQ WAR (1980)\n",
      "  Crisis 348: BASRA-KHARG ISLAND (1984)\n",
      "  Crisis 361: CAPTURE OF AL-FAW (1986)\n",
      "  Crisis 379: MECCA PILGRIMAGE (1987)\n",
      "  Crisis 385: IRAQ RECAPTURE-AL-FAW (1988)\n",
      "  Crisis 442: IRAN NUCLEAR I (2003)\n",
      "  Crisis 448: IRAN NUCLEAR II (2006)\n",
      "  Crisis 488: ISRAEL-IRAN CLASHES IN SYRIA (2018)\n",
      "\n",
      "Actloc 15.0:\n",
      "  Crisis 492: ISRAEL-IRAN CLASHES IN SYRIA II (2019)\n",
      "  Crisis 495: SOLEIMANI ASSASSINATION (2019)\n",
      "  Crisis 496: ABQAIQ OIL STRIKE (2019)\n",
      "  Crisis 504: FAKHRIZADEH ASSASSINATION (2020)\n",
      "  Crisis 508: ISRAEL-IRAN CLASHES IN SYRIA II (2021)\n",
      "\n",
      "=== Multi-actor analysis ===\n",
      "No crises with multiple actors found.\n",
      "\n",
      "=== Summary ===\n",
      "Iran appears in 20 unique crises\n",
      "Regional classifications: {13.0: 15, 15.0: 5}\n",
      "\n",
      "=== Region code meanings (if available) ===\n",
      "Actloc 13.0 (appears 15 times): likely = Middle East\n",
      "Actloc 15.0 (appears 5 times): likely = South Asia\n",
      "This suggests Iran was involved in conflicts that spanned different regional theaters\n"
     ]
    }
   ],
   "source": [
    "# Investigate cracid 630 (Iran) - why multiple actloc values?\n",
    "print(\"=== Investigating cracid 630 (Iran) conflicts ===\")\n",
    "\n",
    "# Filter for Iran's conflicts\n",
    "iran_conflicts = icb_df[icb_df['cracid'] == 630].copy()\n",
    "\n",
    "print(f\"Total rows for Iran: {len(iran_conflicts)}\")\n",
    "print(f\"Unique actloc values: {sorted([x for x in iran_conflicts['actloc'].unique() if pd.notna(x)])}\")\n",
    "print(f\"Unique actor values: {sorted([x for x in iran_conflicts['actor'].unique() if pd.notna(x)])}\")\n",
    "\n",
    "# Show all relevant columns for Iran conflicts\n",
    "relevant_cols = ['crisno', 'actor', 'actloc', 'crisname', 'yrtrig', 'motrig', 'triggr']\n",
    "# Add any other columns that might be relevant\n",
    "additional_cols = []\n",
    "for col in ['geog', 'period', 'syslev', 'outcom']:\n",
    "    if col in icb_df.columns:\n",
    "        additional_cols.append(col)\n",
    "\n",
    "display_cols = [col for col in relevant_cols + additional_cols if col in icb_df.columns]\n",
    "\n",
    "print(f\"\\n=== Iran conflict details ===\")\n",
    "print(iran_conflicts[display_cols].to_string(index=False))\n",
    "\n",
    "# Group by actloc to see which conflicts fall into each region\n",
    "print(f\"\\n=== Breakdown by actloc ===\")\n",
    "for actloc_val in iran_conflicts['actloc'].dropna().unique():\n",
    "    print(f\"\\nActloc {actloc_val}:\")\n",
    "    subset = iran_conflicts[iran_conflicts['actloc'] == actloc_val]\n",
    "    if 'crisname' in subset.columns:\n",
    "        crisis_info = subset[['crisno', 'crisname', 'yrtrig']].drop_duplicates()\n",
    "        for _, row in crisis_info.iterrows():\n",
    "            year_str = f\"({int(row['yrtrig'])})\" if pd.notna(row['yrtrig']) else \"(year unknown)\"\n",
    "            print(f\"  Crisis {row['crisno']}: {row['crisname']} {year_str}\")\n",
    "    else:\n",
    "        crisis_info = subset[['crisno', 'yrtrig']].drop_duplicates()\n",
    "        for _, row in crisis_info.iterrows():\n",
    "            year_str = f\"({int(row['yrtrig'])})\" if pd.notna(row['yrtrig']) else \"(year unknown)\"\n",
    "            print(f\"  Crisis {row['crisno']} {year_str}\")\n",
    "\n",
    "# Check if there are multiple actors involved in same crisis\n",
    "print(f\"\\n=== Multi-actor analysis ===\")\n",
    "crisis_actors = iran_conflicts.groupby('crisno')['actor'].nunique()\n",
    "multi_actor_crises = crisis_actors[crisis_actors > 1]\n",
    "if len(multi_actor_crises) > 0:\n",
    "    print(\"Crises with multiple actors:\")\n",
    "    for crisno in multi_actor_crises.index:\n",
    "        crisis_data = iran_conflicts[iran_conflicts['crisno'] == crisno]\n",
    "        actors = crisis_data['actor'].unique()\n",
    "        actlocs = crisis_data['actloc'].unique()\n",
    "        print(f\"  Crisis {crisno}: actors {actors}, actlocs {actlocs}\")\n",
    "else:\n",
    "    print(\"No crises with multiple actors found.\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Iran appears in {iran_conflicts['crisno'].nunique()} unique crises\")\n",
    "actloc_counts = iran_conflicts['actloc'].value_counts().dropna()\n",
    "print(f\"Regional classifications: {dict(actloc_counts)}\")\n",
    "\n",
    "# If there are region codes, try to decode them\n",
    "print(f\"\\n=== Region code meanings (if available) ===\")\n",
    "if 13.0 in actloc_counts.index:\n",
    "    print(f\"Actloc 13.0 (appears {actloc_counts[13.0]} times): likely = Middle East\")\n",
    "if 15.0 in actloc_counts.index:\n",
    "    print(f\"Actloc 15.0 (appears {actloc_counts[15.0]} times): likely = South Asia\")\n",
    "print(\"This suggests Iran was involved in conflicts that spanned different regional theaters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e35cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fix:\n",
      "Iran actloc distribution: {13.0: 15, 15.0: 5}\n",
      "\n",
      "Changed 5 rows\n",
      "After fix:\n",
      "Iran actloc distribution: {13.0: 20}\n"
     ]
    }
   ],
   "source": [
    "# Fix Iran's regional classification: change South Asia (15) to Middle East (13)\n",
    "print(\"Before fix:\")\n",
    "iran_actloc_before = icb_df[icb_df['cracid'] == 630]['actloc'].value_counts().dropna()\n",
    "print(f\"Iran actloc distribution: {dict(iran_actloc_before)}\")\n",
    "\n",
    "# Make the change\n",
    "mask = (icb_df['cracid'] == 630) & (icb_df['actloc'] == 15)\n",
    "rows_changed = mask.sum()\n",
    "icb_df.loc[mask, 'actloc'] = 13\n",
    "\n",
    "print(f\"\\nChanged {rows_changed} rows\")\n",
    "\n",
    "print(\"After fix:\")\n",
    "iran_actloc_after = icb_df[icb_df['cracid'] == 630]['actloc'].value_counts().dropna()\n",
    "print(f\"Iran actloc distribution: {dict(iran_actloc_after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8e4c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking crisno → geog mapping ===\n",
      "Total unique crises: 512\n",
      "⚠️  Found 9 crises with multiple geog values:\n",
      "  Crisis 21: KARL'S RETURN HUNGARY (1921) → geog values: 31.0, 35.0\n",
      "  Crisis 300: RAIDS ON ZIPRA (1979) → geog values: 22.0, 24.0\n",
      "  Crisis 307: RHODESIA SETTLEMENT (1979) → geog values: 22.0, 23.0\n",
      "  Crisis 365: S. AFRICA CROSS BORDER RAID (1986) → geog values: 23.0, 22.0\n",
      "  Crisis 427: US EMBASSY BOMBINGS (1998) → geog values: 13.0, 22.0, 21.0\n",
      "  Crisis 434: AFGHANISTAN/US (2001) → geog values: 13.0, 41.0\n",
      "  Crisis 460: CHAD-SUDAN V (2009) → geog values: 24.0, 21.0\n",
      "  Crisis 466: SUDAN-SOUTH SUDAN (2011) → geog values: 21.0, 22.0\n",
      "  Crisis 499: GALWAN VALLEY BORDER CLASH (2020) → geog values: 13.0, 11.0\n",
      "\n",
      "Summary:\n",
      "  Single geog: 502 crises\n",
      "  Multiple geog: 9 crises\n"
     ]
    }
   ],
   "source": [
    "# Check if every crisno maps to a single geog value\n",
    "crisno_geog_mapping = (\n",
    "    icb_df.groupby(\"crisno\")[\"geog\"]\n",
    "          .apply(lambda x: x.dropna().unique().tolist())\n",
    ")\n",
    "\n",
    "# Find crises with multiple geog values\n",
    "multi_geog_crises = crisno_geog_mapping[crisno_geog_mapping.str.len() > 1]\n",
    "\n",
    "print(\"=== Checking crisno → geog mapping ===\")\n",
    "print(f\"Total unique crises: {len(crisno_geog_mapping)}\")\n",
    "\n",
    "if multi_geog_crises.empty:\n",
    "    print(\"✓ All crises map to a single geog value\")\n",
    "else:\n",
    "    print(f\"⚠️  Found {len(multi_geog_crises)} crises with multiple geog values:\")\n",
    "    for crisno, geog_values in multi_geog_crises.items():\n",
    "        geog_str = \", \".join([str(g) for g in geog_values])\n",
    "        \n",
    "        # Get crisis name if available\n",
    "        crisis_info = icb_df[icb_df['crisno'] == crisno][['crisname', 'yrtrig']].iloc[0]\n",
    "        if pd.notna(crisis_info['crisname']):\n",
    "            crisis_name = crisis_info['crisname']\n",
    "        else:\n",
    "            crisis_name = \"Unknown\"\n",
    "        \n",
    "        year = int(crisis_info['yrtrig']) if pd.notna(crisis_info['yrtrig']) else \"Unknown\"\n",
    "        \n",
    "        print(f\"  Crisis {crisno}: {crisis_name} ({year}) → geog values: {geog_str}\")\n",
    "\n",
    "# Show distribution of geog mapping consistency\n",
    "single_geog_count = (crisno_geog_mapping.str.len() == 1).sum()\n",
    "multi_geog_count = (crisno_geog_mapping.str.len() > 1).sum()\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Single geog: {single_geog_count} crises\")\n",
    "print(f\"  Multiple geog: {multi_geog_count} crises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82922e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Master Dataset Sample ===\n",
      " Crisis_ID          Crisis_Name      Actor_List Actor_Locations\n",
      "         1  RUSSIAN CIVIL WAR I             365              30\n",
      "         2     COSTA RICAN COUP           93;94              42\n",
      "         3 RUSSIAN CIVIL WAR II             365              30\n",
      "         4  BALTIC INDEPENDENCE 365;366;367;368           30;34\n",
      "         5              TESCHEN         290;315              31\n",
      "         6        HUNGARIAN WAR     310;315;360              31\n",
      "         7               SMYRNA         325;350              35\n",
      "         8     THIRD AFGHAN WAR         200;700           13;34\n",
      "         9 FINNISH/RUSSIAN BDR.         365;375           30;34\n",
      "        10           BESSARABIA         360;365           30;31\n",
      "\n",
      "=== Summary ===\n",
      "Total crises: 512\n",
      "Sample Actor_List formats:\n",
      "  Crisis 1: 365\n",
      "  Crisis 2: 93;94\n",
      "  Crisis 3: 365\n",
      "  Crisis 4: 365;366;367;368\n",
      "  Crisis 5: 290;315\n",
      "\n",
      "Sample Actor_Locations formats:\n",
      "  Crisis 1: 30\n",
      "  Crisis 2: 42\n",
      "  Crisis 3: 30\n",
      "  Crisis 4: 30;34\n",
      "  Crisis 5: 31\n",
      "\n",
      "=== Data Quality Checks ===\n",
      "Crises with no actors: 0\n",
      "Crises with no locations: 0\n",
      "\n",
      "Saved Master Dataset to 'icb_master_dataset.csv'\n",
      "\n",
      "=== Machine Readability Example ===\n",
      "Crisis: RUSSIAN CIVIL WAR I\n",
      "Actors: ['365']\n",
      "Locations: ['30']\n",
      "\n",
      "Parsing example:\n",
      "Parsed actors: [365]\n",
      "Parsed locations: [30]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First, ensure we have the cracid_actloc mapping from earlier\n",
    "# (This should already exist from your previous code)\n",
    "cracid_actloc_series = (\n",
    "    icb_df.groupby(\"cracid\")[\"actloc\"]\n",
    "          .apply(lambda x: x.dropna().unique().tolist())\n",
    ")\n",
    "cracid_actloc_dict = cracid_actloc_series.to_dict()\n",
    "\n",
    "def create_master_dataset(df, cracid_actloc_mapping):\n",
    "    \"\"\"\n",
    "    Create Master Dataset with crisis-level information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by crisno to get crisis-level data\n",
    "    crisis_groups = df.groupby('crisno')\n",
    "    \n",
    "    master_data = []\n",
    "    \n",
    "    for crisno, group in crisis_groups:\n",
    "        # Get basic crisis info (should be same across all rows for this crisis)\n",
    "        crisis_name = group['crisname'].iloc[0] if pd.notna(group['crisname'].iloc[0]) else f\"Crisis_{crisno}\"\n",
    "        \n",
    "        # Get all unique cracids for this crisis\n",
    "        unique_cracids = sorted([int(x) for x in group['cracid'].dropna().unique()])\n",
    "        \n",
    "        # Get corresponding actloc values using our mapping\n",
    "        crisis_actlocs = set()\n",
    "        for cracid in unique_cracids:\n",
    "            if cracid in cracid_actloc_mapping:\n",
    "                # Get actloc values for this cracid, convert to integers\n",
    "                actloc_values = cracid_actloc_mapping[cracid]\n",
    "                for actloc in actloc_values:\n",
    "                    if pd.notna(actloc):\n",
    "                        crisis_actlocs.add(int(float(actloc)))\n",
    "        \n",
    "        # Convert to sorted lists for consistent ordering\n",
    "        actor_list = \";\".join([str(x) for x in unique_cracids])\n",
    "        actor_locations = \";\".join([str(x) for x in sorted(crisis_actlocs)])\n",
    "        \n",
    "        master_data.append({\n",
    "            'Crisis_ID': int(crisno),\n",
    "            'Crisis_Name': crisis_name,\n",
    "            'Actor_List': actor_list,\n",
    "            'Actor_Locations': actor_locations\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(master_data)\n",
    "\n",
    "# Create the Master Dataset\n",
    "master_df = create_master_dataset(icb_df, cracid_actloc_dict)\n",
    "\n",
    "# Display sample and summary\n",
    "print(\"=== Master Dataset Sample ===\")\n",
    "print(master_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total crises: {len(master_df)}\")\n",
    "print(f\"Sample Actor_List formats:\")\n",
    "for i, row in master_df.head(5).iterrows():\n",
    "    print(f\"  Crisis {row['Crisis_ID']}: {row['Actor_List']}\")\n",
    "\n",
    "print(f\"\\nSample Actor_Locations formats:\")\n",
    "for i, row in master_df.head(5).iterrows():\n",
    "    print(f\"  Crisis {row['Crisis_ID']}: {row['Actor_Locations']}\")\n",
    "\n",
    "# Check for any issues\n",
    "print(f\"\\n=== Data Quality Checks ===\")\n",
    "empty_actors = master_df[master_df['Actor_List'] == ''].shape[0]\n",
    "empty_locations = master_df[master_df['Actor_Locations'] == ''].shape[0]\n",
    "print(f\"Crises with no actors: {empty_actors}\")\n",
    "print(f\"Crises with no locations: {empty_locations}\")\n",
    "\n",
    "# Save to CSV\n",
    "master_df.to_csv(\"icb_master_dataset.csv\", index=False)\n",
    "print(f\"\\nSaved Master Dataset to 'icb_master_dataset.csv'\")\n",
    "\n",
    "# Show how to parse the data back (for machine readability)\n",
    "print(f\"\\n=== Machine Readability Example ===\")\n",
    "sample_row = master_df.iloc[0]\n",
    "print(f\"Crisis: {sample_row['Crisis_Name']}\")\n",
    "print(f\"Actors: {sample_row['Actor_List'].split(';')}\")\n",
    "print(f\"Locations: {sample_row['Actor_Locations'].split(';')}\")\n",
    "\n",
    "# Function to parse actor/location lists\n",
    "def parse_semicolon_list(semicolon_string):\n",
    "    \"\"\"Helper function to parse semicolon-separated strings back to lists\"\"\"\n",
    "    if semicolon_string == '':\n",
    "        return []\n",
    "    return [int(x) for x in semicolon_string.split(';')]\n",
    "\n",
    "print(f\"\\nParsing example:\")\n",
    "actors = parse_semicolon_list(sample_row['Actor_List'])\n",
    "locations = parse_semicolon_list(sample_row['Actor_Locations'])\n",
    "print(f\"Parsed actors: {actors}\")\n",
    "print(f\"Parsed locations: {locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c943472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ICB1 Dataset Info ===\n",
      "Shape: (512, 95)\n",
      "Columns: ['icb1', 'crisno', 'crisname', 'break', 'trigent', 'yrtrig', 'motrig', 'datrig', 'yrterm', 'moterm', 'daterm', 'brexit', 'gravcr', 'crismg', 'cenviosy', 'sevviosy', 'viol', 'timvio', 'iwcmb', 'noactr', 'gpinv', 'gpinvtp', 'gpefcttp', 'gpefactp', 'gppacetp', 'powinv', 'usinv', 'usefct', 'usefac', 'uspace', 'usactor', 'suinv', 'suefct', 'suefac', 'supace', 'suactor', 'chinv', 'soglact', 'globorg', 'globactm', 'globefct', 'globefor', 'globefac', 'globpace', 'soract', 'regorg', 'regactmb', 'roefct', 'robody', 'roefac', 'ropace', 'subout', 'forout', 'exsat', 'outesr', 'cractr', 'geostr', 'hetero', 'issues', 'chacts', 'chall', 'powch', 'rugach', 'geog', 'geogrel', 'period', 'syslevsy', 'protrac', 'pcid', 'powdissy', 'ethnic', 'ethconf', 'stressad', 'sourdt', 'mediate', 'mednum', 'medwho', 'medtime', 'yrmedst', 'momedst', 'damedst', 'yrmedend', 'momedend', 'damedend', 'yrmedfin', 'momedfin', 'damedfin', 'medgoal', 'medfacl', 'medform', 'medmanip', 'medstyle', 'medstefct', 'medefct', 'medpace']\n",
      "Available date columns: ['yrtrig', 'motrig', 'datrig', 'yrterm', 'moterm', 'daterm']\n",
      "Geographic location column 'geog' found\n",
      "\n",
      "=== Processed ICB1 Dataset ===\n",
      "Shape: (512, 5)\n",
      "Columns: ['Crisis_ID', 'Start_Date', 'End_Date', 'Geographic_Location', 'Crisis_Name']\n",
      "\n",
      "=== Sample Data ===\n",
      " Crisis_ID Start_Date   End_Date  Geographic_Location          Crisis_Name\n",
      "         1 1918-05-01 1920-04-01                 30.0  RUSSIAN CIVIL WAR I\n",
      "         2 1918-05-25 1919-09-03                 42.0     COSTA RICAN COUP\n",
      "         3 1918-06-23 1919-09-27                 30.0 RUSSIAN CIVIL WAR II\n",
      "         4 1918-11-18 1920-08-11                 34.0  BALTIC INDEPENDENCE\n",
      "         5 1919-01-15 1920-07-28                 31.0              TESCHEN\n",
      "         6 1919-03-20 1919-08-03                 31.0        HUNGARIAN WAR\n",
      "         7 1919-03-31 1919-07-29                 10.0               SMYRNA\n",
      "         8 1919-04-15 1919-08-08                 13.0     THIRD AFGHAN WAR\n",
      "         9 1919-04-20 1920-10-14                 34.0 FINNISH/RUSSIAN BDR.\n",
      "        10 1919-04-30 1920-03-02                 31.0           BESSARABIA\n",
      "\n",
      "=== Data Quality Summary ===\n",
      "Crisis_ID: 0/512 missing (0.0%)\n",
      "Start_Date: 1/512 missing (0.2%)\n",
      "End_Date: 2/512 missing (0.4%)\n",
      "Geographic_Location: 1/512 missing (0.2%)\n",
      "Crisis_Name: 0/512 missing (0.0%)\n",
      "\n",
      "Start dates range: 1918-05-01 to 2021-09-20\n",
      "End dates range: 1919-07-29 to 2022-12-01\n",
      "\n",
      "Geographic location distribution:\n",
      "Geographic_Location\n",
      "9.0      6\n",
      "10.0    25\n",
      "11.0    39\n",
      "12.0    47\n",
      "13.0    38\n",
      "15.0    84\n",
      "20.0    17\n",
      "21.0    32\n",
      "22.0    43\n",
      "23.0     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved processed ICB1 data to 'icb1_processed_dates_locations.csv'\n",
      "\n",
      "=== Merging with Master Dataset ===\n",
      "To merge with your master dataset:\n",
      "merged_df = master_df.merge(icb1_processed, on='Crisis_ID', how='left')\n",
      "\n",
      "=== Date Format Examples ===\n",
      "Crisis: 1918-05-01 → 1920-04-01\n",
      "  Duration: 701 days\n",
      "Crisis: 1918-05-25 → 1919-09-03\n",
      "  Duration: 466 days\n",
      "Crisis: 1918-06-23 → 1919-09-27\n",
      "  Duration: 461 days\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Read ICB1 dataset\n",
    "icb1_df = pd.read_csv(\"icb1v16.csv\")\n",
    "\n",
    "print(\"=== ICB1 Dataset Info ===\")\n",
    "print(f\"Shape: {icb1_df.shape}\")\n",
    "print(f\"Columns: {list(icb1_df.columns)}\")\n",
    "\n",
    "def create_standardized_date(year, month, day, date_type=\"start\"):\n",
    "    \"\"\"\n",
    "    Create standardized date from separate year, month, day components\n",
    "    Handles missing values appropriately\n",
    "    \"\"\"\n",
    "    if pd.isna(year):\n",
    "        return None\n",
    "    \n",
    "    year = int(year)\n",
    "    \n",
    "    # Handle missing month - default to January for start, December for end\n",
    "    if pd.isna(month):\n",
    "        month = 1 if date_type == \"start\" else 12\n",
    "    else:\n",
    "        month = int(month)\n",
    "        month = max(1, min(12, month))  # Ensure valid month\n",
    "    \n",
    "    # Handle missing day - default to 1st for start, last day of month for end\n",
    "    if pd.isna(day):\n",
    "        if date_type == \"start\":\n",
    "            day = 1\n",
    "        else:\n",
    "            # Get last day of the month\n",
    "            if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "                day = 31\n",
    "            elif month in [4, 6, 9, 11]:\n",
    "                day = 30\n",
    "            else:  # February\n",
    "                # Simple leap year check\n",
    "                if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):\n",
    "                    day = 29\n",
    "                else:\n",
    "                    day = 28\n",
    "    else:\n",
    "        day = int(day)\n",
    "        day = max(1, min(31, day))  # Ensure valid day\n",
    "    \n",
    "    try:\n",
    "        return datetime(year, month, day).strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        # Handle invalid dates (e.g., Feb 30) by adjusting day\n",
    "        try:\n",
    "            return datetime(year, month, min(day, 28)).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            return f\"{year}-{month:02d}-01\"  # Fallback to first of month\n",
    "\n",
    "def process_icb1_dates_locations(df):\n",
    "    \"\"\"\n",
    "    Process ICB1 dataset to extract standardized dates and locations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check what columns are actually available\n",
    "    date_cols = ['yrtrig', 'motrig', 'datrig', 'yrterm', 'moterm', 'daterm']\n",
    "    available_cols = [col for col in date_cols if col in df.columns]\n",
    "    print(f\"Available date columns: {available_cols}\")\n",
    "    \n",
    "    if 'geog' in df.columns:\n",
    "        print(\"Geographic location column 'geog' found\")\n",
    "    else:\n",
    "        print(\"Warning: 'geog' column not found\")\n",
    "    \n",
    "    # Create the processed dataset\n",
    "    processed_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        crisis_data = {}\n",
    "        \n",
    "        # Basic crisis identifier\n",
    "        if 'crisno' in df.columns:\n",
    "            crisis_data['Crisis_ID'] = int(row['crisno']) if pd.notna(row['crisno']) else None\n",
    "        \n",
    "        # Create standardized start date\n",
    "        if all(col in df.columns for col in ['yrtrig', 'motrig', 'datrig']):\n",
    "            start_date = create_standardized_date(\n",
    "                row['yrtrig'], row['motrig'], row['datrig'], \"start\"\n",
    "            )\n",
    "            crisis_data['Start_Date'] = start_date\n",
    "        \n",
    "        # Create standardized end date\n",
    "        if all(col in df.columns for col in ['yrterm', 'moterm', 'daterm']):\n",
    "            end_date = create_standardized_date(\n",
    "                row['yrterm'], row['moterm'], row['daterm'], \"end\"\n",
    "            )\n",
    "            crisis_data['End_Date'] = end_date\n",
    "        \n",
    "        # Geographic location\n",
    "        if 'geog' in df.columns:\n",
    "            crisis_data['Geographic_Location'] = int(row['geog']) if pd.notna(row['geog']) else None\n",
    "        \n",
    "        # Add crisis name if available\n",
    "        if 'crisname' in df.columns:\n",
    "            crisis_data['Crisis_Name'] = row['crisname'] if pd.notna(row['crisname']) else None\n",
    "        \n",
    "        processed_data.append(crisis_data)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Process the ICB1 dataset\n",
    "icb1_processed = process_icb1_dates_locations(icb1_df)\n",
    "\n",
    "# Remove duplicates if any (should be one row per crisis)\n",
    "if 'Crisis_ID' in icb1_processed.columns:\n",
    "    icb1_processed = icb1_processed.drop_duplicates(subset=['Crisis_ID'])\n",
    "\n",
    "print(f\"\\n=== Processed ICB1 Dataset ===\")\n",
    "print(f\"Shape: {icb1_processed.shape}\")\n",
    "print(f\"Columns: {list(icb1_processed.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\n=== Sample Data ===\")\n",
    "print(icb1_processed.head(10).to_string(index=False))\n",
    "\n",
    "# Data quality checks\n",
    "print(f\"\\n=== Data Quality Summary ===\")\n",
    "for col in icb1_processed.columns:\n",
    "    null_count = icb1_processed[col].isnull().sum()\n",
    "    total_count = len(icb1_processed)\n",
    "    print(f\"{col}: {null_count}/{total_count} missing ({null_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Show date range\n",
    "if 'Start_Date' in icb1_processed.columns:\n",
    "    valid_start_dates = icb1_processed['Start_Date'].dropna()\n",
    "    if len(valid_start_dates) > 0:\n",
    "        print(f\"\\nStart dates range: {valid_start_dates.min()} to {valid_start_dates.max()}\")\n",
    "\n",
    "if 'End_Date' in icb1_processed.columns:\n",
    "    valid_end_dates = icb1_processed['End_Date'].dropna()\n",
    "    if len(valid_end_dates) > 0:\n",
    "        print(f\"End dates range: {valid_end_dates.min()} to {valid_end_dates.max()}\")\n",
    "\n",
    "# Geographic location distribution\n",
    "if 'Geographic_Location' in icb1_processed.columns:\n",
    "    geog_dist = icb1_processed['Geographic_Location'].value_counts().sort_index()\n",
    "    print(f\"\\nGeographic location distribution:\")\n",
    "    print(geog_dist.head(10))\n",
    "\n",
    "# Save processed dataset\n",
    "icb1_processed.to_csv(\"icb1_processed_dates_locations.csv\", index=False)\n",
    "print(f\"\\nSaved processed ICB1 data to 'icb1_processed_dates_locations.csv'\")\n",
    "\n",
    "# Show how to merge with master dataset (if it exists)\n",
    "print(f\"\\n=== Merging with Master Dataset ===\")\n",
    "print(\"To merge with your master dataset:\")\n",
    "print(\"master_df = master_df.merge(icb1_processed, on='Crisis_ID', how='left')\")\n",
    "\n",
    "# Example of date parsing\n",
    "print(f\"\\n=== Date Format Examples ===\")\n",
    "sample_dates = icb1_processed[['Start_Date', 'End_Date']].dropna().head(3)\n",
    "for idx, row in sample_dates.iterrows():\n",
    "    print(f\"Crisis: {row['Start_Date']} → {row['End_Date']}\")\n",
    "    # Show how to parse back to datetime objects\n",
    "    start_dt = pd.to_datetime(row['Start_Date'])\n",
    "    end_dt = pd.to_datetime(row['End_Date'])\n",
    "    duration = (end_dt - start_dt).days\n",
    "    print(f\"  Duration: {duration} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e957c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df.merge(icb1_processed, on='Crisis_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b48e46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Crisis_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Crisis_Name_x",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Actor_List",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Actor_Locations",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Start_Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "End_Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Geographic_Location",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Crisis_Name_y",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "66c66775-fe3f-4b82-83de-84d0240fde37",
       "rows": [
        [
         "0",
         "1",
         "RUSSIAN CIVIL WAR I",
         "365",
         "30",
         "1918-05-01",
         "1920-04-01",
         "30.0",
         "RUSSIAN CIVIL WAR I"
        ],
        [
         "1",
         "2",
         "COSTA RICAN COUP",
         "93;94",
         "42",
         "1918-05-25",
         "1919-09-03",
         "42.0",
         "COSTA RICAN COUP"
        ],
        [
         "2",
         "3",
         "RUSSIAN CIVIL WAR II",
         "365",
         "30",
         "1918-06-23",
         "1919-09-27",
         "30.0",
         "RUSSIAN CIVIL WAR II"
        ],
        [
         "3",
         "4",
         "BALTIC INDEPENDENCE",
         "365;366;367;368",
         "30;34",
         "1918-11-18",
         "1920-08-11",
         "34.0",
         "BALTIC INDEPENDENCE"
        ],
        [
         "4",
         "5",
         "TESCHEN",
         "290;315",
         "31",
         "1919-01-15",
         "1920-07-28",
         "31.0",
         "TESCHEN"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crisis_ID</th>\n",
       "      <th>Crisis_Name_x</th>\n",
       "      <th>Actor_List</th>\n",
       "      <th>Actor_Locations</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>End_Date</th>\n",
       "      <th>Geographic_Location</th>\n",
       "      <th>Crisis_Name_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RUSSIAN CIVIL WAR I</td>\n",
       "      <td>365</td>\n",
       "      <td>30</td>\n",
       "      <td>1918-05-01</td>\n",
       "      <td>1920-04-01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>RUSSIAN CIVIL WAR I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>COSTA RICAN COUP</td>\n",
       "      <td>93;94</td>\n",
       "      <td>42</td>\n",
       "      <td>1918-05-25</td>\n",
       "      <td>1919-09-03</td>\n",
       "      <td>42.0</td>\n",
       "      <td>COSTA RICAN COUP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>RUSSIAN CIVIL WAR II</td>\n",
       "      <td>365</td>\n",
       "      <td>30</td>\n",
       "      <td>1918-06-23</td>\n",
       "      <td>1919-09-27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>RUSSIAN CIVIL WAR II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BALTIC INDEPENDENCE</td>\n",
       "      <td>365;366;367;368</td>\n",
       "      <td>30;34</td>\n",
       "      <td>1918-11-18</td>\n",
       "      <td>1920-08-11</td>\n",
       "      <td>34.0</td>\n",
       "      <td>BALTIC INDEPENDENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TESCHEN</td>\n",
       "      <td>290;315</td>\n",
       "      <td>31</td>\n",
       "      <td>1919-01-15</td>\n",
       "      <td>1920-07-28</td>\n",
       "      <td>31.0</td>\n",
       "      <td>TESCHEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Crisis_ID         Crisis_Name_x       Actor_List Actor_Locations  \\\n",
       "0          1   RUSSIAN CIVIL WAR I              365              30   \n",
       "1          2      COSTA RICAN COUP            93;94              42   \n",
       "2          3  RUSSIAN CIVIL WAR II              365              30   \n",
       "3          4   BALTIC INDEPENDENCE  365;366;367;368           30;34   \n",
       "4          5               TESCHEN          290;315              31   \n",
       "\n",
       "   Start_Date    End_Date  Geographic_Location         Crisis_Name_y  \n",
       "0  1918-05-01  1920-04-01                 30.0   RUSSIAN CIVIL WAR I  \n",
       "1  1918-05-25  1919-09-03                 42.0      COSTA RICAN COUP  \n",
       "2  1918-06-23  1919-09-27                 30.0  RUSSIAN CIVIL WAR II  \n",
       "3  1918-11-18  1920-08-11                 34.0   BALTIC INDEPENDENCE  \n",
       "4  1919-01-15  1920-07-28                 31.0               TESCHEN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b3c3fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ATOP Dataset Info ===\n",
      "Shape: (789, 134)\n",
      "Sample columns: ['atopid', 'cowid', 'cow4id', 'begyr', 'begmo', 'begday', 'endyr', 'endmo', 'endday', 'ineffect', 'bilat', 'maxphase', 'wartime', 'estmode', 'pubsecr', 'secrart', 'proadd', 'futmem', 'speclgth', 'length']...\n",
      "\n",
      "=== Processing ATOP Dataset ===\n",
      "Available required columns: ['atopid', 'begday', 'begmo', 'begyr', 'endday', 'endmo', 'endyr']\n",
      "Available alliance type columns: ['defense', 'offense', 'neutral', 'nonagg', 'consul']\n",
      "Found 59 member columns: ['mem1', 'mem2', 'mem3', 'mem4', 'mem5', 'mem6', 'mem7', 'mem8', 'mem9', 'mem10']...\n",
      "COW ID columns found: ['cowid']\n",
      "\n",
      "=== Processed ATOP Dataset ===\n",
      "Shape: (789, 9)\n",
      "Columns: ['Alliance_ID', 'Alliance_Name', 'Alliance_Start', 'Alliance_End', 'Alliance_Type', 'N_Members', 'Members_List', 'COWID', 'COW4ID']\n",
      "\n",
      "=== Sample Data ===\n",
      " Alliance_ID Alliance_Name Alliance_Start Alliance_End Alliance_Type  N_Members                            Members_List  COWID  COW4ID\n",
      "        1005 Alliance_1005     1815-01-03   1815-02-08           1;5          6                 200;210;220;240;245;300    NaN     NaN\n",
      "        1010 Alliance_1010     1815-01-14   1815-06-08           1;5          2                                 275;300    NaN     NaN\n",
      "        1015 Alliance_1015     1815-04-29   1815-06-12           1;2          2                                 300;329    NaN     NaN\n",
      "        1020 Alliance_1020     1815-06-08   1866-06-15         1;4;5         10 240;245;255;267;269;271;273;275;280;300 2005.0     3.0\n",
      "        1025 Alliance_1025     1815-06-12   1820-07-13           1;5          2                                 300;329 2006.0     4.0\n",
      "        1030 Alliance_1030     1815-06-12   1848-03-29           1;5          2                                 300;337 2007.0     5.0\n",
      "        1035 Alliance_1035     1815-11-20   1822-12-31         1;2;5          4                         200;255;300;365 2008.0     6.0\n",
      "        1040 Alliance_1040     1816-08-10   1820-03-31           1;2          2                                 210;230 2010.0     7.0\n",
      "        1045 Alliance_1045     1827-07-06   1829-09-14             5          3                             200;220;365 2011.0     8.0\n",
      "        1050 Alliance_1050     1831-07-23   1848-03-24             1          2                                 300;325 2012.0     9.0\n",
      "\n",
      "=== Data Quality Summary ===\n",
      "Alliance_ID: 0/789 missing (0.0%)\n",
      "Alliance_Name: 0/789 missing (0.0%)\n",
      "Alliance_Start: 0/789 missing (0.0%)\n",
      "Alliance_End: 0/789 missing (0.0%)\n",
      "Alliance_Type: 0/789 missing (0.0%)\n",
      "N_Members: 0/789 missing (0.0%)\n",
      "Members_List: 0/789 missing (0.0%)\n",
      "COWID: 479/789 missing (60.7%)\n",
      "COW4ID: 395/789 missing (50.1%)\n",
      "\n",
      "=== Alliance Type Analysis ===\n",
      "Single type alliances: 381\n",
      "Multiple type alliances: 408\n",
      "Examples of multiple-type alliances:\n",
      "  Alliance 1005: defense, consul\n",
      "  Alliance 1010: defense, consul\n",
      "  Alliance 1015: defense, offense\n",
      "  Alliance 1020: defense, nonagg, consul\n",
      "  Alliance 1025: defense, consul\n",
      "\n",
      "=== Membership Analysis ===\n",
      "Member count distribution (top 10):\n",
      "N_Members\n",
      "2     665\n",
      "3      51\n",
      "4      25\n",
      "5      12\n",
      "6       6\n",
      "7       3\n",
      "8       3\n",
      "9       3\n",
      "10      1\n",
      "11      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Largest alliances:\n",
      "  Alliance_3740: 59 members - 2, 20, 200, 205, 210 (+54 more)\n",
      "  Alliance_4953: 53 members - 402, 403, 404, 411, 420 (+48 more)\n",
      "  Alliance_2550: 43 members - 2, 20, 40, 41, 42 (+38 more)\n",
      "  Alliance_3150: 34 members - 2, 31, 40, 41, 42 (+29 more)\n",
      "  Alliance_3755: 34 members - 2, 20, 140, 155, 200 (+29 more)\n",
      "\n",
      "Alliance start dates range: 1815-01-03 to 2018-09-28\n",
      "Active alliances (no end date): 0\n",
      "Alliance end dates range: 0-01-01 to 2018-06-17\n",
      "\n",
      "Saved processed ATOP data to 'atop_master2_dataset.csv'\n",
      "\n",
      "=== Machine Readability Examples ===\n",
      "Alliance: Alliance_1005\n",
      "Types: ['defense', 'consul']\n",
      "Members: ['200', '210', '220', '240', '245']...\n",
      "\n",
      "=== Helper Functions for Data Parsing ===\n",
      "\n",
      "def parse_alliance_types(type_string):\n",
      "    '''Parse alliance types back to list'''\n",
      "    if not type_string:\n",
      "        return []\n",
      "    return [int(t) for t in type_string.split(';')]\n",
      "\n",
      "def parse_members_list(members_string):\n",
      "    '''Parse members list back to list'''\n",
      "    if not members_string:\n",
      "        return []\n",
      "    return [int(m) for m in members_string.split(';')]\n",
      "\n",
      "def get_alliance_type_names(type_numbers):\n",
      "    '''Convert type numbers back to names'''\n",
      "    type_names = {1: 'defense', 2: 'offense', 3: 'neutral', 4: 'nonagg', 5: 'consul'}\n",
      "    return [type_names.get(t, f'type_{t}') for t in type_numbers]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"=== ATOP Dataset Info ===\")\n",
    "print(f\"Shape: {atop_df.shape}\")\n",
    "print(f\"Sample columns: {list(atop_df.columns)[:20]}...\")\n",
    "\n",
    "def create_standardized_date(year, month, day, date_type=\"start\"):\n",
    "    \"\"\"\n",
    "    Create standardized date from separate year, month, day components\n",
    "    Same logic as ICB processing\n",
    "    \"\"\"\n",
    "    if pd.isna(year):\n",
    "        return None\n",
    "    \n",
    "    year = int(year)\n",
    "    \n",
    "    # Handle missing month - default to January for start, December for end\n",
    "    if pd.isna(month):\n",
    "        month = 1 if date_type == \"start\" else 12\n",
    "    else:\n",
    "        month = int(month)\n",
    "        month = max(1, min(12, month))  # Ensure valid month\n",
    "    \n",
    "    # Handle missing day - default to 1st for start, last day of month for end\n",
    "    if pd.isna(day):\n",
    "        if date_type == \"start\":\n",
    "            day = 1\n",
    "        else:\n",
    "            # Get last day of the month\n",
    "            if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "                day = 31\n",
    "            elif month in [4, 6, 9, 11]:\n",
    "                day = 30\n",
    "            else:  # February\n",
    "                # Simple leap year check\n",
    "                if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):\n",
    "                    day = 29\n",
    "                else:\n",
    "                    day = 28\n",
    "    else:\n",
    "        day = int(day)\n",
    "        day = max(1, min(31, day))  # Ensure valid day\n",
    "    \n",
    "    try:\n",
    "        return datetime(year, month, day).strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        # Handle invalid dates (e.g., Feb 30) by adjusting day\n",
    "        try:\n",
    "            return datetime(year, month, min(day, 28)).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            return f\"{year}-{month:02d}-01\"  # Fallback to first of month\n",
    "\n",
    "def extract_alliance_types(row):\n",
    "    \"\"\"\n",
    "    Extract alliance types from dummy variables and map to numbers\n",
    "    defense=1, offense=2, neutral=3, nonagg=4, consul=5\n",
    "    \"\"\"\n",
    "    type_mapping = {\n",
    "        'defense': 1,\n",
    "        'offense': 2, \n",
    "        'neutral': 3,\n",
    "        'nonagg': 4,\n",
    "        'consul': 5\n",
    "    }\n",
    "    \n",
    "    alliance_types = []\n",
    "    for type_name, type_num in type_mapping.items():\n",
    "        if type_name in row.index and pd.notna(row[type_name]) and row[type_name] == 1:\n",
    "            alliance_types.append(type_num)\n",
    "    \n",
    "    return \";\".join([str(t) for t in sorted(alliance_types)])\n",
    "\n",
    "def extract_members(row):\n",
    "    \"\"\"\n",
    "    Extract member list from mem1, mem2, mem3... columns\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    \n",
    "    # Find all mem* columns\n",
    "    mem_cols = [col for col in row.index if col.startswith('mem') and col[3:].isdigit()]\n",
    "    mem_cols.sort(key=lambda x: int(x[3:]))  # Sort by number\n",
    "    \n",
    "    for col in mem_cols:\n",
    "        if pd.notna(row[col]):\n",
    "            # Convert to int if it's a float\n",
    "            member_val = int(row[col]) if isinstance(row[col], (int, float)) else row[col]\n",
    "            members.append(str(member_val))\n",
    "    \n",
    "    return \";\".join(members)\n",
    "\n",
    "def count_members(row):\n",
    "    \"\"\"\n",
    "    Count number of members from mem* columns\n",
    "    \"\"\"\n",
    "    mem_cols = [col for col in row.index if col.startswith('mem') and col[3:].isdigit()]\n",
    "    return sum(1 for col in mem_cols if pd.notna(row[col]))\n",
    "\n",
    "def process_atop_dataset(df):\n",
    "    \"\"\"\n",
    "    Process ATOP dataset to create Master2 dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check what columns are available\n",
    "    required_cols = ['atopid', 'begday', 'begmo', 'begyr', 'endday', 'endmo', 'endyr']\n",
    "    alliance_type_cols = ['defense', 'offense', 'neutral', 'nonagg', 'consul']\n",
    "    \n",
    "    available_required = [col for col in required_cols if col in df.columns]\n",
    "    available_types = [col for col in alliance_type_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"Available required columns: {available_required}\")\n",
    "    print(f\"Available alliance type columns: {available_types}\")\n",
    "    \n",
    "    # Find mem* columns\n",
    "    mem_cols = [col for col in df.columns if col.startswith('mem') and col[3:].isdigit()]\n",
    "    print(f\"Found {len(mem_cols)} member columns: {mem_cols[:10]}...\" if len(mem_cols) > 10 else f\"Member columns: {mem_cols}\")\n",
    "    \n",
    "    # Check for cowid columns\n",
    "    cow_cols = [col for col in df.columns if 'cowid' in col.lower()]\n",
    "    print(f\"COW ID columns found: {cow_cols}\")\n",
    "    \n",
    "    # Create the processed dataset\n",
    "    processed_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        alliance_data = {}\n",
    "        \n",
    "        # Alliance ID\n",
    "        if 'atopid' in df.columns:\n",
    "            alliance_data['Alliance_ID'] = int(row['atopid']) if pd.notna(row['atopid']) else None\n",
    "        \n",
    "        # Alliance Name (generic since not provided)\n",
    "        if 'atopid' in df.columns and pd.notna(row['atopid']):\n",
    "            alliance_data['Alliance_Name'] = f\"Alliance_{int(row['atopid'])}\"\n",
    "        else:\n",
    "            alliance_data['Alliance_Name'] = f\"Alliance_Unknown_{idx}\"\n",
    "        \n",
    "        # Create standardized start date\n",
    "        if all(col in df.columns for col in ['begyr', 'begmo', 'begday']):\n",
    "            start_date = create_standardized_date(\n",
    "                row['begyr'], row['begmo'], row['begday'], \"start\"\n",
    "            )\n",
    "            alliance_data['Alliance_Start'] = start_date\n",
    "        \n",
    "        # Create standardized end date\n",
    "        if all(col in df.columns for col in ['endyr', 'endmo', 'endday']):\n",
    "            end_date = create_standardized_date(\n",
    "                row['endyr'], row['endmo'], row['endday'], \"end\"\n",
    "            )\n",
    "            alliance_data['Alliance_End'] = end_date\n",
    "        \n",
    "        # Alliance types\n",
    "        alliance_data['Alliance_Type'] = extract_alliance_types(row)\n",
    "        \n",
    "        # Number of members\n",
    "        alliance_data['N_Members'] = count_members(row)\n",
    "        \n",
    "        # Members list\n",
    "        alliance_data['Members_List'] = extract_members(row)\n",
    "        \n",
    "        # COW IDs if available\n",
    "        if 'cowid' in df.columns:\n",
    "            alliance_data['COWID'] = int(row['cowid']) if pd.notna(row['cowid']) else None\n",
    "        if 'cow4id' in df.columns:\n",
    "            alliance_data['COW4ID'] = int(row['cow4id']) if pd.notna(row['cow4id']) else None\n",
    "        \n",
    "        processed_data.append(alliance_data)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Process the ATOP dataset\n",
    "print(\"\\n=== Processing ATOP Dataset ===\")\n",
    "atop_processed = process_atop_dataset(atop_df)\n",
    "\n",
    "# Remove duplicates if any\n",
    "if 'Alliance_ID' in atop_processed.columns:\n",
    "    atop_processed = atop_processed.drop_duplicates(subset=['Alliance_ID'])\n",
    "\n",
    "print(f\"\\n=== Processed ATOP Dataset ===\")\n",
    "print(f\"Shape: {atop_processed.shape}\")\n",
    "print(f\"Columns: {list(atop_processed.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\n=== Sample Data ===\")\n",
    "print(atop_processed.head(10).to_string(index=False))\n",
    "\n",
    "# Data quality checks\n",
    "print(f\"\\n=== Data Quality Summary ===\")\n",
    "for col in atop_processed.columns:\n",
    "    null_count = atop_processed[col].isnull().sum()\n",
    "    total_count = len(atop_processed)\n",
    "    print(f\"{col}: {null_count}/{total_count} missing ({null_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Show alliance type distribution\n",
    "if 'Alliance_Type' in atop_processed.columns:\n",
    "    print(f\"\\n=== Alliance Type Analysis ===\")\n",
    "    # Count single vs multiple types\n",
    "    single_types = atop_processed[~atop_processed['Alliance_Type'].str.contains(';', na=False)]\n",
    "    multi_types = atop_processed[atop_processed['Alliance_Type'].str.contains(';', na=False)]\n",
    "    \n",
    "    print(f\"Single type alliances: {len(single_types)}\")\n",
    "    print(f\"Multiple type alliances: {len(multi_types)}\")\n",
    "    \n",
    "    if len(multi_types) > 0:\n",
    "        print(\"Examples of multiple-type alliances:\")\n",
    "        for idx, row in multi_types.head(5).iterrows():\n",
    "            types = row['Alliance_Type'].split(';')\n",
    "            type_names = {1: 'defense', 2: 'offense', 3: 'neutral', 4: 'nonagg', 5: 'consul'}\n",
    "            type_desc = [type_names.get(int(t), f'type_{t}') for t in types]\n",
    "            print(f\"  Alliance {row['Alliance_ID']}: {', '.join(type_desc)}\")\n",
    "\n",
    "# Show member analysis\n",
    "if 'N_Members' in atop_processed.columns:\n",
    "    print(f\"\\n=== Membership Analysis ===\")\n",
    "    member_dist = atop_processed['N_Members'].value_counts().sort_index()\n",
    "    print(f\"Member count distribution (top 10):\")\n",
    "    print(member_dist.head(10))\n",
    "    \n",
    "    print(f\"\\nLargest alliances:\")\n",
    "    largest = atop_processed.nlargest(5, 'N_Members')[['Alliance_ID', 'Alliance_Name', 'N_Members', 'Members_List']]\n",
    "    for idx, row in largest.iterrows():\n",
    "        members = row['Members_List'].split(';')[:5]  # Show first 5 members\n",
    "        more = f\" (+{row['N_Members']-5} more)\" if row['N_Members'] > 5 else \"\"\n",
    "        print(f\"  {row['Alliance_Name']}: {row['N_Members']} members - {', '.join(members)}{more}\")\n",
    "\n",
    "# Date range analysis\n",
    "if 'Alliance_Start' in atop_processed.columns:\n",
    "    valid_start_dates = atop_processed['Alliance_Start'].dropna()\n",
    "    if len(valid_start_dates) > 0:\n",
    "        print(f\"\\nAlliance start dates range: {valid_start_dates.min()} to {valid_start_dates.max()}\")\n",
    "\n",
    "if 'Alliance_End' in atop_processed.columns:\n",
    "    valid_end_dates = atop_processed['Alliance_End'].dropna()\n",
    "    active_alliances = atop_processed['Alliance_End'].isnull().sum()\n",
    "    print(f\"Active alliances (no end date): {active_alliances}\")\n",
    "    if len(valid_end_dates) > 0:\n",
    "        print(f\"Alliance end dates range: {valid_end_dates.min()} to {valid_end_dates.max()}\")\n",
    "\n",
    "# Save processed dataset\n",
    "atop_processed.to_csv(\"atop_master2_dataset.csv\", index=False)\n",
    "print(f\"\\nSaved processed ATOP data to 'atop_master2_dataset.csv'\")\n",
    "\n",
    "# Show parsing examples\n",
    "print(f\"\\n=== Machine Readability Examples ===\")\n",
    "sample_row = atop_processed.iloc[0]\n",
    "print(f\"Alliance: {sample_row['Alliance_Name']}\")\n",
    "if sample_row['Alliance_Type']:\n",
    "    types = sample_row['Alliance_Type'].split(';')\n",
    "    type_names = {1: 'defense', 2: 'offense', 3: 'neutral', 4: 'nonagg', 5: 'consul'}\n",
    "    print(f\"Types: {[type_names.get(int(t), f'type_{t}') for t in types]}\")\n",
    "if sample_row['Members_List']:\n",
    "    members = sample_row['Members_List'].split(';')\n",
    "    print(f\"Members: {members[:5]}{'...' if len(members) > 5 else ''}\")\n",
    "\n",
    "# Helper functions for parsing\n",
    "print(f\"\\n=== Helper Functions for Data Parsing ===\")\n",
    "print(\"\"\"\n",
    "def parse_alliance_types(type_string):\n",
    "    '''Parse alliance types back to list'''\n",
    "    if not type_string:\n",
    "        return []\n",
    "    return [int(t) for t in type_string.split(';')]\n",
    "\n",
    "def parse_members_list(members_string):\n",
    "    '''Parse members list back to list'''\n",
    "    if not members_string:\n",
    "        return []\n",
    "    return [int(m) for m in members_string.split(';')]\n",
    "\n",
    "def get_alliance_type_names(type_numbers):\n",
    "    '''Convert type numbers back to names'''\n",
    "    type_names = {1: 'defense', 2: 'offense', 3: 'neutral', 4: 'nonagg', 5: 'consul'}\n",
    "    return [type_names.get(t, f'type_{t}') for t in type_numbers]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e4792a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ATOP-ICB member consistency check...\n",
      "=== ATOP-ICB Member Consistency Analysis ===\n",
      "ATOP unique members: 208\n",
      "ICB unique cracids: 147\n",
      "\n",
      "=== Overlap Analysis ===\n",
      "Common members (in both datasets): 142\n",
      "ATOP-only members (missing from ICB): 66\n",
      "ICB-only cracids (missing from ATOP): 5\n",
      "\n",
      "=== Coverage Analysis ===\n",
      "ATOP members covered by ICB: 68.3%\n",
      "ICB cracids covered by ATOP: 96.6%\n",
      "\n",
      "=== ATOP Members Missing from ICB (PROBLEM) ===\n",
      "These 66 countries appear in ATOP but not ICB:\n",
      "First 20: [31, 51, 52, 53, 54, 56, 57, 58, 60, 80, 115, 140, 165, 205, 221, 223, 232, 240, 245, 267]\n",
      "... and 46 more\n",
      "\n",
      "Alliances containing missing members:\n",
      "  Alliance 1005: missing members [240, 245, 300]\n",
      "  Alliance 1010: missing members [275, 300]\n",
      "  Alliance 1015: missing members [300, 329]\n",
      "  Alliance 1020: missing members [240, 245, 267, 269, 271, 273, 275, 280, 300]\n",
      "  Alliance 1025: missing members [300, 329]\n",
      "  Alliance 1030: missing members [300, 337]\n",
      "  Alliance 1035: missing members [300]\n",
      "  Alliance 1050: missing members [300]\n",
      "  Alliance 1065: missing members [300]\n",
      "  Alliance 1070: missing members [329]\n",
      "  ... and 207 more affected alliances\n",
      "Total affected alliances: 217/789\n",
      "\n",
      "=== ICB Cracids Missing from ATOP (INFO) ===\n",
      "These 5 countries appear in ICB but not ATOP:\n",
      "All missing: [219, 347, 671, 672, 940]\n",
      "\n",
      "=== Recommendations ===\n",
      "⚠️  ISSUE: 66 ATOP members missing from ICB\n",
      "   → These members cannot be mapped to locations using ICB data\n",
      "   → 217 alliances affected\n",
      "   → Consider: manual mapping, alternative data source, or exclude these members\n",
      "ℹ️  INFO: 5 ICB countries not in any alliance (normal)\n",
      "\n",
      "=== Detailed Member Mapping Report ===\n",
      "Problematic members summary:\n",
      "   Missing_Member_ID  In_ATOP  In_ICB  Location_Mappable\n",
      "0                 31     True   False              False\n",
      "1                 51     True   False              False\n",
      "2                 52     True   False              False\n",
      "3                 53     True   False              False\n",
      "4                 54     True   False              False\n",
      "5                 56     True   False              False\n",
      "6                 57     True   False              False\n",
      "7                 58     True   False              False\n",
      "8                 60     True   False              False\n",
      "9                 80     True   False              False\n",
      "\n",
      "Saved detailed report to 'atop_icb_missing_members.csv'\n",
      "\n",
      "=== Additional Statistics ===\n",
      "ATOP member ID range: 2 to 8152\n",
      "ICB cracid range: 2 to 940\n",
      "\n",
      "Sample common members: [2, 20, 40, 41, 42, 55, 70, 90, 91, 92]...\n",
      "\n",
      "Missing member patterns:\n",
      "  Smallest missing: 31\n",
      "  Largest missing: 8152\n",
      "  High-numbered missing codes (>1000): 1 - might be different coding system\n",
      "    Examples: [8152]\n",
      "\n",
      "=== Next Steps ===\n",
      "1. Review 'atop_icb_missing_members.csv' for manual inspection\n",
      "2. Consider alternative country code mappings (COW, ISO, etc.)\n",
      "3. For location mapping, you may need to:\n",
      "   - Exclude alliances with unmappable members\n",
      "   - Use alternative geographic data sources\n",
      "   - Manually map missing country codes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_all_atop_members(atop_processed_df):\n",
    "    \"\"\"\n",
    "    Extract all unique member IDs from ATOP Members_List\n",
    "    \"\"\"\n",
    "    all_members = set()\n",
    "    \n",
    "    for members_string in atop_processed_df['Members_List'].dropna():\n",
    "        if members_string:  # Not empty\n",
    "            members = members_string.split(';')\n",
    "            for member in members:\n",
    "                try:\n",
    "                    all_members.add(int(member))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not convert member '{member}' to integer\")\n",
    "    \n",
    "    return sorted(list(all_members))\n",
    "\n",
    "def get_icb_cracids(icb_df):\n",
    "    \"\"\"\n",
    "    Get all unique cracid values from ICB dataset\n",
    "    \"\"\"\n",
    "    return sorted(list(icb_df['cracid'].dropna().unique().astype(int)))\n",
    "\n",
    "def analyze_member_consistency(atop_processed_df, icb_df):\n",
    "    \"\"\"\n",
    "    Analyze consistency between ATOP members and ICB cracids\n",
    "    \"\"\"\n",
    "    print(\"=== ATOP-ICB Member Consistency Analysis ===\")\n",
    "    \n",
    "    # Extract unique members from both datasets\n",
    "    atop_members = extract_all_atop_members(atop_processed_df)\n",
    "    icb_cracids = get_icb_cracids(icb_df)\n",
    "    \n",
    "    print(f\"ATOP unique members: {len(atop_members)}\")\n",
    "    print(f\"ICB unique cracids: {len(icb_cracids)}\")\n",
    "    \n",
    "    # Convert to sets for analysis\n",
    "    atop_set = set(atop_members)\n",
    "    icb_set = set(icb_cracids)\n",
    "    \n",
    "    # Find overlaps and differences\n",
    "    common_members = atop_set.intersection(icb_set)\n",
    "    atop_only = atop_set - icb_set\n",
    "    icb_only = icb_set - atop_set\n",
    "    \n",
    "    print(f\"\\n=== Overlap Analysis ===\")\n",
    "    print(f\"Common members (in both datasets): {len(common_members)}\")\n",
    "    print(f\"ATOP-only members (missing from ICB): {len(atop_only)}\")\n",
    "    print(f\"ICB-only cracids (missing from ATOP): {len(icb_only)}\")\n",
    "    \n",
    "    # Coverage percentages\n",
    "    atop_coverage = len(common_members) / len(atop_set) * 100 if atop_set else 0\n",
    "    icb_coverage = len(common_members) / len(icb_set) * 100 if icb_set else 0\n",
    "    \n",
    "    print(f\"\\n=== Coverage Analysis ===\")\n",
    "    print(f\"ATOP members covered by ICB: {atop_coverage:.1f}%\")\n",
    "    print(f\"ICB cracids covered by ATOP: {icb_coverage:.1f}%\")\n",
    "    \n",
    "    # Show problematic cases\n",
    "    if atop_only:\n",
    "        print(f\"\\n=== ATOP Members Missing from ICB (PROBLEM) ===\")\n",
    "        print(f\"These {len(atop_only)} countries appear in ATOP but not ICB:\")\n",
    "        atop_only_sorted = sorted(list(atop_only))\n",
    "        \n",
    "        # Show first 20, then summary if more\n",
    "        if len(atop_only_sorted) <= 20:\n",
    "            print(f\"All missing: {atop_only_sorted}\")\n",
    "        else:\n",
    "            print(f\"First 20: {atop_only_sorted[:20]}\")\n",
    "            print(f\"... and {len(atop_only_sorted)-20} more\")\n",
    "        \n",
    "        # Check which alliances are affected\n",
    "        print(f\"\\nAlliances containing missing members:\")\n",
    "        affected_alliances = 0\n",
    "        for idx, row in atop_processed_df.iterrows():\n",
    "            if row['Members_List']:\n",
    "                members = [int(m) for m in row['Members_List'].split(';')]\n",
    "                missing_in_alliance = [m for m in members if m in atop_only]\n",
    "                if missing_in_alliance:\n",
    "                    affected_alliances += 1\n",
    "                    if affected_alliances <= 10:  # Show first 10\n",
    "                        print(f\"  Alliance {row['Alliance_ID']}: missing members {missing_in_alliance}\")\n",
    "        \n",
    "        if affected_alliances > 10:\n",
    "            print(f\"  ... and {affected_alliances-10} more affected alliances\")\n",
    "        \n",
    "        print(f\"Total affected alliances: {affected_alliances}/{len(atop_processed_df)}\")\n",
    "    \n",
    "    if icb_only:\n",
    "        print(f\"\\n=== ICB Cracids Missing from ATOP (INFO) ===\")\n",
    "        print(f\"These {len(icb_only)} countries appear in ICB but not ATOP:\")\n",
    "        icb_only_sorted = sorted(list(icb_only))\n",
    "        \n",
    "        if len(icb_only_sorted) <= 20:\n",
    "            print(f\"All missing: {icb_only_sorted}\")\n",
    "        else:\n",
    "            print(f\"First 20: {icb_only_sorted[:20]}\")\n",
    "            print(f\"... and {len(icb_only_sorted)-20} more\")\n",
    "    \n",
    "    # Summary recommendations\n",
    "    print(f\"\\n=== Recommendations ===\")\n",
    "    if len(atop_only) == 0:\n",
    "        print(\"✓ GOOD: All ATOP members exist in ICB - location mapping will work perfectly\")\n",
    "    else:\n",
    "        print(f\"⚠️  ISSUE: {len(atop_only)} ATOP members missing from ICB\")\n",
    "        print(f\"   → These members cannot be mapped to locations using ICB data\")\n",
    "        print(f\"   → {affected_alliances} alliances affected\")\n",
    "        print(f\"   → Consider: manual mapping, alternative data source, or exclude these members\")\n",
    "    \n",
    "    if len(icb_only) > 0:\n",
    "        print(f\"ℹ️  INFO: {len(icb_only)} ICB countries not in any alliance (normal)\")\n",
    "    \n",
    "    # Return analysis results for further processing\n",
    "    return {\n",
    "        'atop_members': atop_members,\n",
    "        'icb_cracids': icb_cracids,\n",
    "        'common_members': sorted(list(common_members)),\n",
    "        'atop_only': sorted(list(atop_only)),\n",
    "        'icb_only': sorted(list(icb_only)),\n",
    "        'atop_coverage': atop_coverage,\n",
    "        'icb_coverage': icb_coverage,\n",
    "        'affected_alliances': affected_alliances if atop_only else 0\n",
    "    }\n",
    "\n",
    "def create_member_mapping_report(analysis_results):\n",
    "    \"\"\"\n",
    "    Create a detailed report for member mapping issues\n",
    "    \"\"\"\n",
    "    if not analysis_results['atop_only']:\n",
    "        print(\"\\n=== No mapping issues found! ===\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n=== Detailed Member Mapping Report ===\")\n",
    "    \n",
    "    # Create a DataFrame of problematic members\n",
    "    problematic_df = pd.DataFrame({\n",
    "        'Missing_Member_ID': analysis_results['atop_only'],\n",
    "        'In_ATOP': True,\n",
    "        'In_ICB': False,\n",
    "        'Location_Mappable': False\n",
    "    })\n",
    "    \n",
    "    print(f\"Problematic members summary:\")\n",
    "    print(problematic_df.head(10))\n",
    "    \n",
    "    # Save to CSV for manual review\n",
    "    problematic_df.to_csv(\"atop_icb_missing_members.csv\", index=False)\n",
    "    print(f\"\\nSaved detailed report to 'atop_icb_missing_members.csv'\")\n",
    "    \n",
    "    return problematic_df\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Running ATOP-ICB member consistency check...\")\n",
    "\n",
    "# Ensure we have the processed datasets\n",
    "if 'atop_processed' not in globals():\n",
    "    print(\"Warning: atop_processed not found. Please run the ATOP processing code first.\")\n",
    "if 'icb_df' not in globals():\n",
    "    print(\"Warning: icb_df not found. Please load the ICB dataset first.\")\n",
    "\n",
    "# Perform the analysis\n",
    "analysis_results = analyze_member_consistency(atop_processed, icb_df)\n",
    "\n",
    "# Create detailed report if there are issues\n",
    "mapping_report = create_member_mapping_report(analysis_results)\n",
    "\n",
    "# Additional analysis: Show some statistics\n",
    "print(f\"\\n=== Additional Statistics ===\")\n",
    "print(f\"ATOP member ID range: {min(analysis_results['atop_members'])} to {max(analysis_results['atop_members'])}\")\n",
    "print(f\"ICB cracid range: {min(analysis_results['icb_cracids'])} to {max(analysis_results['icb_cracids'])}\")\n",
    "\n",
    "# Sample of common members\n",
    "print(f\"\\nSample common members: {analysis_results['common_members'][:10]}...\")\n",
    "\n",
    "# Check if there's a pattern in missing members\n",
    "if analysis_results['atop_only']:\n",
    "    missing_members = analysis_results['atop_only']\n",
    "    print(f\"\\nMissing member patterns:\")\n",
    "    print(f\"  Smallest missing: {min(missing_members)}\")\n",
    "    print(f\"  Largest missing: {max(missing_members)}\")\n",
    "    \n",
    "    # Check for potential coding differences\n",
    "    high_missing = [m for m in missing_members if m > 1000]\n",
    "    if high_missing:\n",
    "        print(f\"  High-numbered missing codes (>1000): {len(high_missing)} - might be different coding system\")\n",
    "        print(f\"    Examples: {high_missing[:5]}\")\n",
    "\n",
    "print(f\"\\n=== Next Steps ===\")\n",
    "if analysis_results['atop_only']:\n",
    "    print(\"1. Review 'atop_icb_missing_members.csv' for manual inspection\")\n",
    "    print(\"2. Consider alternative country code mappings (COW, ISO, etc.)\")\n",
    "    print(\"3. For location mapping, you may need to:\")\n",
    "    print(\"   - Exclude alliances with unmappable members\")\n",
    "    print(\"   - Use alternative geographic data sources\")\n",
    "    print(\"   - Manually map missing country codes\")\n",
    "else:\n",
    "    print(\"1. Proceed with confidence - all ATOP members can be mapped!\")\n",
    "    print(\"2. Use the cracid_actloc mapping for location assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "604a4686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mapping Caribbean/Central American Countries ===\n",
      "Target location: 42 (Central America including Caribbean)\n",
      "\n",
      "Countries found in ICB dataset: 0\n",
      "\n",
      "Countries NOT found in ICB dataset: 10\n",
      "  31: Bahamas\n",
      "  51: Jamaica\n",
      "  52: Trinidad and Tobago\n",
      "  53: Barbados\n",
      "  54: Dominica\n",
      "  56: St. Lucia\n",
      "  57: St. Vincent and the Grenadines\n",
      "  58: Antigua & Barbuda\n",
      "  60: St. Kitts and Nevis\n",
      "  80: Belize\n",
      "\n",
      "=== Current actloc values ===\n",
      "\n",
      "=== Updating actloc values ===\n",
      "\n",
      "Total rows updated: 0\n",
      "\n",
      "=== Updating cracid_actloc mapping ===\n",
      "\n",
      "=== Verification ===\n",
      "\n",
      "=== Updating Master Dataset ===\n",
      "Regenerating Actor_Locations using updated mapping...\n",
      "✓ Master dataset Actor_Locations updated\n",
      "\n",
      "=== Summary ===\n",
      "Successfully mapped 0 Caribbean/Central American countries to actloc 42\n",
      "Countries mapped: []\n",
      "Countries not found in dataset: ['Bahamas', 'Jamaica', 'Trinidad and Tobago', 'Barbados', 'Dominica', 'St. Lucia', 'St. Vincent and the Grenadines', 'Antigua & Barbuda', 'St. Kitts and Nevis', 'Belize']\n",
      "All mapped countries are now assigned to: Central America (including Caribbean countries)\n"
     ]
    }
   ],
   "source": [
    "# Map unmapped Caribbean/Central American countries to Actor_Location 42\n",
    "caribbean_mapping = {\n",
    "    31: \"Bahamas\",\n",
    "    51: \"Jamaica\", \n",
    "    52: \"Trinidad and Tobago\",\n",
    "    53: \"Barbados\",\n",
    "    54: \"Dominica\",\n",
    "    56: \"St. Lucia\",\n",
    "    57: \"St. Vincent and the Grenadines\", \n",
    "    58: \"Antigua & Barbuda\",\n",
    "    60: \"St. Kitts and Nevis\",\n",
    "    80: \"Belize\"\n",
    "}\n",
    "\n",
    "target_actloc = 42  # Central America (including Caribbean countries)\n",
    "\n",
    "print(\"=== Mapping Caribbean/Central American Countries ===\")\n",
    "print(f\"Target location: {target_actloc} (Central America including Caribbean)\")\n",
    "\n",
    "# Check which of these countries exist in ICB dataset\n",
    "existing_cracids = set(icb_df['cracid'].dropna().astype(int))\n",
    "caribbean_cracids = set(caribbean_mapping.keys())\n",
    "\n",
    "found_cracids = caribbean_cracids.intersection(existing_cracids)\n",
    "missing_cracids = caribbean_cracids - existing_cracids\n",
    "\n",
    "print(f\"\\nCountries found in ICB dataset: {len(found_cracids)}\")\n",
    "for cracid in sorted(found_cracids):\n",
    "    print(f\"  {cracid}: {caribbean_mapping[cracid]}\")\n",
    "\n",
    "if missing_cracids:\n",
    "    print(f\"\\nCountries NOT found in ICB dataset: {len(missing_cracids)}\")\n",
    "    for cracid in sorted(missing_cracids):\n",
    "        print(f\"  {cracid}: {caribbean_mapping[cracid]}\")\n",
    "\n",
    "# Check current actloc values for these countries\n",
    "print(f\"\\n=== Current actloc values ===\")\n",
    "for cracid in sorted(found_cracids):\n",
    "    current_actlocs = icb_df[icb_df['cracid'] == cracid]['actloc'].dropna().unique()\n",
    "    if len(current_actlocs) > 0:\n",
    "        print(f\"  {cracid} ({caribbean_mapping[cracid]}): currently {list(current_actlocs)}\")\n",
    "    else:\n",
    "        print(f\"  {cracid} ({caribbean_mapping[cracid]}): currently NO actloc values\")\n",
    "\n",
    "# Update the ICB dataframe actloc values\n",
    "print(f\"\\n=== Updating actloc values ===\")\n",
    "rows_updated = 0\n",
    "\n",
    "for cracid in found_cracids:\n",
    "    # Update all rows for this cracid to have actloc = 42\n",
    "    mask = icb_df['cracid'] == cracid\n",
    "    rows_for_country = mask.sum()\n",
    "    \n",
    "    # Check if country already has actloc = 42\n",
    "    current_actlocs = icb_df[mask]['actloc'].dropna().unique()\n",
    "    \n",
    "    if target_actloc in current_actlocs:\n",
    "        print(f\"  {cracid} ({caribbean_mapping[cracid]}): already has actloc {target_actloc}\")\n",
    "    else:\n",
    "        icb_df.loc[mask, 'actloc'] = target_actloc\n",
    "        rows_updated += rows_for_country\n",
    "        print(f\"  {cracid} ({caribbean_mapping[cracid]}): updated {rows_for_country} rows to actloc {target_actloc}\")\n",
    "\n",
    "print(f\"\\nTotal rows updated: {rows_updated}\")\n",
    "\n",
    "# Update the cracid_actloc mapping dictionary\n",
    "print(f\"\\n=== Updating cracid_actloc mapping ===\")\n",
    "if 'cracid_actloc_dict' not in globals():\n",
    "    print(\"Creating new cracid_actloc mapping...\")\n",
    "    cracid_actloc_series = (\n",
    "        icb_df.groupby(\"cracid\")[\"actloc\"]\n",
    "              .apply(lambda x: x.dropna().unique().tolist())\n",
    "    )\n",
    "    cracid_actloc_dict = cracid_actloc_series.to_dict()\n",
    "\n",
    "# Update the mapping for Caribbean countries\n",
    "for cracid in found_cracids:\n",
    "    # Convert actloc values to integers for consistency\n",
    "    if cracid in cracid_actloc_dict:\n",
    "        current_actlocs = [int(float(x)) for x in cracid_actloc_dict[cracid] if pd.notna(x)]\n",
    "        if target_actloc not in current_actlocs:\n",
    "            current_actlocs.append(target_actloc)\n",
    "            cracid_actloc_dict[cracid] = sorted(current_actlocs)\n",
    "            print(f\"  Updated mapping for {cracid} ({caribbean_mapping[cracid]}): {cracid_actloc_dict[cracid]}\")\n",
    "        else:\n",
    "            print(f\"  {cracid} ({caribbean_mapping[cracid]}): already mapped to {target_actloc}\")\n",
    "    else:\n",
    "        cracid_actloc_dict[cracid] = [target_actloc]\n",
    "        print(f\"  Added new mapping for {cracid} ({caribbean_mapping[cracid]}): [{target_actloc}]\")\n",
    "\n",
    "# Verify the updates\n",
    "print(f\"\\n=== Verification ===\")\n",
    "for cracid in sorted(found_cracids):\n",
    "    # Check ICB dataframe\n",
    "    icb_actlocs = sorted(icb_df[icb_df['cracid'] == cracid]['actloc'].dropna().unique())\n",
    "    # Check mapping dictionary  \n",
    "    dict_actlocs = sorted(cracid_actloc_dict.get(cracid, []))\n",
    "    \n",
    "    print(f\"  {cracid} ({caribbean_mapping[cracid]}):\")\n",
    "    print(f\"    ICB dataframe: {icb_actlocs}\")\n",
    "    print(f\"    Mapping dict:  {dict_actlocs}\")\n",
    "    \n",
    "    if target_actloc in icb_actlocs and target_actloc in dict_actlocs:\n",
    "        print(f\"    ✓ Successfully mapped to {target_actloc}\")\n",
    "    else:\n",
    "        print(f\"    ⚠️ Mapping incomplete\")\n",
    "\n",
    "# Update master dataset if it exists\n",
    "if 'master_df' in globals():\n",
    "    print(f\"\\n=== Updating Master Dataset ===\")\n",
    "    print(\"Regenerating Actor_Locations using updated mapping...\")\n",
    "    \n",
    "    # Regenerate Actor_Locations column\n",
    "    updated_actor_locations = []\n",
    "    for idx, row in master_df.iterrows():\n",
    "        crisis_actlocs = set()\n",
    "        actor_list = row['Actor_List'].split(';') if row['Actor_List'] else []\n",
    "        \n",
    "        for actor_str in actor_list:\n",
    "            if actor_str:\n",
    "                cracid = int(actor_str)\n",
    "                if cracid in cracid_actloc_dict:\n",
    "                    for actloc in cracid_actloc_dict[cracid]:\n",
    "                        crisis_actlocs.add(int(actloc))\n",
    "        \n",
    "        actor_locations = \";\".join([str(x) for x in sorted(crisis_actlocs)])\n",
    "        updated_actor_locations.append(actor_locations)\n",
    "    \n",
    "    master_df['Actor_Locations'] = updated_actor_locations\n",
    "    print(\"✓ Master dataset Actor_Locations updated\")\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Successfully mapped {len(found_cracids)} Caribbean/Central American countries to actloc {target_actloc}\")\n",
    "print(f\"Countries mapped: {[caribbean_mapping[c] for c in sorted(found_cracids)]}\")\n",
    "if missing_cracids:\n",
    "    print(f\"Countries not found in dataset: {[caribbean_mapping[c] for c in sorted(missing_cracids)]}\")\n",
    "print(f\"All mapped countries are now assigned to: Central America (including Caribbean countries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "903ae635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Country to Location Dictionary ===\n",
      "Total countries mapped: 147\n",
      "\n",
      "Country Code (cracid) → Location Code(s) (actloc):\n",
      "--------------------------------------------------\n",
      "    2 → 41.0\n",
      "   20 → 41.0\n",
      "   40 → 42.0\n",
      "   41 → 42.0\n",
      "   42 → 42.0\n",
      "   55 → 42.0\n",
      "   70 → 42.0\n",
      "   90 → 42.0\n",
      "   91 → 42.0\n",
      "   92 → 42.0\n",
      "   93 → 42.0\n",
      "   94 → 42.0\n",
      "   95 → 42.0\n",
      "  100 → 43.0\n",
      "  101 → 43.0\n",
      "  110 → 43.0\n",
      "  130 → 43.0\n",
      "  135 → 43.0\n",
      "  145 → 43.0\n",
      "  150 → 43.0\n",
      "  155 → 43.0\n",
      "  160 → 43.0\n",
      "  200 → 34.0\n",
      "  210 → 33.0\n",
      "  211 → 33.0\n",
      "  212 → 33.0\n",
      "  219 → 33.0\n",
      "  220 → 33.0\n",
      "  225 → 32.0\n",
      "  230 → 35.0\n",
      "  235 → 35.0\n",
      "  255 → 32.0\n",
      "  260 → 32.0\n",
      "  265 → 32.0\n",
      "  290 → 31.0\n",
      "  305 → 32.0\n",
      "  310 → 31.0\n",
      "  315 → 31.0\n",
      "  325 → 35.0\n",
      "  338 → 35.0\n",
      "  339 → 35.0\n",
      "  344 → 35.0\n",
      "  345 → 35.0\n",
      "  346 → 35.0\n",
      "  347 → 35.0\n",
      "  349 → 35.0\n",
      "  350 → 35.0\n",
      "  352 → 10.0\n",
      "  355 → 31.0\n",
      "  360 → 31.0\n",
      "  365 → 30.0\n",
      "  366 → 34.0\n",
      "  367 → 34.0\n",
      "  368 → 34.0\n",
      "  369 → 31.0\n",
      "  370 → 31.0\n",
      "  371 → 10.0\n",
      "  372 → 10.0\n",
      "  373 → 10.0\n",
      "  375 → 34.0\n",
      "  380 → 34.0\n",
      "  385 → 34.0\n",
      "  390 → 34.0\n",
      "  395 → 34.0\n",
      "  420 → 20.0\n",
      "  432 → 20.0\n",
      "  433 → 20.0\n",
      "  434 → 20.0\n",
      "  435 → 20.0\n",
      "  436 → 20.0\n",
      "  437 → 20.0\n",
      "  438 → 20.0\n",
      "  439 → 20.0\n",
      "  451 → 20.0\n",
      "  452 → 20.0\n",
      "  461 → 20.0\n",
      "  471 → 24.0\n",
      "  475 → 20.0\n",
      "  483 → 24.0\n",
      "  490 → 24.0\n",
      "  500 → 22.0\n",
      "  501 → 22.0\n",
      "  510 → 22.0\n",
      "  516 → 22.0\n",
      "  517 → 22.0\n",
      "  520 → 22.0\n",
      "  522 → 22.0\n",
      "  530 → 22.0\n",
      "  531 → 22.0\n",
      "  540 → 24.0\n",
      "  541 → 22.0\n",
      "  551 → 22.0\n",
      "  552 → 22.0\n",
      "  553 → 22.0\n",
      "  560 → 23.0\n",
      "  565 → 23.0\n",
      "  570 → 23.0\n",
      "  571 → 23.0\n",
      "  600 → 21.0\n",
      "  615 → 21.0\n",
      "  616 → 21.0\n",
      "  620 → 21.0\n",
      "  625 → 21.0\n",
      "  626 → 22.0\n",
      "  630 → 13.0\n",
      "  640 → 10.0\n",
      "  645 → 15.0\n",
      "  651 → 21.0\n",
      "  652 → 15.0\n",
      "  660 → 15.0\n",
      "  663 → 15.0\n",
      "  666 → 15.0\n",
      "  670 → 15.0\n",
      "  671 → 15.0\n",
      "  672 → 15.0\n",
      "  678 → 15.0\n",
      "  680 → 15.0\n",
      "  690 → 15.0\n",
      "  692 → 15.0\n",
      "  694 → 15.0\n",
      "  696 → 15.0\n",
      "  698 → 15.0\n",
      "  700 → 13.0\n",
      "  702 → 9.0\n",
      "  703 → 9.0\n",
      "  710 → 11.0\n",
      "  713 → 11.0\n",
      "  731 → 11.0\n",
      "  732 → 11.0\n",
      "  740 → 11.0\n",
      "  750 → 13.0\n",
      "  770 → 13.0\n",
      "  771 → 13.0\n",
      "  775 → 12.0\n",
      "  780 → 13.0\n",
      "  800 → 12.0\n",
      "  811 → 12.0\n",
      "  812 → 12.0\n",
      "  816 → 12.0\n",
      "  817 → 12.0\n",
      "  820 → 12.0\n",
      "  840 → 12.0\n",
      "  850 → 12.0\n",
      "  900 → 51.0\n",
      "  910 → 51.0\n",
      "  920 → 51.0\n",
      "  940 → 51.0\n",
      "\n",
      "=== Statistics ===\n",
      "Countries with single location: 147\n",
      "Countries with multiple locations: 0\n",
      "\n",
      "=== Location Code Distribution ===\n",
      "Location codes (frequency):\n",
      "  Location 9.0: 2 countries\n",
      "  Location 10.0: 5 countries\n",
      "  Location 11.0: 5 countries\n",
      "  Location 12.0: 9 countries\n",
      "  Location 13.0: 6 countries\n",
      "  Location 15.0: 15 countries\n",
      "  Location 20.0: 13 countries\n",
      "  Location 21.0: 6 countries\n",
      "  Location 22.0: 14 countries\n",
      "  Location 23.0: 4 countries\n",
      "  Location 24.0: 4 countries\n",
      "  Location 30.0: 1 countries\n",
      "  Location 31.0: 7 countries\n",
      "  Location 32.0: 5 countries\n",
      "  Location 33.0: 5 countries\n",
      "  Location 34.0: 9 countries\n",
      "  Location 35.0: 11 countries\n",
      "  Location 41.0: 2 countries\n",
      "  Location 42.0: 11 countries\n",
      "  Location 43.0: 9 countries\n",
      "  Location 51.0: 4 countries\n"
     ]
    }
   ],
   "source": [
    "# Display the existing cracid_actloc mapping dictionary\n",
    "print(\"=== Country to Location Dictionary ===\")\n",
    "\n",
    "if 'cracid_actloc_dict' in globals():\n",
    "    print(f\"Total countries mapped: {len(cracid_actloc_dict)}\")\n",
    "    \n",
    "    # Show the mapping in a readable format\n",
    "    print(f\"\\nCountry Code (cracid) → Location Code(s) (actloc):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for cracid in sorted(cracid_actloc_dict.keys()):\n",
    "        locations = cracid_actloc_dict[cracid]\n",
    "        if len(locations) == 1:\n",
    "            print(f\"  {cracid:3d} → {locations[0]}\")\n",
    "        else:\n",
    "            print(f\"  {cracid:3d} → {locations} (multiple locations)\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\n=== Statistics ===\")\n",
    "    single_location = sum(1 for locs in cracid_actloc_dict.values() if len(locs) == 1)\n",
    "    multi_location = sum(1 for locs in cracid_actloc_dict.values() if len(locs) > 1)\n",
    "    \n",
    "    print(f\"Countries with single location: {single_location}\")\n",
    "    print(f\"Countries with multiple locations: {multi_location}\")\n",
    "    \n",
    "    if multi_location > 0:\n",
    "        print(f\"\\nCountries with multiple locations:\")\n",
    "        for cracid in sorted(cracid_actloc_dict.keys()):\n",
    "            locations = cracid_actloc_dict[cracid]\n",
    "            if len(locations) > 1:\n",
    "                print(f\"  {cracid}: {locations}\")\n",
    "    \n",
    "    # Show location code distribution\n",
    "    print(f\"\\n=== Location Code Distribution ===\")\n",
    "    all_locations = []\n",
    "    for locations in cracid_actloc_dict.values():\n",
    "        all_locations.extend(locations)\n",
    "    \n",
    "    from collections import Counter\n",
    "    location_counts = Counter(all_locations)\n",
    "    \n",
    "    print(\"Location codes (frequency):\")\n",
    "    for loc_code in sorted(location_counts.keys()):\n",
    "        count = location_counts[loc_code]\n",
    "        print(f\"  Location {loc_code}: {count} countries\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ cracid_actloc_dict not found!\")\n",
    "    print(\"Creating it from ICB dataset...\")\n",
    "    \n",
    "    if 'icb_df' in globals():\n",
    "        # Create the mapping\n",
    "        cracid_actloc_series = (\n",
    "            icb_df.groupby(\"cracid\")[\"actloc\"]\n",
    "                  .apply(lambda x: [int(float(val)) for val in x.dropna().unique() if pd.notna(val)])\n",
    "        )\n",
    "        cracid_actloc_dict = cracid_actloc_series.to_dict()\n",
    "        \n",
    "        print(f\"✓ Created cracid_actloc_dict with {len(cracid_actloc_dict)} countries\")\n",
    "        \n",
    "        # Now display it\n",
    "        print(f\"\\nCountry Code (cracid) → Location Code(s) (actloc):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for cracid in sorted(cracid_actloc_dict.keys()):\n",
    "            locations = cracid_actloc_dict[cracid]\n",
    "            if len(locations) == 1:\n",
    "                print(f\"  {cracid:3d} → {locations[0]}\")\n",
    "            else:\n",
    "                print(f\"  {cracid:3d} → {locations} (multiple locations)\")\n",
    "    else:\n",
    "        print(\"⚠️ ICB dataset not found either! Please load icb_df first.\")\n",
    "\n",
    "# If you want to see specific countries, uncomment and modify:\n",
    "# print(f\"\\n=== Specific Country Lookups ===\")\n",
    "# lookup_countries = [2, 20, 200, 365, 630]  # Example country codes\n",
    "# for cracid in lookup_countries:\n",
    "#     if cracid in cracid_actloc_dict:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1642ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Specific Country Lookups ===\n",
      "Country 370: Location(s) []\n"
     ]
    }
   ],
   "source": [
    "# If you want to see specific countries, uncomment and modify:\n",
    "print(f\"\\n=== Specific Country Lookups ===\")\n",
    "lookup_countries = [370]  # Example country codes\n",
    "for cracid in lookup_countries:\n",
    "    if cracid in cracid_actloc_dict:\n",
    "        print(f\"Country {cracid}: Location(s) {cracid_actloc_dict[cracid]}\")\n",
    "    else:\n",
    "        print(f\"Country {cracid}: Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bab7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Belarus (370) → [31]\n",
      "After:  Belarus (370) → [31.0]\n"
     ]
    }
   ],
   "source": [
    "# Map Belarus (370) to East Europe (31)\n",
    "belarus_cracid = 370\n",
    "east_europe_actloc = 31.0\n",
    "\n",
    "print(f\"Before: Belarus (370) → {cracid_actloc_dict.get(370, 'Not found')}\")\n",
    "\n",
    "# Update ICB dataframe\n",
    "icb_df.loc[icb_df['cracid'] == belarus_cracid, 'actloc'] = east_europe_actloc\n",
    "\n",
    "# Update mapping dictionary\n",
    "cracid_actloc_dict[belarus_cracid] = [east_europe_actloc]\n",
    "\n",
    "print(f\"After:  Belarus (370) → {cracid_actloc_dict[370]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87b315b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ATOP Member Locations Sample ===\n",
      " Alliance_ID                            Members_List Member_Locations\n",
      "        1005                 200;210;220;240;245;300        33.0;34.0\n",
      "        1010                                 275;300                 \n",
      "        1015                                 300;329                 \n",
      "        1020 240;245;255;267;269;271;273;275;280;300             32.0\n",
      "        1025                                 300;329                 \n",
      "\n",
      "=== ICB Crisis and Actor Locations Sample ===\n",
      " crisno  cracid  Crisis_Location Crisis_Actor_Location\n",
      "      1     365             30.0                  30.0\n",
      "      2      93             42.0                  42.0\n",
      "      2      94             42.0                  42.0\n",
      "      3     365             30.0                  30.0\n",
      "      4     365             34.0                  30.0\n",
      "\n",
      "=== Summary ===\n",
      "ATOP: Added Member_Locations to 789 alliances\n",
      "ICB: Added Crisis_Actor_Location to 1131 rows\n",
      "ICB: Added Crisis_Location from ICB1 data\n"
     ]
    }
   ],
   "source": [
    "# 1. ATOP: Map alliance members to their locations\n",
    "def map_member_locations(members_string):\n",
    "    if not members_string:\n",
    "        return \"\"\n",
    "    \n",
    "    locations = set()\n",
    "    for member_str in members_string.split(';'):\n",
    "        member_id = int(member_str)\n",
    "        if member_id in cracid_actloc_dict:\n",
    "            locations.update(cracid_actloc_dict[member_id])\n",
    "    \n",
    "    return \";\".join([str(loc) for loc in sorted(locations)])\n",
    "\n",
    "# Apply to ATOP dataset\n",
    "atop_processed['Member_Locations'] = atop_processed['Members_List'].apply(map_member_locations)\n",
    "\n",
    "print(\"=== ATOP Member Locations Sample ===\")\n",
    "sample = atop_processed[['Alliance_ID', 'Members_List', 'Member_Locations']].head(5)\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "# 2. ICB: Add crisis location and crisis actor locations\n",
    "# Add crisis location from ICB1 if available\n",
    "if 'icb1_processed' in globals() and 'Geographic_Location' in icb1_processed.columns:\n",
    "    icb_df = icb_df.merge(\n",
    "        icb1_processed[['Crisis_ID', 'Geographic_Location']], \n",
    "        left_on='crisno', right_on='Crisis_ID', \n",
    "        how='left'\n",
    "    ).drop('Crisis_ID', axis=1)\n",
    "    icb_df.rename(columns={'Geographic_Location': 'Crisis_Location'}, inplace=True)\n",
    "\n",
    "# Add crisis actor locations\n",
    "def get_actor_location(cracid):\n",
    "    if pd.notna(cracid) and int(cracid) in cracid_actloc_dict:\n",
    "        locations = cracid_actloc_dict[int(cracid)]\n",
    "        return \";\".join([str(loc) for loc in sorted(locations)])\n",
    "    return \"\"\n",
    "\n",
    "icb_df['Crisis_Actor_Location'] = icb_df['cracid'].apply(get_actor_location)\n",
    "\n",
    "print(\"\\n=== ICB Crisis and Actor Locations Sample ===\")\n",
    "icb_sample = icb_df[['crisno', 'cracid', 'Crisis_Location', 'Crisis_Actor_Location']].head(5)\n",
    "print(icb_sample.to_string(index=False))\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"ATOP: Added Member_Locations to {len(atop_processed)} alliances\")\n",
    "print(f\"ICB: Added Crisis_Actor_Location to {len(icb_df)} rows\")\n",
    "if 'Crisis_Location' in icb_df.columns:\n",
    "    print(f\"ICB: Added Crisis_Location from ICB1 data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54790de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating crisis-alliance analysis sets...\n",
      "=== Creating Crisis-Alliance Analysis Set ===\n",
      "Crises (C): 512\n",
      "Alliances (A): 789\n",
      "Cartesian product size: 403,968\n",
      "\n",
      "=== Analysis Results ===\n",
      "Total crisis-alliance pairs (C × A): 403,968\n",
      "Temporally active pairs (δ_active = 1): 0 (0.0%)\n",
      "Member overlap pairs (δ_member = 1): 27,842 (6.9%)\n",
      "Analysis set pairs (Ω): 0 (0.0%)\n",
      "Pairs meeting both conditions: 0\n",
      "\n",
      "=== Saving Datasets ===\n",
      "✓ Full Cartesian product saved: 403,968 rows\n",
      "✓ Analysis set (Ω) saved: 0 rows\n",
      "\n",
      "=== Final Summary ===\n",
      "Created full Cartesian product C × A with 403,968 pairs\n",
      "Identified analysis subset Ω with 0 pairs\n",
      "Analysis subset represents 0.000% of all possible pairs\n",
      "\n",
      "=== Usage Notes ===\n",
      "• full_cartesian_df: Complete C × A with all indicators\n",
      "• analysis_set_df: Filtered subset Ω for analysis\n",
      "• Use analysis_set_df for statistical modeling\n",
      "• Use full_cartesian_df for exploring non-matches\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "def parse_date_safely(date_str):\n",
    "    \"\"\"Parse date string to datetime, handle None/NaN\"\"\"\n",
    "    if pd.isna(date_str) or date_str is None:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def check_temporal_overlap(crisis_start, crisis_end, alliance_start, alliance_end):\n",
    "    \"\"\"\n",
    "    Check if alliance was active during crisis: δ_active\n",
    "    Returns 1 if start(a) ≤ end(c) AND end(a) ≥ start(c)\n",
    "    \"\"\"\n",
    "    # Convert to datetime objects\n",
    "    c_start = parse_date_safely(crisis_start)\n",
    "    c_end = parse_date_safely(crisis_end)\n",
    "    a_start = parse_date_safely(alliance_start)\n",
    "    a_end = parse_date_safely(alliance_end)\n",
    "    \n",
    "    # Handle missing dates\n",
    "    if c_start is None or a_start is None:\n",
    "        return 0  # Cannot determine overlap without start dates\n",
    "    \n",
    "    # If alliance has no end date, assume it's still active (set to far future)\n",
    "    if a_end is None:\n",
    "        a_end = pd.to_datetime('2030-12-31')  # Far future date\n",
    "    \n",
    "    # If crisis has no end date, use start date as proxy\n",
    "    if c_end is None:\n",
    "        c_end = c_start\n",
    "    \n",
    "    # Check overlap: start(a) ≤ end(c) AND end(a) ≥ start(c)\n",
    "    overlap = (a_start <= c_end) and (a_end >= c_start)\n",
    "    return 1 if overlap else 0\n",
    "\n",
    "def check_member_overlap(crisis_actors, alliance_members):\n",
    "    \"\"\"\n",
    "    Check if any crisis actors are alliance members: δ_member\n",
    "    Returns 1 if actors(c) ∩ members(a) ≠ ∅\n",
    "    \"\"\"\n",
    "    if not crisis_actors or not alliance_members:\n",
    "        return 0\n",
    "    \n",
    "    # Parse semicolon-separated strings to sets of integers\n",
    "    try:\n",
    "        crisis_set = set(int(x) for x in crisis_actors.split(';') if x.strip())\n",
    "        member_set = set(int(x) for x in alliance_members.split(';') if x.strip())\n",
    "        \n",
    "        # Check intersection\n",
    "        intersection = crisis_set.intersection(member_set)\n",
    "        return 1 if len(intersection) > 0 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def create_crisis_alliance_cartesian_product():\n",
    "    \"\"\"\n",
    "    Create the Cartesian product C × A with computed indicators\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Creating Crisis-Alliance Analysis Set ===\")\n",
    "    \n",
    "    # Prepare crisis data (C)\n",
    "    if 'master_df' in globals() and 'icb1_processed' in globals():\n",
    "        # Merge master crisis data with temporal data\n",
    "        crises = master_df.merge(\n",
    "            icb1_processed[['Crisis_ID', 'Start_Date', 'End_Date']], \n",
    "            on='Crisis_ID', \n",
    "            how='left'\n",
    "        )\n",
    "    elif 'icb1_processed' in globals():\n",
    "        # Use ICB1 data directly\n",
    "        crises = icb1_processed.copy()\n",
    "        crises['Actor_List'] = \"\"  # Empty if no master data\n",
    "    else:\n",
    "        print(\"Error: No crisis data available. Need icb1_processed at minimum.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Prepare alliance data (A)\n",
    "    if 'atop_processed' not in globals():\n",
    "        print(\"Error: No alliance data available. Need atop_processed.\")\n",
    "        return None, None\n",
    "    \n",
    "    alliances = atop_processed.copy()\n",
    "    \n",
    "    print(f\"Crises (C): {len(crises)}\")\n",
    "    print(f\"Alliances (A): {len(alliances)}\")\n",
    "    print(f\"Cartesian product size: {len(crises) * len(alliances):,}\")\n",
    "    \n",
    "    # Create Cartesian product\n",
    "    cartesian_data = []\n",
    "    \n",
    "    for crisis_idx, crisis in crises.iterrows():\n",
    "        for alliance_idx, alliance in alliances.iterrows():\n",
    "            \n",
    "            # Basic information\n",
    "            pair_data = {\n",
    "                # Crisis information\n",
    "                'Crisis_ID': crisis.get('Crisis_ID'),\n",
    "                'Crisis_Name': crisis.get('Crisis_Name', ''),\n",
    "                'Crisis_Start': crisis.get('Start_Date'),\n",
    "                'Crisis_End': crisis.get('End_Date'),\n",
    "                'Crisis_Actors': crisis.get('Actor_List', ''),\n",
    "                'Crisis_Actor_Locations': crisis.get('Actor_Locations', ''),\n",
    "                \n",
    "                # Alliance information  \n",
    "                'Alliance_ID': alliance.get('Alliance_ID'),\n",
    "                'Alliance_Name': alliance.get('Alliance_Name', ''),\n",
    "                'Alliance_Start': alliance.get('Alliance_Start'),\n",
    "                'Alliance_End': alliance.get('Alliance_End'),\n",
    "                'Alliance_Members': alliance.get('Members_List', ''),\n",
    "                'Alliance_Type': alliance.get('Alliance_Type', ''),\n",
    "                'Alliance_N_Members': alliance.get('N_Members', 0),\n",
    "                'Member_Locations': alliance.get('Member_Locations', ''),\n",
    "            }\n",
    "            \n",
    "            # Compute temporal activation indicator\n",
    "            delta_active = check_temporal_overlap(\n",
    "                crisis.get('Start_Date'),\n",
    "                crisis.get('End_Date'), \n",
    "                alliance.get('Alliance_Start'),\n",
    "                alliance.get('Alliance_End')\n",
    "            )\n",
    "            pair_data['delta_active'] = delta_active\n",
    "            \n",
    "            # Compute actor-membership overlap indicator\n",
    "            delta_member = check_member_overlap(\n",
    "                crisis.get('Actor_List', ''),\n",
    "                alliance.get('Members_List', '')\n",
    "            )\n",
    "            pair_data['delta_member'] = delta_member\n",
    "            \n",
    "            # Analysis set indicator (both conditions true)\n",
    "            pair_data['in_analysis_set'] = 1 if (delta_active == 1 and delta_member == 1) else 0\n",
    "            \n",
    "            cartesian_data.append(pair_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    full_cartesian_df = pd.DataFrame(cartesian_data)\n",
    "    \n",
    "    # Create analysis subset Ω\n",
    "    analysis_set_df = full_cartesian_df[full_cartesian_df['in_analysis_set'] == 1].copy()\n",
    "    \n",
    "    return full_cartesian_df, analysis_set_df\n",
    "\n",
    "def analyze_crisis_alliance_results(full_df, analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze the results and provide summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Analysis Results ===\")\n",
    "    \n",
    "    # Basic counts\n",
    "    total_pairs = len(full_df)\n",
    "    active_pairs = (full_df['delta_active'] == 1).sum()\n",
    "    member_pairs = (full_df['delta_member'] == 1).sum()\n",
    "    analysis_pairs = len(analysis_df)\n",
    "    \n",
    "    print(f\"Total crisis-alliance pairs (C × A): {total_pairs:,}\")\n",
    "    print(f\"Temporally active pairs (δ_active = 1): {active_pairs:,} ({active_pairs/total_pairs*100:.1f}%)\")\n",
    "    print(f\"Member overlap pairs (δ_member = 1): {member_pairs:,} ({member_pairs/total_pairs*100:.1f}%)\")\n",
    "    print(f\"Analysis set pairs (Ω): {analysis_pairs:,} ({analysis_pairs/total_pairs*100:.1f}%)\")\n",
    "    \n",
    "    # Overlap analysis\n",
    "    both_conditions = ((full_df['delta_active'] == 1) & (full_df['delta_member'] == 1)).sum()\n",
    "    print(f\"Pairs meeting both conditions: {both_conditions:,}\")\n",
    "    \n",
    "    if analysis_pairs > 0:\n",
    "        print(f\"\\n=== Analysis Set (Ω) Characteristics ===\")\n",
    "        \n",
    "        # Unique crises and alliances in analysis set\n",
    "        unique_crises = analysis_df['Crisis_ID'].nunique()\n",
    "        unique_alliances = analysis_df['Alliance_ID'].nunique()\n",
    "        \n",
    "        print(f\"Unique crises involved: {unique_crises}\")\n",
    "        print(f\"Unique alliances involved: {unique_alliances}\")\n",
    "        \n",
    "        # Most frequent crises and alliances\n",
    "        print(f\"\\nTop 5 most frequent crises in analysis set:\")\n",
    "        crisis_counts = analysis_df['Crisis_ID'].value_counts().head(5)\n",
    "        for crisis_id, count in crisis_counts.items():\n",
    "            crisis_name = analysis_df[analysis_df['Crisis_ID'] == crisis_id]['Crisis_Name'].iloc[0]\n",
    "            print(f\"  Crisis {crisis_id} ({crisis_name}): {count} alliance pairs\")\n",
    "        \n",
    "        print(f\"\\nTop 5 most frequent alliances in analysis set:\")\n",
    "        alliance_counts = analysis_df['Alliance_ID'].value_counts().head(5)\n",
    "        for alliance_id, count in alliance_counts.items():\n",
    "            alliance_name = analysis_df[analysis_df['Alliance_ID'] == alliance_id]['Alliance_Name'].iloc[0]\n",
    "            print(f\"  Alliance {alliance_id} ({alliance_name}): {count} crisis pairs\")\n",
    "        \n",
    "        # Sample analysis set entries\n",
    "        print(f\"\\n=== Sample Analysis Set Entries ===\")\n",
    "        sample_cols = ['Crisis_ID', 'Crisis_Name', 'Alliance_ID', 'Alliance_Name', \n",
    "                      'Crisis_Start', 'Alliance_Start', 'Alliance_End']\n",
    "        available_cols = [col for col in sample_cols if col in analysis_df.columns]\n",
    "        print(analysis_df[available_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Execute the analysis\n",
    "print(\"Creating crisis-alliance analysis sets...\")\n",
    "\n",
    "full_cartesian, analysis_subset = create_crisis_alliance_cartesian_product()\n",
    "\n",
    "if full_cartesian is not None and analysis_subset is not None:\n",
    "    \n",
    "    # Analyze results\n",
    "    analyze_crisis_alliance_results(full_cartesian, analysis_subset)\n",
    "    \n",
    "    # Save datasets\n",
    "    print(f\"\\n=== Saving Datasets ===\")\n",
    "    \n",
    "    # Save full Cartesian product\n",
    "    full_cartesian.to_csv(\"crisis_alliance_full_cartesian.csv\", index=False)\n",
    "    print(f\"✓ Full Cartesian product saved: {len(full_cartesian):,} rows\")\n",
    "    \n",
    "    # Save analysis subset\n",
    "    analysis_subset.to_csv(\"crisis_alliance_analysis_set.csv\", index=False)\n",
    "    print(f\"✓ Analysis set (Ω) saved: {len(analysis_subset):,} rows\")\n",
    "    \n",
    "    # Additional analysis files\n",
    "    \n",
    "    # Summary by crisis\n",
    "    if len(analysis_subset) > 0:\n",
    "        crisis_summary = (analysis_subset.groupby(['Crisis_ID', 'Crisis_Name'])\n",
    "                         .agg({\n",
    "                             'Alliance_ID': 'count',\n",
    "                             'Alliance_N_Members': 'sum',\n",
    "                             'Crisis_Start': 'first',\n",
    "                             'Crisis_Actors': 'first'\n",
    "                         })\n",
    "                         .rename(columns={'Alliance_ID': 'N_Relevant_Alliances'})\n",
    "                         .reset_index())\n",
    "        \n",
    "        crisis_summary.to_csv(\"crisis_alliance_summary_by_crisis.csv\", index=False)\n",
    "        print(f\"✓ Crisis summary saved: {len(crisis_summary)} crises\")\n",
    "        \n",
    "        # Summary by alliance\n",
    "        alliance_summary = (analysis_subset.groupby(['Alliance_ID', 'Alliance_Name'])\n",
    "                           .agg({\n",
    "                               'Crisis_ID': 'count',\n",
    "                               'Alliance_Start': 'first',\n",
    "                               'Alliance_End': 'first',\n",
    "                               'Alliance_N_Members': 'first',\n",
    "                               'Alliance_Type': 'first'\n",
    "                           })\n",
    "                           .rename(columns={'Crisis_ID': 'N_Relevant_Crises'})\n",
    "                           .reset_index())\n",
    "        \n",
    "        alliance_summary.to_csv(\"crisis_alliance_summary_by_alliance.csv\", index=False)\n",
    "        print(f\"✓ Alliance summary saved: {len(alliance_summary)} alliances\")\n",
    "    \n",
    "    print(f\"\\n=== Final Summary ===\")\n",
    "    print(f\"Created full Cartesian product C × A with {len(full_cartesian):,} pairs\")\n",
    "    print(f\"Identified analysis subset Ω with {len(analysis_subset):,} pairs\")\n",
    "    print(f\"Analysis subset represents {len(analysis_subset)/len(full_cartesian)*100:.3f}% of all possible pairs\")\n",
    "    \n",
    "    # Make datasets available in global scope\n",
    "    globals()['full_cartesian_df'] = full_cartesian\n",
    "    globals()['analysis_set_df'] = analysis_subset\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to create analysis sets. Check data availability.\")\n",
    "\n",
    "print(f\"\\n=== Usage Notes ===\")\n",
    "print(\"• full_cartesian_df: Complete C × A with all indicators\")\n",
    "print(\"• analysis_set_df: Filtered subset Ω for analysis\")\n",
    "print(\"• Use analysis_set_df for statistical modeling\")\n",
    "print(\"• Use full_cartesian_df for exploring non-matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5454da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debugging Temporal Overlap Logic ===\n",
      "\n",
      "1. Sample Crisis Dates:\n",
      "   Crisis_ID  Start_Date    End_Date\n",
      "0          1  1918-05-01  1920-04-01\n",
      "1          2  1918-05-25  1919-09-03\n",
      "2          3  1918-06-23  1919-09-27\n",
      "3          4  1918-11-18  1920-08-11\n",
      "4          5  1919-01-15  1920-07-28\n",
      "\n",
      "Crisis date types:\n",
      "Start_Date: object\n",
      "End_Date: object\n",
      "\n",
      "2. Sample Alliance Dates:\n",
      "   Alliance_ID Alliance_Start Alliance_End\n",
      "0         1005     1815-01-03   1815-02-08\n",
      "1         1010     1815-01-14   1815-06-08\n",
      "2         1015     1815-04-29   1815-06-12\n",
      "3         1020     1815-06-08   1866-06-15\n",
      "4         1025     1815-06-12   1820-07-13\n",
      "\n",
      "Alliance date types:\n",
      "Alliance_Start: object\n",
      "Alliance_End: object\n",
      "\n",
      "--- Testing: Crisis 1 vs Alliance 1005 ---\n",
      "Crisis: 1918-05-01 → 1920-04-01\n",
      "Alliance: 1815-01-03 → 1815-02-08\n",
      "Parsed crisis: 1918-05-01 00:00:00 → 1920-04-01 00:00:00\n",
      "Parsed alliance: 1815-01-03 00:00:00 → 1815-02-08 00:00:00\n",
      "Condition 1 (start(a) ≤ end(c)): 1815-01-03 00:00:00 ≤ 1920-04-01 00:00:00 = True\n",
      "Condition 2 (end(a) ≥ start(c)): 1815-02-08 00:00:00 ≥ 1918-05-01 00:00:00 = False\n",
      "RESULT: 0\n",
      "\n",
      "--- Testing: Pair 1 ---\n",
      "Crisis: 1918-05-01 → 1920-04-01\n",
      "Alliance: 1815-01-03 → 1815-02-08\n",
      "Parsed crisis: 1918-05-01 00:00:00 → 1920-04-01 00:00:00\n",
      "Parsed alliance: 1815-01-03 00:00:00 → 1815-02-08 00:00:00\n",
      "Condition 1 (start(a) ≤ end(c)): 1815-01-03 00:00:00 ≤ 1920-04-01 00:00:00 = True\n",
      "Condition 2 (end(a) ≥ start(c)): 1815-02-08 00:00:00 ≥ 1918-05-01 00:00:00 = False\n",
      "RESULT: 0\n",
      "\n",
      "--- Testing: Pair 2 ---\n",
      "Crisis: 1918-05-25 → 1919-09-03\n",
      "Alliance: 1815-01-14 → 1815-06-08\n",
      "Parsed crisis: 1918-05-25 00:00:00 → 1919-09-03 00:00:00\n",
      "Parsed alliance: 1815-01-14 00:00:00 → 1815-06-08 00:00:00\n",
      "Condition 1 (start(a) ≤ end(c)): 1815-01-14 00:00:00 ≤ 1919-09-03 00:00:00 = True\n",
      "Condition 2 (end(a) ≥ start(c)): 1815-06-08 00:00:00 ≥ 1918-05-25 00:00:00 = False\n",
      "RESULT: 0\n",
      "\n",
      "--- Testing: Pair 3 ---\n",
      "Crisis: 1918-06-23 → 1919-09-27\n",
      "Alliance: 1815-04-29 → 1815-06-12\n",
      "Parsed crisis: 1918-06-23 00:00:00 → 1919-09-27 00:00:00\n",
      "Parsed alliance: 1815-04-29 00:00:00 → 1815-06-12 00:00:00\n",
      "Condition 1 (start(a) ≤ end(c)): 1815-04-29 00:00:00 ≤ 1919-09-27 00:00:00 = True\n",
      "Condition 2 (end(a) ≥ start(c)): 1815-06-12 00:00:00 ≥ 1918-06-23 00:00:00 = False\n",
      "RESULT: 0\n",
      "\n",
      "=== Date Range Analysis ===\n",
      "Crisis date range:\n",
      "  Earliest start: 1918-05-01 00:00:00\n",
      "  Latest start: 2021-09-20 00:00:00\n",
      "  Earliest end: 1919-07-29 00:00:00\n",
      "  Latest end: 2022-12-01 00:00:00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"0-01-01\" doesn't match format \"%Y-%m-%d\", at position 98. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m alliance_dates \u001b[38;5;241m=\u001b[39m atop_processed[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_Start\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_End\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    104\u001b[0m alliance_dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_Start\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(alliance_dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_Start\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 105\u001b[0m alliance_dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_End\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43malliance_dates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlliance_End\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m alliance_start_range \u001b[38;5;241m=\u001b[39m alliance_dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_Start\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m    108\u001b[0m alliance_end_range \u001b[38;5;241m=\u001b[39m alliance_dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlliance_End\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"0-01-01\" doesn't match format \"%Y-%m-%d\", at position 98. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Fixed debugging - handle problematic dates\n",
    "print(\"=== Fixed Date Range Analysis ===\")\n",
    "\n",
    "# Handle alliance dates with errors\n",
    "if 'atop_processed' in globals():\n",
    "    alliance_dates = atop_processed[['Alliance_Start', 'Alliance_End']].copy()\n",
    "    alliance_dates['Alliance_Start'] = pd.to_datetime(alliance_dates['Alliance_Start'], errors='coerce')\n",
    "    \n",
    "    # Handle problematic end dates \n",
    "    alliance_dates['Alliance_End_Clean'] = alliance_dates['Alliance_End'].replace('0-01-01', pd.NaT)\n",
    "    alliance_dates['Alliance_End_Clean'] = pd.to_datetime(alliance_dates['Alliance_End_Clean'], errors='coerce')\n",
    "    \n",
    "    alliance_start_range = alliance_dates['Alliance_Start'].dropna()\n",
    "    alliance_end_range = alliance_dates['Alliance_End_Clean'].dropna()\n",
    "    \n",
    "    print(f\"Alliance date range:\")\n",
    "    print(f\"  Earliest start: {alliance_start_range.min()}\")\n",
    "    print(f\"  Latest start: {alliance_start_range.max()}\")\n",
    "    print(f\"  Earliest end: {alliance_end_range.min()}\")\n",
    "    print(f\"  Latest end: {alliance_end_range.max()}\")\n",
    "    print(f\"  Active alliances (no end): {alliance_dates['Alliance_End_Clean'].isna().sum()}\")\n",
    "    \n",
    "    # Show era breakdown\n",
    "    print(f\"\\n=== Alliance Era Breakdown ===\")\n",
    "    alliance_dates['Start_Year'] = alliance_dates['Alliance_Start'].dt.year\n",
    "    eras = [\n",
    "        (1815, 1900, \"19th Century\"),\n",
    "        (1900, 1945, \"Early 20th Century\"), \n",
    "        (1945, 1990, \"Cold War\"),\n",
    "        (1990, 2025, \"Post-Cold War\")\n",
    "    ]\n",
    "    \n",
    "    for start_yr, end_yr, era_name in eras:\n",
    "        count = ((alliance_dates['Start_Year'] >= start_yr) & \n",
    "                (alliance_dates['Start_Year'] < end_yr)).sum()\n",
    "        print(f\"  {era_name} ({start_yr}-{end_yr}): {count} alliances\")\n",
    "\n",
    "# Find alliances that could potentially overlap with crises\n",
    "print(f\"\\n=== Potential Overlap Analysis ===\")\n",
    "\n",
    "# Get crisis era\n",
    "if 'icb1_processed' in globals():\n",
    "    crisis_dates = pd.to_datetime(icb1_processed['Start_Date'])\n",
    "    crisis_start_year = crisis_dates.dt.year.min()\n",
    "    crisis_end_year = crisis_dates.dt.year.max()\n",
    "    \n",
    "    print(f\"Crisis era: {crisis_start_year}-{crisis_end_year}\")\n",
    "    \n",
    "    # Find alliances active during crisis era\n",
    "    if 'atop_processed' in globals():\n",
    "        # Alliance is potentially relevant if:\n",
    "        # alliance_start <= crisis_end AND (alliance_end >= crisis_start OR alliance_end is null)\n",
    "        \n",
    "        alliance_start_years = pd.to_datetime(alliance_dates['Alliance_Start']).dt.year\n",
    "        alliance_end_years = pd.to_datetime(alliance_dates['Alliance_End_Clean']).dt.year\n",
    "        \n",
    "        # Alliances that started before crisis era ended\n",
    "        started_before_crisis_end = alliance_start_years <= crisis_end_year\n",
    "        \n",
    "        # Alliances that ended after crisis era started (or are still active)\n",
    "        ended_after_crisis_start = (alliance_end_years >= crisis_start_year) | (alliance_end_years.isna())\n",
    "        \n",
    "        potentially_relevant = started_before_crisis_end & ended_after_crisis_start\n",
    "        relevant_count = potentially_relevant.sum()\n",
    "        \n",
    "        print(f\"Alliances potentially overlapping with crisis era: {relevant_count}/{len(alliance_dates)}\")\n",
    "        \n",
    "        if relevant_count > 0:\n",
    "            print(f\"\\nSample potentially relevant alliances:\")\n",
    "            relevant_alliances = atop_processed[potentially_relevant][['Alliance_ID', 'Alliance_Start', 'Alliance_End']].head(5)\n",
    "            print(relevant_alliances.to_string(index=False))\n",
    "\n",
    "# Test with a relevant alliance\n",
    "print(f\"\\n=== Testing with Relevant Alliance ===\")\n",
    "if relevant_count > 0:\n",
    "    # Get a recent alliance and recent crisis\n",
    "    recent_alliance = atop_processed[potentially_relevant].iloc[0]\n",
    "    recent_crisis = icb1_processed.iloc[-1]  # Last crisis\n",
    "    \n",
    "    print(f\"Testing: Crisis {recent_crisis['Crisis_ID']} ({recent_crisis['Start_Date']}-{recent_crisis['End_Date']})\")\n",
    "    print(f\"vs Alliance {recent_alliance['Alliance_ID']} ({recent_alliance['Alliance_Start']}-{recent_alliance['Alliance_End']})\")\n",
    "    \n",
    "    # Apply temporal logic\n",
    "    c_start = pd.to_datetime(recent_crisis['Start_Date'])\n",
    "    c_end = pd.to_datetime(recent_crisis['End_Date']) if pd.notna(recent_crisis['End_Date']) else c_start\n",
    "    a_start = pd.to_datetime(recent_alliance['Alliance_Start'])\n",
    "    a_end = pd.to_datetime(recent_alliance['Alliance_End']) if pd.notna(recent_alliance['Alliance_End']) else pd.to_datetime('2030-12-31')\n",
    "    \n",
    "    cond1 = a_start <= c_end\n",
    "    cond2 = a_end >= c_start\n",
    "    overlap = cond1 and cond2\n",
    "    \n",
    "    print(f\"Temporal overlap result: {overlap}\")\n",
    "    print(f\"  Condition 1: {a_start} ≤ {c_end} = {cond1}\")\n",
    "    print(f\"  Condition 2: {a_end} ≥ {c_start} = {cond2}\")\n",
    "\n",
    "print(f\"\\n=== Conclusion ===\")\n",
    "print(\"The temporal logic is CORRECT. The issue is:\")\n",
    "print(\"1. Most ATOP alliances are from 1815-1900s\")\n",
    "print(\"2. ICB crises are from 1918-2022\")\n",
    "print(\"3. Many early alliances naturally don't overlap with modern crises\")\n",
    "print(\"4. Some alliance end dates have invalid '0-01-01' format\")\n",
    "print(f\"5. Only {relevant_count if 'relevant_count' in locals() else 'some'} alliances potentially overlap with crisis era\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8df79f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING FLAG VARIABLES ===\n",
      "=== INVESTIGATING ZERO TEMPORAL OVERLAPS ===\n",
      "Crisis date range:\n",
      "  Start: 1918-05-01 00:00:00 to 2021-09-20 00:00:00\n",
      "  End: 1919-07-29 00:00:00 to 2022-12-01 00:00:00\n",
      "\n",
      "Alliance date range:\n",
      "  Start: 1815-01-03 00:00:00 to 2018-09-28 00:00:00\n",
      "  End: 1815-02-08 00:00:00 to 2018-06-17 00:00:00\n",
      "  Active (no end): 372/789\n",
      "\n",
      "Overlap analysis:\n",
      "Crisis era: 1918-05-01 00:00:00 to 2022-12-01 00:00:00\n",
      "Alliances potentially overlapping crisis era: 692/789\n",
      "\n",
      "Sample overlapping alliances:\n",
      " Alliance_ID Alliance_Start Alliance_End_Fixed\n",
      "        1335     1879-10-07         1918-11-03\n",
      "        1350     1882-05-20         1918-11-03\n",
      "        1355     1883-10-30         1918-11-03\n",
      "        1400     1899-10-14         1949-04-04\n",
      "        1415     1902-01-30         1921-12-13\n",
      "        1420     1902-06-30         1918-11-11\n",
      "        1467     1911-04-17                NaT\n",
      "        1485     1912-11-23         1918-11-11\n",
      "        1490     1913-05-19         1918-11-11\n",
      "        2005     1914-08-02         1918-10-30\n",
      "\n",
      "Processing 512 crises × 789 alliances...\n",
      "\n",
      "=== FLAG VARIABLE RESULTS ===\n",
      "Total crisis-alliance pairs: 403,968\n",
      "Flag 1 (alliance_active_during_crisis = 1): 72,531 (17.95%)\n",
      "Flag 2 (alliance_member_is_crisis_actor = 1): 27,842 (6.89%)\n",
      "Both flags = 1 (analysis set): 5,100 (1.262%)\n",
      "\n",
      "=== Sample: Alliance Active During Crisis ===\n",
      " Crisis_ID  Alliance_ID Crisis_Start Crisis_End Alliance_Start Alliance_End\n",
      "         1         1335   1918-05-01 1920-04-01     1879-10-07   1918-11-03\n",
      "         1         1350   1918-05-01 1920-04-01     1882-05-20   1918-11-03\n",
      "         1         1355   1918-05-01 1920-04-01     1883-10-30   1918-11-03\n",
      "         1         1400   1918-05-01 1920-04-01     1899-10-14   1949-04-04\n",
      "         1         1415   1918-05-01 1920-04-01     1902-01-30   1921-12-13\n",
      "\n",
      "=== Sample: Alliance Member is Crisis Actor ===\n",
      " Crisis_ID  Alliance_ID crisis_actors alliance_members\n",
      "         1         1035           365  200;255;300;365\n",
      "         1         1045           365      200;220;365\n",
      "         1         1065           365      255;300;365\n",
      "         1         1075           365          365;640\n",
      "         1         1080           365          300;365\n",
      "\n",
      "=== Sample: Both Conditions Met (Analysis Set) ===\n",
      " Crisis_ID  Alliance_ID Crisis_Start Alliance_Start Alliance_End\n",
      "         1         2015   1918-05-01     1914-09-05   1918-11-11\n",
      "         1         2025   1918-05-01     1915-04-26   1918-11-11\n",
      "         1         2040   1918-05-01     1916-08-17   1918-11-11\n",
      "         3         2015   1918-06-23     1914-09-05   1918-11-11\n",
      "         3         2025   1918-06-23     1915-04-26   1918-11-11\n",
      "\n",
      "✓ Saved flag variables to 'crisis_alliance_flags.csv'\n",
      "✓ Saved analysis set (5100 rows) to 'crisis_alliance_analysis_set_corrected.csv'\n",
      "\n",
      "=== SUMMARY ===\n",
      "SUCCESS: Found 5100 crisis-alliance pairs meeting both conditions\n",
      "These represent meaningful crisis-alliance interactions for analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def investigate_zero_overlaps():\n",
    "    \"\"\"\n",
    "    Investigate why there are NO temporal overlaps\n",
    "    \"\"\"\n",
    "    print(\"=== INVESTIGATING ZERO TEMPORAL OVERLAPS ===\")\n",
    "    \n",
    "    # Get date ranges\n",
    "    crisis_dates = icb1_processed[['Crisis_ID', 'Start_Date', 'End_Date']].copy()\n",
    "    crisis_dates['Start_Date'] = pd.to_datetime(crisis_dates['Start_Date'], errors='coerce')\n",
    "    crisis_dates['End_Date'] = pd.to_datetime(crisis_dates['End_Date'], errors='coerce')\n",
    "    \n",
    "    # Handle alliance dates carefully\n",
    "    alliance_dates = atop_processed[['Alliance_ID', 'Alliance_Start', 'Alliance_End']].copy()\n",
    "    alliance_dates['Alliance_Start'] = pd.to_datetime(alliance_dates['Alliance_Start'], errors='coerce')\n",
    "    \n",
    "    # Fix problematic alliance end dates\n",
    "    alliance_dates['Alliance_End_Fixed'] = alliance_dates['Alliance_End'].replace('0-01-01', pd.NaT)\n",
    "    alliance_dates['Alliance_End_Fixed'] = pd.to_datetime(alliance_dates['Alliance_End_Fixed'], errors='coerce')\n",
    "    \n",
    "    print(f\"Crisis date range:\")\n",
    "    print(f\"  Start: {crisis_dates['Start_Date'].min()} to {crisis_dates['Start_Date'].max()}\")\n",
    "    print(f\"  End: {crisis_dates['End_Date'].min()} to {crisis_dates['End_Date'].max()}\")\n",
    "    \n",
    "    print(f\"\\nAlliance date range:\")\n",
    "    print(f\"  Start: {alliance_dates['Alliance_Start'].min()} to {alliance_dates['Alliance_Start'].max()}\")\n",
    "    print(f\"  End: {alliance_dates['Alliance_End_Fixed'].min()} to {alliance_dates['Alliance_End_Fixed'].max()}\")\n",
    "    print(f\"  Active (no end): {alliance_dates['Alliance_End_Fixed'].isna().sum()}/{len(alliance_dates)}\")\n",
    "    \n",
    "    # Check overlaps manually\n",
    "    crisis_era_start = crisis_dates['Start_Date'].min()\n",
    "    crisis_era_end = crisis_dates['End_Date'].max()\n",
    "    \n",
    "    print(f\"\\nOverlap analysis:\")\n",
    "    print(f\"Crisis era: {crisis_era_start} to {crisis_era_end}\")\n",
    "    \n",
    "    # Alliances that could overlap\n",
    "    alliance_overlaps = (\n",
    "        (alliance_dates['Alliance_Start'] <= crisis_era_end) &\n",
    "        ((alliance_dates['Alliance_End_Fixed'] >= crisis_era_start) | \n",
    "         (alliance_dates['Alliance_End_Fixed'].isna()))\n",
    "    )\n",
    "    \n",
    "    overlapping_alliances = alliance_overlaps.sum()\n",
    "    print(f\"Alliances potentially overlapping crisis era: {overlapping_alliances}/{len(alliance_dates)}\")\n",
    "    \n",
    "    if overlapping_alliances > 0:\n",
    "        print(\"\\nSample overlapping alliances:\")\n",
    "        samples = alliance_dates[alliance_overlaps][['Alliance_ID', 'Alliance_Start', 'Alliance_End_Fixed']].head(10)\n",
    "        print(samples.to_string(index=False))\n",
    "    \n",
    "    return crisis_dates, alliance_dates\n",
    "\n",
    "def create_flag_variables():\n",
    "    \"\"\"\n",
    "    Create the two flag variables with correct logic\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CREATING FLAG VARIABLES ===\")\n",
    "    \n",
    "    # Get clean date data\n",
    "    crisis_dates, alliance_dates = investigate_zero_overlaps()\n",
    "    \n",
    "    # Prepare data for Cartesian product\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(crisis_dates)} crises × {len(alliance_dates)} alliances...\")\n",
    "    \n",
    "    for crisis_idx, crisis in crisis_dates.iterrows():\n",
    "        crisis_id = crisis['Crisis_ID']\n",
    "        c_start = crisis['Start_Date']\n",
    "        c_end = crisis['End_Date']\n",
    "        \n",
    "        # Get crisis actors\n",
    "        crisis_actors = set()\n",
    "        if crisis_id in master_df['Crisis_ID'].values:\n",
    "            crisis_row = master_df[master_df['Crisis_ID'] == crisis_id].iloc[0]\n",
    "            if pd.notna(crisis_row['Actor_List']) and crisis_row['Actor_List']:\n",
    "                crisis_actors = set(int(x) for x in crisis_row['Actor_List'].split(';'))\n",
    "        \n",
    "        for alliance_idx, alliance in alliance_dates.iterrows():\n",
    "            alliance_id = alliance['Alliance_ID']\n",
    "            a_start = alliance['Alliance_Start']\n",
    "            a_end = alliance['Alliance_End_Fixed']\n",
    "            \n",
    "            # Get alliance members\n",
    "            alliance_members = set()\n",
    "            alliance_row = atop_processed[atop_processed['Alliance_ID'] == alliance_id].iloc[0]\n",
    "            if pd.notna(alliance_row['Members_List']) and alliance_row['Members_List']:\n",
    "                alliance_members = set(int(x) for x in alliance_row['Members_List'].split(';'))\n",
    "            \n",
    "            # FLAG 1: alliance_active_during_crisis\n",
    "            alliance_active_during_crisis = 0\n",
    "            if pd.notna(c_start) and pd.notna(c_end) and pd.notna(a_start):\n",
    "                # Handle missing alliance end date (still active)\n",
    "                if pd.isna(a_end):\n",
    "                    a_end_check = pd.Timestamp('2030-12-31')  # Far future\n",
    "                else:\n",
    "                    a_end_check = a_end\n",
    "                \n",
    "                # Logic: Alliance start ≤ crisis end AND Alliance end ≥ crisis start\n",
    "                if (a_start <= c_end) and (a_end_check >= c_start):\n",
    "                    alliance_active_during_crisis = 1\n",
    "            \n",
    "            # FLAG 2: alliance_member_is_crisis_actor\n",
    "            alliance_member_is_crisis_actor = 0\n",
    "            if crisis_actors and alliance_members:\n",
    "                # Check if any alliance member is a crisis actor\n",
    "                if len(crisis_actors.intersection(alliance_members)) > 0:\n",
    "                    alliance_member_is_crisis_actor = 1\n",
    "            \n",
    "            results.append({\n",
    "                'Crisis_ID': crisis_id,\n",
    "                'Alliance_ID': alliance_id,\n",
    "                'Crisis_Start': c_start,\n",
    "                'Crisis_End': c_end,\n",
    "                'Alliance_Start': a_start,\n",
    "                'Alliance_End': a_end,\n",
    "                'alliance_active_during_crisis': alliance_active_during_crisis,\n",
    "                'alliance_member_is_crisis_actor': alliance_member_is_crisis_actor,\n",
    "                'crisis_actors': ';'.join([str(x) for x in sorted(crisis_actors)]),\n",
    "                'alliance_members': ';'.join([str(x) for x in sorted(alliance_members)])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Execute the analysis\n",
    "crisis_alliance_flags = create_flag_variables()\n",
    "\n",
    "# Analyze results\n",
    "print(f\"\\n=== FLAG VARIABLE RESULTS ===\")\n",
    "print(f\"Total crisis-alliance pairs: {len(crisis_alliance_flags):,}\")\n",
    "\n",
    "flag1_count = crisis_alliance_flags['alliance_active_during_crisis'].sum()\n",
    "flag2_count = crisis_alliance_flags['alliance_member_is_crisis_actor'].sum()\n",
    "both_flags = ((crisis_alliance_flags['alliance_active_during_crisis'] == 1) & \n",
    "              (crisis_alliance_flags['alliance_member_is_crisis_actor'] == 1)).sum()\n",
    "\n",
    "print(f\"Flag 1 (alliance_active_during_crisis = 1): {flag1_count:,} ({flag1_count/len(crisis_alliance_flags)*100:.2f}%)\")\n",
    "print(f\"Flag 2 (alliance_member_is_crisis_actor = 1): {flag2_count:,} ({flag2_count/len(crisis_alliance_flags)*100:.2f}%)\")\n",
    "print(f\"Both flags = 1 (analysis set): {both_flags:,} ({both_flags/len(crisis_alliance_flags)*100:.3f}%)\")\n",
    "\n",
    "# Show examples of each flag\n",
    "if flag1_count > 0:\n",
    "    print(f\"\\n=== Sample: Alliance Active During Crisis ===\")\n",
    "    temporal_examples = crisis_alliance_flags[crisis_alliance_flags['alliance_active_during_crisis'] == 1].head(5)\n",
    "    display_cols = ['Crisis_ID', 'Alliance_ID', 'Crisis_Start', 'Crisis_End', 'Alliance_Start', 'Alliance_End']\n",
    "    print(temporal_examples[display_cols].to_string(index=False))\n",
    "\n",
    "if flag2_count > 0:\n",
    "    print(f\"\\n=== Sample: Alliance Member is Crisis Actor ===\")\n",
    "    member_examples = crisis_alliance_flags[crisis_alliance_flags['alliance_member_is_crisis_actor'] == 1].head(5)\n",
    "    display_cols = ['Crisis_ID', 'Alliance_ID', 'crisis_actors', 'alliance_members']\n",
    "    print(member_examples[display_cols].to_string(index=False))\n",
    "\n",
    "if both_flags > 0:\n",
    "    print(f\"\\n=== Sample: Both Conditions Met (Analysis Set) ===\")\n",
    "    analysis_examples = crisis_alliance_flags[\n",
    "        (crisis_alliance_flags['alliance_active_during_crisis'] == 1) & \n",
    "        (crisis_alliance_flags['alliance_member_is_crisis_actor'] == 1)\n",
    "    ].head(5)\n",
    "    display_cols = ['Crisis_ID', 'Alliance_ID', 'Crisis_Start', 'Alliance_Start', 'Alliance_End']\n",
    "    print(analysis_examples[display_cols].to_string(index=False))\n",
    "\n",
    "# Diagnose zero overlaps\n",
    "if flag1_count == 0:\n",
    "    print(f\"\\n=== DIAGNOSING ZERO TEMPORAL OVERLAPS ===\")\n",
    "    \n",
    "    # Check specific examples\n",
    "    print(\"Testing manual examples:\")\n",
    "    \n",
    "    # Get a recent crisis and recent alliance\n",
    "    recent_crisis = crisis_alliance_flags.iloc[0]\n",
    "    print(f\"Example crisis: {recent_crisis['Crisis_ID']} ({recent_crisis['Crisis_Start']} - {recent_crisis['Crisis_End']})\")\n",
    "    print(f\"Example alliance: {recent_crisis['Alliance_ID']} ({recent_crisis['Alliance_Start']} - {recent_crisis['Alliance_End']})\")\n",
    "    \n",
    "    # Check the conditions step by step\n",
    "    c_start = pd.to_datetime(recent_crisis['Crisis_Start'])\n",
    "    c_end = pd.to_datetime(recent_crisis['Crisis_End'])\n",
    "    a_start = pd.to_datetime(recent_crisis['Alliance_Start'])\n",
    "    a_end = pd.to_datetime(recent_crisis['Alliance_End']) if pd.notna(recent_crisis['Alliance_End']) else pd.Timestamp('2030-12-31')\n",
    "    \n",
    "    print(f\"\\nStep-by-step check:\")\n",
    "    print(f\"Alliance start ≤ Crisis end: {a_start} ≤ {c_end} = {a_start <= c_end}\")\n",
    "    print(f\"Alliance end ≥ Crisis start: {a_end} ≥ {c_start} = {a_end >= c_start}\")\n",
    "    print(f\"Both conditions: {(a_start <= c_end) and (a_end >= c_start)}\")\n",
    "\n",
    "# Save results\n",
    "crisis_alliance_flags.to_csv(\"crisis_alliance_flags.csv\", index=False)\n",
    "print(f\"\\n✓ Saved flag variables to 'crisis_alliance_flags.csv'\")\n",
    "\n",
    "# Create analysis subset\n",
    "analysis_set = crisis_alliance_flags[\n",
    "    (crisis_alliance_flags['alliance_active_during_crisis'] == 1) & \n",
    "    (crisis_alliance_flags['alliance_member_is_crisis_actor'] == 1)\n",
    "].copy()\n",
    "\n",
    "analysis_set.to_csv(\"crisis_alliance_analysis_set_corrected.csv\", index=False)\n",
    "print(f\"✓ Saved analysis set ({len(analysis_set)} rows) to 'crisis_alliance_analysis_set_corrected.csv'\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "if both_flags > 0:\n",
    "    print(f\"SUCCESS: Found {both_flags} crisis-alliance pairs meeting both conditions\")\n",
    "    print(f\"These represent meaningful crisis-alliance interactions for analysis\")\n",
    "else:\n",
    "    print(f\"ISSUE: No pairs meet both conditions simultaneously\")\n",
    "    print(f\"This suggests either data quality issues or truly no relevant interactions\")\n",
    "    \n",
    "globals()['crisis_alliance_flags_df'] = crisis_alliance_flags\n",
    "globals()['analysis_set_corrected_df'] = analysis_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d52a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking Available Columns ===\n",
      "master_df columns: ['Crisis_ID', 'Crisis_Name_x', 'Actor_List', 'Actor_Locations', 'Start_Date', 'End_Date', 'Geographic_Location', 'Crisis_Name_y']\n",
      "master_df shape: (512, 8)\n",
      "icb1_processed columns: ['Crisis_ID', 'Start_Date', 'End_Date', 'Geographic_Location', 'Crisis_Name']\n",
      "icb1_processed shape: (512, 5)\n",
      "atop_processed columns: ['Alliance_ID', 'Alliance_Name', 'Alliance_Start', 'Alliance_End', 'Alliance_Type', 'N_Members', 'Members_List', 'COWID', 'COW4ID', 'Member_Locations']\n",
      "atop_processed shape: (789, 10)\n"
     ]
    }
   ],
   "source": [
    "# Debug and fix the column names\n",
    "print(\"=== Checking Available Columns ===\")\n",
    "\n",
    "if 'master_df' in globals():\n",
    "    print(f\"master_df columns: {list(master_df.columns)}\")\n",
    "    print(f\"master_df shape: {master_df.shape}\")\n",
    "else:\n",
    "    print(\"master_df not found\")\n",
    "\n",
    "if 'icb1_processed' in globals():\n",
    "    print(f\"icb1_processed columns: {list(icb1_processed.columns)}\")\n",
    "    print(f\"icb1_processed shape: {icb1_processed.shape}\")\n",
    "else:\n",
    "    print(\"icb1_processed not found\")\n",
    "\n",
    "if 'atop_processed' in globals():\n",
    "    print(f\"atop_processed columns: {list(atop_processed.columns)}\")\n",
    "    print(f\"atop_processed shape: {atop_processed.shape}\")\n",
    "else:\n",
    "    print(\"atop_processed not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7afc70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Crisis-Alliance Flags ===\n",
      "Creating flags for 512 crises × 789 alliances...\n",
      "✓ Created 403,968 crisis-alliance pairs\n",
      "  Active during crisis: 72,531\n",
      "  Member is actor: 27,842\n",
      "  Both conditions: 5,100\n",
      "\n",
      "Sample pairs with temporal overlap:\n",
      " Crisis_ID  Alliance_ID Crisis_Start Crisis_End Alliance_Start Alliance_End\n",
      "         1         1335   1918-05-01 1920-04-01     1879-10-07   1918-11-03\n",
      "         1         1350   1918-05-01 1920-04-01     1882-05-20   1918-11-03\n",
      "         1         1355   1918-05-01 1920-04-01     1883-10-30   1918-11-03\n",
      "         1         1400   1918-05-01 1920-04-01     1899-10-14   1949-04-04\n",
      "         1         1415   1918-05-01 1920-04-01     1902-01-30   1921-12-13\n",
      "\n",
      "Sample pairs with member overlap:\n",
      " Crisis_ID  Alliance_ID crisis_actors alliance_members\n",
      "         1         1035           365  200;255;300;365\n",
      "         1         1045           365      200;220;365\n",
      "         1         1065           365      255;300;365\n",
      "         1         1075           365          365;640\n",
      "         1         1080           365          300;365\n",
      "\n",
      "✓ crisis_alliance_flags_df is ready! Now you can run the formatting code.\n"
     ]
    }
   ],
   "source": [
    "def create_crisis_alliance_flags_simple():\n",
    "    \"\"\"Create the flag variables dataset - simplified version\"\"\"\n",
    "    \n",
    "    # Use master_df directly - it already has dates and actors\n",
    "    crises = master_df.copy()\n",
    "    alliances = atop_processed.copy()\n",
    "    \n",
    "    print(f\"Creating flags for {len(crises)} crises × {len(alliances)} alliances...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for crisis_idx, crisis in crises.iterrows():\n",
    "        crisis_id = crisis['Crisis_ID']\n",
    "        \n",
    "        # Get dates (already in master_df)\n",
    "        c_start = pd.to_datetime(crisis['Start_Date'], errors='coerce')\n",
    "        c_end = pd.to_datetime(crisis['End_Date'], errors='coerce')\n",
    "        \n",
    "        # Get crisis actors (already in master_df)\n",
    "        crisis_actors = set()\n",
    "        if pd.notna(crisis['Actor_List']) and crisis['Actor_List']:\n",
    "            try:\n",
    "                crisis_actors = set(int(x) for x in crisis['Actor_List'].split(';') if x.strip())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for alliance_idx, alliance in alliances.iterrows():\n",
    "            alliance_id = alliance['Alliance_ID']\n",
    "            \n",
    "            # Get alliance dates\n",
    "            a_start = pd.to_datetime(alliance['Alliance_Start'], errors='coerce')\n",
    "            \n",
    "            # Handle alliance end date\n",
    "            if pd.notna(alliance['Alliance_End']) and str(alliance['Alliance_End']) not in ['0-01-01', '']:\n",
    "                a_end = pd.to_datetime(alliance['Alliance_End'], errors='coerce')\n",
    "            else:\n",
    "                a_end = pd.Timestamp('2030-12-31')  # Active alliance\n",
    "            \n",
    "            # Get alliance members\n",
    "            alliance_members = set()\n",
    "            if pd.notna(alliance['Members_List']) and alliance['Members_List']:\n",
    "                try:\n",
    "                    alliance_members = set(int(x) for x in alliance['Members_List'].split(';') if x.strip())\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # FLAG 1: alliance_active_during_crisis\n",
    "            alliance_active_during_crisis = 0\n",
    "            if pd.notna(c_start) and pd.notna(c_end) and pd.notna(a_start) and pd.notna(a_end):\n",
    "                if (a_start <= c_end) and (a_end >= c_start):\n",
    "                    alliance_active_during_crisis = 1\n",
    "            \n",
    "            # FLAG 2: alliance_member_is_crisis_actor  \n",
    "            alliance_member_is_crisis_actor = 0\n",
    "            if crisis_actors and alliance_members:\n",
    "                if len(crisis_actors.intersection(alliance_members)) > 0:\n",
    "                    alliance_member_is_crisis_actor = 1\n",
    "            \n",
    "            results.append({\n",
    "                'Crisis_ID': crisis_id,\n",
    "                'Alliance_ID': alliance_id,\n",
    "                'Crisis_Start': c_start,\n",
    "                'Crisis_End': c_end,\n",
    "                'Alliance_Start': a_start,\n",
    "                'Alliance_End': a_end,\n",
    "                'alliance_active_during_crisis': alliance_active_during_crisis,\n",
    "                'alliance_member_is_crisis_actor': alliance_member_is_crisis_actor,\n",
    "                'crisis_actors': ';'.join([str(x) for x in sorted(crisis_actors)]),\n",
    "                'alliance_members': ';'.join([str(x) for x in sorted(alliance_members)])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Create the flags dataset\n",
    "print(\"=== Creating Crisis-Alliance Flags ===\")\n",
    "crisis_alliance_flags_df = create_crisis_alliance_flags_simple()\n",
    "\n",
    "# Quick summary\n",
    "flag1_count = crisis_alliance_flags_df['alliance_active_during_crisis'].sum()\n",
    "flag2_count = crisis_alliance_flags_df['alliance_member_is_crisis_actor'].sum()\n",
    "both_flags = ((crisis_alliance_flags_df['alliance_active_during_crisis'] == 1) & \n",
    "              (crisis_alliance_flags_df['alliance_member_is_crisis_actor'] == 1)).sum()\n",
    "\n",
    "print(f\"✓ Created {len(crisis_alliance_flags_df):,} crisis-alliance pairs\")\n",
    "print(f\"  Active during crisis: {flag1_count:,}\")\n",
    "print(f\"  Member is actor: {flag2_count:,}\")\n",
    "print(f\"  Both conditions: {both_flags:,}\")\n",
    "\n",
    "# Show sample with actual dates to verify logic\n",
    "print(f\"\\nSample pairs with temporal overlap:\")\n",
    "temporal_examples = crisis_alliance_flags_df[crisis_alliance_flags_df['alliance_active_during_crisis'] == 1]\n",
    "if len(temporal_examples) > 0:\n",
    "    sample_cols = ['Crisis_ID', 'Alliance_ID', 'Crisis_Start', 'Crisis_End', 'Alliance_Start', 'Alliance_End']\n",
    "    print(temporal_examples[sample_cols].head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No temporal overlaps found!\")\n",
    "\n",
    "print(f\"\\nSample pairs with member overlap:\")\n",
    "member_examples = crisis_alliance_flags_df[crisis_alliance_flags_df['alliance_member_is_crisis_actor'] == 1]\n",
    "if len(member_examples) > 0:\n",
    "    sample_cols = ['Crisis_ID', 'Alliance_ID', 'crisis_actors', 'alliance_members']\n",
    "    print(member_examples[sample_cols].head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No member overlaps found!\")\n",
    "\n",
    "# Make available globally\n",
    "globals()['crisis_alliance_flags_df'] = crisis_alliance_flags_df\n",
    "\n",
    "print(\"\\n✓ crisis_alliance_flags_df is ready! Now you can run the formatting code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cea0cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAVING DATASETS (SIMPLE VERSION) ===\n",
      "✓ SAVED DATASETS:\n",
      "  Full dataset: 403,968 rows\n",
      "    - ICB_ATOP_full_20250702.csv\n",
      "    - ICB_ATOP_full_20250702.json\n",
      "  Analysis dataset: 5,100 rows\n",
      "    - ICB_ATOP_merged_20250702.csv\n",
      "    - ICB_ATOP_merged_20250702.json\n",
      "\n",
      "Analysis dataset sample:\n",
      " Crisis_ID  Alliance_ID Crisis_Start Crisis_End Alliance_Start Alliance_End  Active_During_Crisis  Member_Is_Actor Actors_List        Members_List\n",
      "         1         2015   1918-05-01 1920-04-01     1914-09-05   1918-11-11                     1                1         365 200;220;325;365;740\n",
      "         1         2025   1918-05-01 1920-04-01     1915-04-26   1918-11-11                     1                1         365     200;220;325;365\n",
      "         1         2040   1918-05-01 1920-04-01     1916-08-17   1918-11-11                     1                1         365 200;220;325;360;365\n",
      "         3         2015   1918-06-23 1919-09-27     1914-09-05   1918-11-11                     1                1         365 200;220;325;365;740\n",
      "         3         2025   1918-06-23 1919-09-27     1915-04-26   1918-11-11                     1                1         365     200;220;325;365\n"
     ]
    }
   ],
   "source": [
    "# Simple backup save function\n",
    "def save_datasets_simple():\n",
    "    \"\"\"Simple backup function to save datasets if main formatting fails\"\"\"\n",
    "    \n",
    "    if 'crisis_alliance_flags_df' not in globals():\n",
    "        print(\"ERROR: crisis_alliance_flags_df not found\")\n",
    "        return\n",
    "    \n",
    "    # Get the full dataset\n",
    "    full_df = crisis_alliance_flags_df.copy()\n",
    "    \n",
    "    # Create analysis subset (both flags = 1)\n",
    "    analysis_df = full_df[\n",
    "        (full_df['alliance_active_during_crisis'] == 1) & \n",
    "        (full_df['alliance_member_is_crisis_actor'] == 1)\n",
    "    ].copy()\n",
    "    \n",
    "    # Rename columns to match requirements\n",
    "    full_df = full_df.rename(columns={\n",
    "        'alliance_active_during_crisis': 'Active_During_Crisis',\n",
    "        'alliance_member_is_crisis_actor': 'Member_Is_Actor',\n",
    "        'crisis_actors': 'Actors_List',\n",
    "        'alliance_members': 'Members_List'\n",
    "    })\n",
    "    \n",
    "    analysis_df = analysis_df.rename(columns={\n",
    "        'alliance_active_during_crisis': 'Active_During_Crisis', \n",
    "        'alliance_member_is_crisis_actor': 'Member_Is_Actor',\n",
    "        'crisis_actors': 'Actors_List',\n",
    "        'alliance_members': 'Members_List'\n",
    "    })\n",
    "    \n",
    "    # Convert dates to strings for JSON compatibility\n",
    "    date_columns = ['Crisis_Start', 'Crisis_End', 'Alliance_Start', 'Alliance_End']\n",
    "    for col in date_columns:\n",
    "        if col in full_df.columns:\n",
    "            full_df[col] = full_df[col].astype(str)\n",
    "            analysis_df[col] = analysis_df[col].astype(str)\n",
    "    \n",
    "    # Save full dataset\n",
    "    full_df.to_csv(\"ICB_ATOP_full_20250702.csv\", index=False)\n",
    "    full_df.to_json(\"ICB_ATOP_full_20250702.json\", orient='records', indent=2)\n",
    "    \n",
    "    # Save analysis dataset\n",
    "    analysis_df.to_csv(\"ICB_ATOP_merged_20250702.csv\", index=False)\n",
    "    analysis_df.to_json(\"ICB_ATOP_merged_20250702.json\", orient='records', indent=2)\n",
    "    \n",
    "    print(f\"✓ SAVED DATASETS:\")\n",
    "    print(f\"  Full dataset: {len(full_df):,} rows\")\n",
    "    print(f\"    - ICB_ATOP_full_20250702.csv\")\n",
    "    print(f\"    - ICB_ATOP_full_20250702.json\")\n",
    "    print(f\"  Analysis dataset: {len(analysis_df):,} rows\")\n",
    "    print(f\"    - ICB_ATOP_merged_20250702.csv\") \n",
    "    print(f\"    - ICB_ATOP_merged_20250702.json\")\n",
    "    \n",
    "    return full_df, analysis_df\n",
    "\n",
    "# Run the simple save function\n",
    "print(\"=== SAVING DATASETS (SIMPLE VERSION) ===\")\n",
    "saved_full, saved_analysis = save_datasets_simple()\n",
    "\n",
    "# Show basic stats\n",
    "if len(saved_analysis) > 0:\n",
    "    print(f\"\\nAnalysis dataset sample:\")\n",
    "    print(saved_analysis.head().to_string(index=False))\n",
    "else:\n",
    "    print(f\"\\nWARNING: Analysis dataset is empty (no pairs meet both conditions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ab52cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING END DATES ANALYSIS ===\n",
      "1. CRISIS END DATES:\n",
      "   Total crises: 512\n",
      "   Missing end dates: 2\n",
      "   Percentage missing: 0.4%\n",
      "\n",
      "   Sample crises with missing end dates:\n",
      " Crisis_ID               Crisis_Name_x Start_Date End_Date\n",
      "       161                WEST IRIAN I 1957-12-01     None\n",
      "       510 RUSSIAN INVASION OF UKRAINE       None     None\n",
      "\n",
      "   Crisis date range (valid end dates):\n",
      "   Earliest end: 1919-07-29 00:00:00\n",
      "   Latest end: 2022-12-01 00:00:00\n",
      "\n",
      "==================================================\n",
      "2. ALLIANCE END DATES:\n",
      "   Total alliances: 789\n",
      "   Truly missing (NaN): 0\n",
      "   Invalid dates ('0-01-01'): 372\n",
      "   Empty strings: 0\n",
      "   Total effectively missing: 372\n",
      "   Percentage missing: 47.1%\n",
      "\n",
      "   Sample alliances with missing/invalid end dates:\n",
      "   '0-01-01' end dates:\n",
      " Alliance_ID Alliance_Start Alliance_End\n",
      "        1467     1911-04-17      0-01-01\n",
      "        2340     1934-05-20      0-01-01\n",
      "        2423     1938-07-21      0-01-01\n",
      "        2495     1939-12-17      0-01-01\n",
      "        2565     1944-01-21      0-01-01\n",
      "\n",
      "   Alliance date range (valid end dates):\n",
      "   Earliest end: 1815-02-08 00:00:00\n",
      "   Latest end: 2018-06-17 00:00:00\n",
      "\n",
      "   Active alliances (no valid end date): 372 (47.1%)\n",
      "\n",
      "==================================================\n",
      "3. IMPACT ANALYSIS:\n",
      "   % Crises with missing end dates: 0.4%\n",
      "   % Alliances with missing end dates: 47.1%\n",
      "\n",
      "   CONCLUSION:\n",
      "   - 0.4% of crises need end date imputation\n",
      "   - 47.1% of alliances are likely still active\n",
      "   - Missing end dates could significantly affect temporal overlap calculations\n",
      "   - Previous zero temporal overlaps likely due to strict missing data handling\n"
     ]
    }
   ],
   "source": [
    "# Check missing end dates in original datasets\n",
    "print(\"=== MISSING END DATES ANALYSIS ===\")\n",
    "\n",
    "# 1. Crisis End Dates\n",
    "if 'master_df' in globals():\n",
    "    print(\"1. CRISIS END DATES:\")\n",
    "    total_crises = len(master_df)\n",
    "    missing_crisis_end = master_df['End_Date'].isna().sum()\n",
    "    \n",
    "    print(f\"   Total crises: {total_crises}\")\n",
    "    print(f\"   Missing end dates: {missing_crisis_end}\")\n",
    "    print(f\"   Percentage missing: {missing_crisis_end/total_crises*100:.1f}%\")\n",
    "    \n",
    "    if missing_crisis_end > 0:\n",
    "        print(f\"\\n   Sample crises with missing end dates:\")\n",
    "        missing_sample = master_df[master_df['End_Date'].isna()][['Crisis_ID', 'Crisis_Name_x', 'Start_Date', 'End_Date']].head(10)\n",
    "        print(missing_sample.to_string(index=False))\n",
    "    \n",
    "    # Check date range for non-missing end dates\n",
    "    valid_end_dates = master_df['End_Date'].dropna()\n",
    "    if len(valid_end_dates) > 0:\n",
    "        print(f\"\\n   Crisis date range (valid end dates):\")\n",
    "        print(f\"   Earliest end: {pd.to_datetime(valid_end_dates).min()}\")\n",
    "        print(f\"   Latest end: {pd.to_datetime(valid_end_dates).max()}\")\n",
    "\n",
    "else:\n",
    "    print(\"master_df not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. Alliance End Dates  \n",
    "if 'atop_processed' in globals():\n",
    "    print(\"2. ALLIANCE END DATES:\")\n",
    "    total_alliances = len(atop_processed)\n",
    "    \n",
    "    # Check for various types of missing/invalid end dates\n",
    "    missing_alliance_end = atop_processed['Alliance_End'].isna().sum()\n",
    "    zero_dates = (atop_processed['Alliance_End'] == '0-01-01').sum()\n",
    "    empty_strings = (atop_processed['Alliance_End'] == '').sum()\n",
    "    \n",
    "    print(f\"   Total alliances: {total_alliances}\")\n",
    "    print(f\"   Truly missing (NaN): {missing_alliance_end}\")\n",
    "    print(f\"   Invalid dates ('0-01-01'): {zero_dates}\")\n",
    "    print(f\"   Empty strings: {empty_strings}\")\n",
    "    \n",
    "    total_missing = missing_alliance_end + zero_dates + empty_strings\n",
    "    print(f\"   Total effectively missing: {total_missing}\")\n",
    "    print(f\"   Percentage missing: {total_missing/total_alliances*100:.1f}%\")\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        print(f\"\\n   Sample alliances with missing/invalid end dates:\")\n",
    "        \n",
    "        # Show samples of each type\n",
    "        if missing_alliance_end > 0:\n",
    "            print(\"   NaN end dates:\")\n",
    "            nan_sample = atop_processed[atop_processed['Alliance_End'].isna()][['Alliance_ID', 'Alliance_Start', 'Alliance_End']].head(5)\n",
    "            print(nan_sample.to_string(index=False))\n",
    "        \n",
    "        if zero_dates > 0:\n",
    "            print(\"   '0-01-01' end dates:\")\n",
    "            zero_sample = atop_processed[atop_processed['Alliance_End'] == '0-01-01'][['Alliance_ID', 'Alliance_Start', 'Alliance_End']].head(5)\n",
    "            print(zero_sample.to_string(index=False))\n",
    "    \n",
    "    # Check date range for valid end dates\n",
    "    valid_alliance_ends = atop_processed[\n",
    "        (atop_processed['Alliance_End'].notna()) & \n",
    "        (atop_processed['Alliance_End'] != '0-01-01') &\n",
    "        (atop_processed['Alliance_End'] != '')\n",
    "    ]['Alliance_End']\n",
    "    \n",
    "    if len(valid_alliance_ends) > 0:\n",
    "        print(f\"\\n   Alliance date range (valid end dates):\")\n",
    "        valid_end_parsed = pd.to_datetime(valid_alliance_ends, errors='coerce').dropna()\n",
    "        if len(valid_end_parsed) > 0:\n",
    "            print(f\"   Earliest end: {valid_end_parsed.min()}\")\n",
    "            print(f\"   Latest end: {valid_end_parsed.max()}\")\n",
    "    \n",
    "    # Show active alliances (those without valid end dates)\n",
    "    active_alliances = total_missing\n",
    "    print(f\"\\n   Active alliances (no valid end date): {active_alliances} ({active_alliances/total_alliances*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"atop_processed not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 3. Summary Impact\n",
    "print(\"3. IMPACT ANALYSIS:\")\n",
    "if 'master_df' in globals() and 'atop_processed' in globals():\n",
    "    crisis_missing_pct = (master_df['End_Date'].isna().sum() / len(master_df)) * 100\n",
    "    alliance_missing_pct = (total_missing / len(atop_processed)) * 100\n",
    "    \n",
    "    print(f\"   % Crises with missing end dates: {crisis_missing_pct:.1f}%\")\n",
    "    print(f\"   % Alliances with missing end dates: {alliance_missing_pct:.1f}%\")\n",
    "    \n",
    "    if crisis_missing_pct > 0 or alliance_missing_pct > 0:\n",
    "        print(f\"\\n   CONCLUSION:\")\n",
    "        if crisis_missing_pct > 0:\n",
    "            print(f\"   - {crisis_missing_pct:.1f}% of crises need end date imputation\")\n",
    "        if alliance_missing_pct > 0:\n",
    "            print(f\"   - {alliance_missing_pct:.1f}% of alliances are likely still active\")\n",
    "        print(f\"   - Missing end dates could significantly affect temporal overlap calculations\")\n",
    "        print(f\"   - Previous zero temporal overlaps likely due to strict missing data handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8969fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE ALLIANCE END DATE ANALYSIS ===\n",
      "Total unique end date values: 331\n",
      "\n",
      "All unique values (showing frequency):\n",
      "Alliance_End\n",
      "0-01-01       372\n",
      "1990-10-03     13\n",
      "1856-03-30      7\n",
      "1918-11-11      7\n",
      "1990-05-22      7\n",
      "1940-06-16      5\n",
      "1938-09-30      4\n",
      "1939-09-27      4\n",
      "1918-09-30      4\n",
      "1941-04-20      4\n",
      "1949-09-30      4\n",
      "1917-11-08      4\n",
      "1945-08-14      3\n",
      "1866-07-26      3\n",
      "1937-07-08      3\n",
      "1918-11-03      3\n",
      "1939-03-15      3\n",
      "1939-09-03      3\n",
      "1940-06-22      3\n",
      "1872-05-10      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== INVALID DATE PATTERNS ===\n",
      "'0-01-01' pattern: 372\n",
      "Starts with zeros: 372\n",
      "Starts with '1-': 0\n",
      "Dates before 1800: 0\n",
      "\n",
      "=== SUSPICIOUS PATTERNS ===\n",
      "All values starting with '0': ['0-01-01']\n",
      "Unparseable date strings: ['0-01-01']\n",
      "\n",
      "=== SAMPLE ALLIANCES WITH INVALID DATES ===\n",
      "Sample alliances with '0-01-01':\n",
      " Alliance_ID Alliance_Start Alliance_End  N_Members\n",
      "        1467     1911-04-17      0-01-01          2\n",
      "        2340     1934-05-20      0-01-01          3\n",
      "        2423     1938-07-21      0-01-01          2\n",
      "        2495     1939-12-17      0-01-01          2\n",
      "        2565     1944-01-21      0-01-01          2\n",
      "\n",
      "=== SUMMARY OF INVALID DATES ===\n",
      "Total alliances: 789\n",
      "Missing (NaN): 0\n",
      "'0-01-01' pattern: 372\n",
      "Other invalid patterns: 0\n",
      "Total invalid/missing: 372\n",
      "Valid end dates: 417\n",
      "Percentage with valid end dates: 52.9%\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive analysis of alliance end dates\n",
    "print(\"=== COMPREHENSIVE ALLIANCE END DATE ANALYSIS ===\")\n",
    "\n",
    "if 'atop_processed' in globals():\n",
    "    \n",
    "    # Get all unique alliance end date values\n",
    "    unique_end_dates = atop_processed['Alliance_End'].value_counts(dropna=False)\n",
    "    \n",
    "    print(f\"Total unique end date values: {len(unique_end_dates)}\")\n",
    "    print(f\"\\nAll unique values (showing frequency):\")\n",
    "    print(unique_end_dates.head(20))  # Show top 20 most common\n",
    "    \n",
    "    # Look for specific invalid patterns\n",
    "    print(f\"\\n=== INVALID DATE PATTERNS ===\")\n",
    "    \n",
    "    # 1. The known '0-01-01' pattern\n",
    "    zero_pattern_1 = (atop_processed['Alliance_End'] == '0-01-01').sum()\n",
    "    print(f\"'0-01-01' pattern: {zero_pattern_1}\")\n",
    "    \n",
    "    # 2. Other zero-year patterns\n",
    "    zero_patterns = atop_processed['Alliance_End'].str.contains('^0+[-/]', na=False).sum()\n",
    "    print(f\"Starts with zeros: {zero_patterns}\")\n",
    "    \n",
    "    # 3. Year 1 patterns\n",
    "    year_1_patterns = atop_processed['Alliance_End'].str.contains('^1[-/]', na=False).sum()\n",
    "    print(f\"Starts with '1-': {year_1_patterns}\")\n",
    "    \n",
    "    # 4. Very early dates (before 1800)\n",
    "    early_dates = 0\n",
    "    for date_str in atop_processed['Alliance_End'].dropna():\n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_str, errors='coerce')\n",
    "            if pd.notna(parsed_date) and parsed_date.year < 1800:\n",
    "                early_dates += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"Dates before 1800: {early_dates}\")\n",
    "    \n",
    "    # 5. Check for other suspicious patterns\n",
    "    print(f\"\\n=== SUSPICIOUS PATTERNS ===\")\n",
    "    \n",
    "    # Find all values that start with 0\n",
    "    starts_with_zero = atop_processed[atop_processed['Alliance_End'].str.startswith('0', na=False)]['Alliance_End'].unique()\n",
    "    print(f\"All values starting with '0': {list(starts_with_zero)}\")\n",
    "    \n",
    "    # Find all values that start with 1- (might be year 1)\n",
    "    starts_with_one = atop_processed[atop_processed['Alliance_End'].str.startswith('1-', na=False)]['Alliance_End'].unique()\n",
    "    if len(starts_with_one) > 0:\n",
    "        print(f\"All values starting with '1-': {list(starts_with_one)}\")\n",
    "    \n",
    "    # Check for non-date strings (anything that can't be parsed)\n",
    "    unparseable = []\n",
    "    for date_str in atop_processed['Alliance_End'].dropna().unique():\n",
    "        try:\n",
    "            parsed = pd.to_datetime(date_str, errors='coerce')\n",
    "            if pd.isna(parsed):\n",
    "                unparseable.append(date_str)\n",
    "        except:\n",
    "            unparseable.append(date_str)\n",
    "    \n",
    "    if len(unparseable) > 0:\n",
    "        print(f\"Unparseable date strings: {unparseable}\")\n",
    "    \n",
    "    # 6. Show sample alliances with each invalid pattern\n",
    "    print(f\"\\n=== SAMPLE ALLIANCES WITH INVALID DATES ===\")\n",
    "    \n",
    "    if zero_pattern_1 > 0:\n",
    "        print(f\"Sample alliances with '0-01-01':\")\n",
    "        zero_sample = atop_processed[atop_processed['Alliance_End'] == '0-01-01'][['Alliance_ID', 'Alliance_Start', 'Alliance_End', 'N_Members']].head(5)\n",
    "        print(zero_sample.to_string(index=False))\n",
    "    \n",
    "    # Show other patterns if they exist\n",
    "    for pattern in starts_with_zero:\n",
    "        if pattern != '0-01-01':\n",
    "            print(f\"\\nSample with pattern '{pattern}':\")\n",
    "            pattern_sample = atop_processed[atop_processed['Alliance_End'] == pattern][['Alliance_ID', 'Alliance_Start', 'Alliance_End']].head(3)\n",
    "            print(pattern_sample.to_string(index=False))\n",
    "    \n",
    "    # 7. Summary of all invalid categories\n",
    "    print(f\"\\n=== SUMMARY OF INVALID DATES ===\")\n",
    "    \n",
    "    total_alliances = len(atop_processed)\n",
    "    truly_missing = atop_processed['Alliance_End'].isna().sum()\n",
    "    zero_dates = (atop_processed['Alliance_End'] == '0-01-01').sum()\n",
    "    other_invalid = len(unparseable) - (1 if '0-01-01' in unparseable else 0)\n",
    "    \n",
    "    print(f\"Total alliances: {total_alliances}\")\n",
    "    print(f\"Missing (NaN): {truly_missing}\")\n",
    "    print(f\"'0-01-01' pattern: {zero_dates}\")\n",
    "    print(f\"Other invalid patterns: {other_invalid}\")\n",
    "    print(f\"Total invalid/missing: {truly_missing + zero_dates + other_invalid}\")\n",
    "    \n",
    "    # Calculate valid alliances\n",
    "    valid_alliances = total_alliances - (truly_missing + zero_dates + other_invalid)\n",
    "    print(f\"Valid end dates: {valid_alliances}\")\n",
    "    print(f\"Percentage with valid end dates: {valid_alliances/total_alliances*100:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"atop_processed not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90e2e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING FOR INEFFECT VARIABLE ===\n",
      "atop_processed columns: ['Alliance_ID', 'Alliance_Name', 'Alliance_Start', 'Alliance_End', 'Alliance_Type', 'N_Members', 'Members_List', 'COWID', 'COW4ID', 'Member_Locations']\n",
      "⚠️ INEFFECT variable NOT found in atop_processed\n",
      "Need to load from original ATOP dataset\n",
      "\n",
      "Checking original atop_df columns...\n",
      "✓ INEFFECT found in original atop_df\n",
      "Alliances in effect: 372/789\n",
      "Merging INEFFECT into atop_processed...\n",
      "✓ Updated atop_processed with INEFFECT variable\n"
     ]
    }
   ],
   "source": [
    "# Check if we have the INEFFECT variable in our data\n",
    "print(\"=== CHECKING FOR INEFFECT VARIABLE ===\")\n",
    "\n",
    "if 'atop_processed' in globals():\n",
    "    print(f\"atop_processed columns: {list(atop_processed.columns)}\")\n",
    "    \n",
    "    if 'ineffect' in atop_processed.columns:\n",
    "        print(\"✓ INEFFECT variable found in atop_processed\")\n",
    "        ineffect_count = atop_processed['ineffect'].sum()\n",
    "        print(f\"Alliances still in effect (ineffect=1): {ineffect_count}\")\n",
    "    else:\n",
    "        print(\"⚠️ INEFFECT variable NOT found in atop_processed\")\n",
    "        print(\"Need to load from original ATOP dataset\")\n",
    "\n",
    "# Load INEFFECT from original ATOP data if needed\n",
    "if 'atop_df' in globals():\n",
    "    print(f\"\\nChecking original atop_df columns...\")\n",
    "    if 'ineffect' in atop_df.columns:\n",
    "        print(\"✓ INEFFECT found in original atop_df\")\n",
    "        ineffect_data = atop_df[['atopid', 'ineffect']].drop_duplicates()\n",
    "        print(f\"Alliances in effect: {ineffect_data['ineffect'].sum()}/{len(ineffect_data)}\")\n",
    "        \n",
    "        # Merge with atop_processed\n",
    "        print(\"Merging INEFFECT into atop_processed...\")\n",
    "        atop_processed_updated = atop_processed.merge(\n",
    "            ineffect_data.rename(columns={'atopid': 'Alliance_ID'}), \n",
    "            on='Alliance_ID', \n",
    "            how='left'\n",
    "        )\n",
    "        atop_processed_updated['ineffect'] = atop_processed_updated['ineffect'].fillna(0)\n",
    "        \n",
    "        # Update global variable\n",
    "        globals()['atop_processed'] = atop_processed_updated\n",
    "        print(\"✓ Updated atop_processed with INEFFECT variable\")\n",
    "    else:\n",
    "        print(\"⚠️ INEFFECT not found in atop_df either\")\n",
    "        print(\"Available columns:\", list(atop_df.columns)[:20])\n",
    "\n",
    "# Function to fix alliance end dates and add ineffect variable\n",
    "def fix_alliance_end_dates():\n",
    "    \"\"\"Fix alliance end dates based on INEFFECT variable\"\"\"\n",
    "    \n",
    "    print(\"\\n=== FIXING ALLIANCE END DATES ===\")\n",
    "    \n",
    "    if 'crisis_alliance_flags_df' not in globals():\n",
    "        print(\"ERROR: crisis_alliance_flags_df not found\")\n",
    "        return None\n",
    "    \n",
    "    # Work with the flags dataset\n",
    "    fixed_df = crisis_alliance_flags_df.copy()\n",
    "    \n",
    "    # Add ineffect information\n",
    "    if 'atop_processed' in globals() and 'ineffect' in atop_processed.columns:\n",
    "        # Get ineffect data\n",
    "        ineffect_data = atop_processed[['Alliance_ID', 'ineffect']].drop_duplicates()\n",
    "        \n",
    "        # Merge with our dataset\n",
    "        fixed_df = fixed_df.merge(ineffect_data, on='Alliance_ID', how='left')\n",
    "        fixed_df['alliance_ineffect'] = fixed_df['ineffect'].fillna(0).astype(int)\n",
    "        \n",
    "        # Fix end dates for alliances still in effect\n",
    "        ineffect_mask = fixed_df['alliance_ineffect'] == 1\n",
    "        ineffect_count = ineffect_mask.sum()\n",
    "        \n",
    "        print(f\"Found {ineffect_count:,} rows with alliances still in effect\")\n",
    "        print(f\"Setting their end dates to NaN...\")\n",
    "        \n",
    "        # Set Alliance_End to NaN for ineffect alliances\n",
    "        fixed_df.loc[ineffect_mask, 'Alliance_End'] = pd.NaT\n",
    "        \n",
    "        # Verify the fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13a83ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPDATING FINAL DATASETS WITH ALLIANCE_INEFFECT ===\n",
      "Loaded 243 COW countries + 3 additional = 220 total\n",
      "Creating updated formatted datasets...\n",
      "Creating updated formatted dataset...\n",
      "\n",
      "=== UPDATED DATASET STATISTICS ===\n",
      "Full dataset: 403,968 crisis-alliance pairs\n",
      "Analysis dataset: 5,100 crisis-alliance pairs\n",
      "\n",
      "Alliance status in full dataset:\n",
      "  Terminated alliances (InEffect=0): 403,968\n",
      "  Active alliances (InEffect=1): 0\n",
      "\n",
      "Alliance status in analysis dataset:\n",
      "  Terminated alliances (InEffect=0): 5,100\n",
      "  Active alliances (InEffect=1): 0\n",
      "\n",
      "=== SAMPLE UPDATED ANALYSIS DATASET ===\n",
      " Crisis_ID          Crisis_Name  Alliance_ID                Alliance_Type  Active_During_Crisis  Member_Is_Actor  Alliance_InEffect  N_Members  N_Members_Actors\n",
      "         1  RUSSIAN CIVIL WAR I         2015                 Consultation                     1                1                  0          5                 1\n",
      "         1  RUSSIAN CIVIL WAR I         2025 Defense;Offense;Consultation                     1                1                  0          4                 1\n",
      "         1  RUSSIAN CIVIL WAR I         2040              Defense;Offense                     1                1                  0          5                 1\n",
      "         3 RUSSIAN CIVIL WAR II         2015                 Consultation                     1                1                  0          5                 1\n",
      "         3 RUSSIAN CIVIL WAR II         2025 Defense;Offense;Consultation                     1                1                  0          4                 1\n",
      "\n",
      "=== SAVING UPDATED DATASETS ===\n",
      "✓ Updated full dataset saved: 403,968 rows\n",
      "  - ICB_ATOP_full_20250702.csv\n",
      "  - ICB_ATOP_full_20250702.json\n",
      "✓ Updated analysis dataset saved: 5,100 rows\n",
      "  - ICB_ATOP_merged_20250702.csv\n",
      "  - ICB_ATOP_merged_20250702.json\n",
      "\n",
      "=== SUCCESS ===\n",
      "Updated datasets saved with Alliance_InEffect variable!\n"
     ]
    }
   ],
   "source": [
    "# Complete code with all function definitions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_cow_country_codes():\n",
    "    \"\"\"Load COW country codes and add additional countries\"\"\"\n",
    "    \n",
    "    # Load COW dataset\n",
    "    cow_df = pd.read_csv(\"COW-country-codes.csv\")\n",
    "    \n",
    "    # Create mapping from CCode to StateNme\n",
    "    cow_mapping = dict(zip(cow_df['CCode'], cow_df['StateNme']))\n",
    "    \n",
    "    # Add additional countries not in COW\n",
    "    additional_countries = {\n",
    "        219: \"Vichy France\",\n",
    "        671: \"Hejaz\", \n",
    "        672: \"Najd\"\n",
    "    }\n",
    "    \n",
    "    cow_mapping.update(additional_countries)\n",
    "    \n",
    "    print(f\"Loaded {len(cow_df)} COW countries + {len(additional_countries)} additional = {len(cow_mapping)} total\")\n",
    "    \n",
    "    return cow_mapping\n",
    "\n",
    "def format_country_list(country_codes_str, country_mapping):\n",
    "    \"\"\"Format country list as 'CountryName(Code);CountryName(Code)'\"\"\"\n",
    "    \n",
    "    if not country_codes_str or pd.isna(country_codes_str):\n",
    "        return \"\"\n",
    "    \n",
    "    formatted_countries = []\n",
    "    for code_str in country_codes_str.split(';'):\n",
    "        if code_str.strip():\n",
    "            try:\n",
    "                code = int(code_str.strip())\n",
    "                country_name = country_mapping.get(code, f\"Unknown_{code}\")\n",
    "                formatted_countries.append(f\"{country_name}({code})\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    return \";\".join(formatted_countries)\n",
    "\n",
    "def get_alliance_type_names(type_codes_str):\n",
    "    \"\"\"Convert alliance type codes to names\"\"\"\n",
    "    \n",
    "    type_mapping = {\n",
    "        1: \"Defense\",\n",
    "        2: \"Offense\", \n",
    "        3: \"Neutral\",\n",
    "        4: \"NonAggression\",\n",
    "        5: \"Consultation\"\n",
    "    }\n",
    "    \n",
    "    if not type_codes_str or pd.isna(type_codes_str):\n",
    "        return \"\"\n",
    "    \n",
    "    type_names = []\n",
    "    for code_str in type_codes_str.split(';'):\n",
    "        if code_str.strip():\n",
    "            try:\n",
    "                code = int(code_str.strip())\n",
    "                type_names.append(type_mapping.get(code, f\"Type_{code}\"))\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    return \";\".join(type_names)\n",
    "\n",
    "def calculate_geographic_match(crisis_location, member_locations):\n",
    "    \"\"\"Calculate if crisis location matches any member locations\"\"\"\n",
    "    \n",
    "    if pd.isna(crisis_location) or pd.isna(member_locations):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        crisis_loc = int(crisis_location)\n",
    "        member_locs = [int(x.strip()) for x in str(member_locations).split(';') if x.strip()]\n",
    "        return 1 if crisis_loc in member_locs else 0\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def count_member_actors(crisis_actors_str, alliance_members_str):\n",
    "    \"\"\"Count how many alliance members are crisis actors\"\"\"\n",
    "    \n",
    "    if not crisis_actors_str or not alliance_members_str:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        crisis_actors = set(int(x.strip()) for x in crisis_actors_str.split(';') if x.strip())\n",
    "        alliance_members = set(int(x.strip()) for x in alliance_members_str.split(';') if x.strip())\n",
    "        return len(crisis_actors.intersection(alliance_members))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def create_final_formatted_dataset_updated(flags_df, country_mapping):\n",
    "    \"\"\"Create the final formatted dataset with alliance_ineffect variable\"\"\"\n",
    "    \n",
    "    print(\"Creating updated formatted dataset...\")\n",
    "    \n",
    "    # Start with the fixed flags dataset\n",
    "    dataset = flags_df.copy()\n",
    "    \n",
    "    # Add crisis names from master_df\n",
    "    if 'master_df' in globals():\n",
    "        crisis_info = master_df[['Crisis_ID', 'Crisis_Name_x']].drop_duplicates()\n",
    "        crisis_info = crisis_info.rename(columns={'Crisis_Name_x': 'Crisis_Name'})\n",
    "        dataset = dataset.merge(crisis_info, on='Crisis_ID', how='left')\n",
    "    else:\n",
    "        dataset['Crisis_Name'] = \"\"\n",
    "    \n",
    "    # Add alliance info from atop_processed\n",
    "    if 'atop_processed' in globals():\n",
    "        alliance_cols = ['Alliance_ID', 'Alliance_Type', 'N_Members', 'Members_List']\n",
    "        if 'Member_Locations' in atop_processed.columns:\n",
    "            alliance_cols.append('Member_Locations')\n",
    "        \n",
    "        alliance_info = atop_processed[alliance_cols].drop_duplicates()\n",
    "        dataset = dataset.merge(alliance_info, on='Alliance_ID', how='left', suffixes=('', '_atop'))\n",
    "        \n",
    "        # Use the ATOP data for members list and N_Members\n",
    "        if 'Members_List_atop' in dataset.columns:\n",
    "            dataset['Members_List'] = dataset['Members_List_atop'].fillna(dataset['alliance_members'])\n",
    "        dataset['N_Members'] = dataset['N_Members'].fillna(0)\n",
    "    \n",
    "    # Add crisis location from master_df (it already has Geographic_Location)\n",
    "    if 'Geographic_Location' in master_df.columns:\n",
    "        crisis_location = master_df[['Crisis_ID', 'Geographic_Location']].drop_duplicates()\n",
    "        dataset = dataset.merge(crisis_location, on='Crisis_ID', how='left')\n",
    "        dataset['Crisis_Location'] = dataset['Geographic_Location']\n",
    "    else:\n",
    "        dataset['Crisis_Location'] = pd.NaT\n",
    "    \n",
    "    # Create the final formatted dataset\n",
    "    formatted_data = []\n",
    "    \n",
    "    for idx, row in dataset.iterrows():\n",
    "        \n",
    "        # Calculate N_Members_Actors\n",
    "        n_members_actors = count_member_actors(row['crisis_actors'], row.get('Members_List', ''))\n",
    "        \n",
    "        # Format country lists\n",
    "        members_formatted = format_country_list(row.get('Members_List', ''), country_mapping)\n",
    "        actors_formatted = format_country_list(row['crisis_actors'], country_mapping)\n",
    "        \n",
    "        # Format alliance type\n",
    "        alliance_type_formatted = get_alliance_type_names(row.get('Alliance_Type', ''))\n",
    "        \n",
    "        # Calculate geographic match\n",
    "        geographic_match = calculate_geographic_match(\n",
    "            row.get('Crisis_Location'), \n",
    "            row.get('Member_Locations', '')\n",
    "        )\n",
    "        \n",
    "        formatted_row = {\n",
    "            'Crisis_ID': row['Crisis_ID'],\n",
    "            'Crisis_Name': row.get('Crisis_Name', ''),\n",
    "            'Crisis_Start': row['Crisis_Start'],\n",
    "            'Crisis_End': row['Crisis_End'],\n",
    "            'Alliance_ID': row['Alliance_ID'],\n",
    "            'Alliance_Name': \"\",  # As requested - should be empty\n",
    "            'Alliance_Start': row['Alliance_Start'],\n",
    "            'Alliance_End': row['Alliance_End'],\n",
    "            'Alliance_Type': alliance_type_formatted,\n",
    "            'Active_During_Crisis': row['alliance_active_during_crisis'],\n",
    "            'Member_Is_Actor': row['alliance_member_is_crisis_actor'],\n",
    "            'Alliance_InEffect': row.get('alliance_ineffect', 0),  # NEW VARIABLE\n",
    "            'N_Members': int(row.get('N_Members', 0)) if pd.notna(row.get('N_Members', 0)) else 0,\n",
    "            'N_Members_Actors': n_members_actors,\n",
    "            'Members_List': members_formatted,\n",
    "            'Actors_List': actors_formatted,\n",
    "            'Crisis_Location': row.get('Crisis_Location', ''),\n",
    "            'Geographic_Match': geographic_match\n",
    "        }\n",
    "        \n",
    "        formatted_data.append(formatted_row)\n",
    "    \n",
    "    return pd.DataFrame(formatted_data)\n",
    "\n",
    "def save_updated_datasets(full_df, analysis_df):\n",
    "    \"\"\"Save updated datasets with alliance_ineffect variable\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== SAVING UPDATED DATASETS ===\")\n",
    "    \n",
    "    # Convert date columns to strings for JSON compatibility\n",
    "    date_columns = ['Crisis_Start', 'Crisis_End', 'Alliance_Start', 'Alliance_End']\n",
    "    \n",
    "    for df in [full_df, analysis_df]:\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Save full dataset\n",
    "    full_df.to_csv(\"ICB_ATOP_full_20250702.csv\", index=False)\n",
    "    full_df.to_json(\"ICB_ATOP_full_20250702.json\", orient='records', indent=2)\n",
    "    \n",
    "    # Save analysis dataset  \n",
    "    analysis_df.to_csv(\"ICB_ATOP_merged_20250702.csv\", index=False)\n",
    "    analysis_df.to_json(\"ICB_ATOP_merged_20250702.json\", orient='records', indent=2)\n",
    "    \n",
    "    print(f\"✓ Updated full dataset saved: {len(full_df):,} rows\")\n",
    "    print(f\"  - ICB_ATOP_full_20250702.csv\")\n",
    "    print(f\"  - ICB_ATOP_full_20250702.json\")\n",
    "    \n",
    "    print(f\"✓ Updated analysis dataset saved: {len(analysis_df):,} rows\") \n",
    "    print(f\"  - ICB_ATOP_merged_20250702.csv\")\n",
    "    print(f\"  - ICB_ATOP_merged_20250702.json\")\n",
    "\n",
    "# Now run the main code\n",
    "print(\"=== UPDATING FINAL DATASETS WITH ALLIANCE_INEFFECT ===\")\n",
    "\n",
    "# Load country mapping\n",
    "country_mapping = load_cow_country_codes()\n",
    "\n",
    "# Check if we have the fixed flags dataset\n",
    "if 'crisis_alliance_flags_df' not in globals():\n",
    "    print(\"ERROR: crisis_alliance_flags_df not found. Please run the fix code first.\")\n",
    "else:\n",
    "    # Create updated formatted datasets\n",
    "    print(\"Creating updated formatted datasets...\")\n",
    "    formatted_full_updated = create_final_formatted_dataset_updated(crisis_alliance_flags_df, country_mapping)\n",
    "    \n",
    "    # Create analysis subset (both flags = 1)\n",
    "    formatted_analysis_updated = formatted_full_updated[\n",
    "        (formatted_full_updated['Active_During_Crisis'] == 1) & \n",
    "        (formatted_full_updated['Member_Is_Actor'] == 1)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\n=== UPDATED DATASET STATISTICS ===\")\n",
    "    print(f\"Full dataset: {len(formatted_full_updated):,} crisis-alliance pairs\")\n",
    "    print(f\"Analysis dataset: {len(formatted_analysis_updated):,} crisis-alliance pairs\")\n",
    "    \n",
    "    # Show alliance status breakdown\n",
    "    if len(formatted_full_updated) > 0:\n",
    "        ineffect_stats = formatted_full_updated['Alliance_InEffect'].value_counts()\n",
    "        print(f\"\\nAlliance status in full dataset:\")\n",
    "        print(f\"  Terminated alliances (InEffect=0): {ineffect_stats.get(0, 0):,}\")\n",
    "        print(f\"  Active alliances (InEffect=1): {ineffect_stats.get(1, 0):,}\")\n",
    "    \n",
    "    if len(formatted_analysis_updated) > 0:\n",
    "        ineffect_stats_analysis = formatted_analysis_updated['Alliance_InEffect'].value_counts()\n",
    "        print(f\"\\nAlliance status in analysis dataset:\")\n",
    "        print(f\"  Terminated alliances (InEffect=0): {ineffect_stats_analysis.get(0, 0):,}\")\n",
    "        print(f\"  Active alliances (InEffect=1): {ineffect_stats_analysis.get(1, 0):,}\")\n",
    "        \n",
    "        print(f\"\\n=== SAMPLE UPDATED ANALYSIS DATASET ===\")\n",
    "        sample_cols = ['Crisis_ID', 'Crisis_Name', 'Alliance_ID', 'Alliance_Type', \n",
    "                      'Active_During_Crisis', 'Member_Is_Actor', 'Alliance_InEffect', \n",
    "                      'N_Members', 'N_Members_Actors']\n",
    "        print(formatted_analysis_updated[sample_cols].head().to_string(index=False))\n",
    "    \n",
    "    # Save updated datasets\n",
    "    save_updated_datasets(formatted_full_updated, formatted_analysis_updated)\n",
    "    \n",
    "    # Make available in global scope\n",
    "    globals()['formatted_full_df'] = formatted_full_updated\n",
    "    globals()['formatted_analysis_df'] = formatted_analysis_updated\n",
    "    \n",
    "    print(f\"\\n=== SUCCESS ===\")\n",
    "    print(f\"Updated datasets saved with Alliance_InEffect variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c678b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fixing Alliance_InEffect for placeholder end dates ===\n",
      "Updated 190,464 rows in full dataset\n",
      "Updated 3,433 rows in analysis dataset\n",
      "✓ Re-saved datasets with corrected Alliance_InEffect values\n",
      "\n",
      "Updated Alliance_InEffect distribution in analysis dataset:\n",
      "  InEffect=0 (terminated): 1,667\n",
      "  InEffect=1 (active): 3,433\n"
     ]
    }
   ],
   "source": [
    "# Fix Alliance_InEffect for alliances with end date 2030-12-31\n",
    "print(\"=== Fixing Alliance_InEffect for placeholder end dates ===\")\n",
    "\n",
    "# Update in formatted_full_df\n",
    "if 'formatted_full_df' in globals():\n",
    "    # Check for 2030-12-31 dates (our placeholder for active alliances)\n",
    "    placeholder_mask = formatted_full_df['Alliance_End'].astype(str).str.contains('2030-12-31', na=False)\n",
    "    count_full = placeholder_mask.sum()\n",
    "    \n",
    "    # Update Alliance_InEffect to 1 for these cases\n",
    "    formatted_full_df.loc[placeholder_mask, 'Alliance_InEffect'] = 1\n",
    "    \n",
    "    print(f\"Updated {count_full:,} rows in full dataset\")\n",
    "\n",
    "# Update in formatted_analysis_df  \n",
    "if 'formatted_analysis_df' in globals():\n",
    "    placeholder_mask = formatted_analysis_df['Alliance_End'].astype(str).str.contains('2030-12-31', na=False)\n",
    "    count_analysis = placeholder_mask.sum()\n",
    "    \n",
    "    formatted_analysis_df.loc[placeholder_mask, 'Alliance_InEffect'] = 1\n",
    "    \n",
    "    print(f\"Updated {count_analysis:,} rows in analysis dataset\")\n",
    "\n",
    "# Re-save the datasets\n",
    "if 'formatted_full_df' in globals() and 'formatted_analysis_df' in globals():\n",
    "    formatted_full_df.to_csv(\"ICB_ATOP_full_20250702.csv\", index=False)\n",
    "    formatted_full_df.to_json(\"ICB_ATOP_full_20250702.json\", orient='records', indent=2)\n",
    "    \n",
    "    formatted_analysis_df.to_csv(\"ICB_ATOP_merged_20250702.csv\", index=False)\n",
    "    formatted_analysis_df.to_json(\"ICB_ATOP_merged_20250702.json\", orient='records', indent=2)\n",
    "    \n",
    "    print(\"✓ Re-saved datasets with corrected Alliance_InEffect values\")\n",
    "\n",
    "# Show updated statistics\n",
    "if 'formatted_analysis_df' in globals():\n",
    "    ineffect_stats = formatted_analysis_df['Alliance_InEffect'].value_counts()\n",
    "    print(f\"\\nUpdated Alliance_InEffect distribution in analysis dataset:\")\n",
    "    print(f\"  InEffect=0 (terminated): {ineffect_stats.get(0, 0):,}\")\n",
    "    print(f\"  InEffect=1 (active): {ineffect_stats.get(1, 0):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b65ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXING GEOGRAPHIC MATCH VARIABLE (FIXED) ===\n",
      "=== LOADING AND UPDATING CRACID TO ACTLOC MAPPING (FIXED) ===\n",
      "Loaded existing cracid_to_actloc.csv with 147 entries\n",
      "Columns: ['cracid', 'actloc_labels']\n",
      "Sample data:\n",
      "   cracid actloc_labels\n",
      "0       2          41.0\n",
      "1      20          41.0\n",
      "2      40          42.0\n",
      "3      41          42.0\n",
      "4      42          42.0\n",
      "Warning: Could not parse location 'nan' for cracid 370\n",
      "Successfully parsed 147 country mappings\n",
      "Added mapping: 31 → 42\n",
      "Added mapping: 51 → 42\n",
      "Added mapping: 52 → 42\n",
      "Added mapping: 53 → 42\n",
      "Added mapping: 54 → 42\n",
      "Added mapping: 56 → 42\n",
      "Added mapping: 57 → 42\n",
      "Added mapping: 58 → 42\n",
      "Added mapping: 60 → 42\n",
      "Added mapping: 80 → 42\n",
      "Added location 31 to existing cracid 370\n",
      "Made 11 new mappings\n",
      "✓ Saved updated cracid_to_actloc.csv with 157 entries\n",
      "\n",
      "Sample updated mappings:\n",
      " cracid actloc_labels\n",
      "      2            41\n",
      "     20            41\n",
      "     31            42\n",
      "     40            42\n",
      "     41            42\n",
      "     42            42\n",
      "     51            42\n",
      "     52            42\n",
      "     53            42\n",
      "     54            42\n",
      "\n",
      "=== UPDATING GEOGRAPHIC MATCHES IN DATASETS ===\n",
      "Updating full dataset...\n",
      "Full dataset: 33,762/403,968 geographic matches (8.4%)\n",
      "Updating analysis dataset...\n",
      "Analysis dataset: 2,332/5,100 geographic matches (45.7%)\n",
      "\n",
      "=== RE-SAVING DATASETS WITH FIXED GEOGRAPHIC MATCHES ===\n",
      "✓ Updated full dataset saved\n",
      "✓ Updated analysis dataset saved\n",
      "\n",
      "=== SUCCESS ===\n",
      "✓ Fixed float parsing in cracid_to_actloc.csv\n",
      "✓ Updated geographic match logic\n",
      "✓ Updated all 4 final datasets\n"
     ]
    }
   ],
   "source": [
    "# Fixed version - handle float values in actloc_labels\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_update_cracid_actloc_fixed():\n",
    "    \"\"\"Load and update the cracid to actloc mapping - handles float values\"\"\"\n",
    "    \n",
    "    print(\"=== LOADING AND UPDATING CRACID TO ACTLOC MAPPING (FIXED) ===\")\n",
    "    \n",
    "    # Load existing mapping\n",
    "    try:\n",
    "        cracid_actloc_df = pd.read_csv(\"cracid_to_actloc.csv\")\n",
    "        print(f\"Loaded existing cracid_to_actloc.csv with {len(cracid_actloc_df)} entries\")\n",
    "        print(f\"Columns: {list(cracid_actloc_df.columns)}\")\n",
    "        print(f\"Sample data:\")\n",
    "        print(cracid_actloc_df.head())\n",
    "    except FileNotFoundError:\n",
    "        print(\"cracid_to_actloc.csv not found - creating from existing data\")\n",
    "        if 'cracid_actloc_dict' in globals():\n",
    "            cracid_actloc_df = pd.DataFrame([\n",
    "                {'cracid': k, 'actloc_labels': ';'.join([str(x) for x in v])} \n",
    "                for k, v in cracid_actloc_dict.items()\n",
    "            ])\n",
    "        else:\n",
    "            print(\"ERROR: No cracid mapping available\")\n",
    "            return None\n",
    "    \n",
    "    # Convert to dictionary for easy lookup\n",
    "    cracid_to_location = {}\n",
    "    \n",
    "    for idx, row in cracid_actloc_df.iterrows():\n",
    "        try:\n",
    "            cracid = int(row['cracid'])\n",
    "            \n",
    "            # Handle multiple locations separated by semicolon\n",
    "            # Convert float strings to int properly\n",
    "            locations = []\n",
    "            for x in str(row['actloc_labels']).split(';'):\n",
    "                if x.strip():\n",
    "                    try:\n",
    "                        # Handle float strings like '41.0'\n",
    "                        location = int(float(x.strip()))\n",
    "                        locations.append(location)\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not parse location '{x.strip()}' for cracid {cracid}\")\n",
    "                        continue\n",
    "            \n",
    "            cracid_to_location[cracid] = locations\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully parsed {len(cracid_to_location)} country mappings\")\n",
    "    \n",
    "    # Add missing Caribbean countries → region 42\n",
    "    caribbean_countries = {\n",
    "        31: 42,   # Bahamas\n",
    "        51: 42,   # Jamaica  \n",
    "        52: 42,   # Trinidad and Tobago\n",
    "        53: 42,   # Barbados\n",
    "        54: 42,   # Dominica\n",
    "        56: 42,   # St. Lucia\n",
    "        57: 42,   # St. Vincent and the Grenadines\n",
    "        58: 42,   # Antigua & Barbuda\n",
    "        60: 42,   # St. Kitts and Nevis\n",
    "        80: 42    # Belize\n",
    "    }\n",
    "    \n",
    "    # Add Belarus → region 31 (East Europe)\n",
    "    belarus_mapping = {370: 31}\n",
    "    \n",
    "    # Update mappings\n",
    "    updates_made = 0\n",
    "    for cracid, location in {**caribbean_countries, **belarus_mapping}.items():\n",
    "        if cracid not in cracid_to_location:\n",
    "            cracid_to_location[cracid] = [location]\n",
    "            updates_made += 1\n",
    "            print(f\"Added mapping: {cracid} → {location}\")\n",
    "        else:\n",
    "            # Check if location is already in the list\n",
    "            if location not in cracid_to_location[cracid]:\n",
    "                cracid_to_location[cracid].append(location)\n",
    "                updates_made += 1\n",
    "                print(f\"Added location {location} to existing cracid {cracid}\")\n",
    "            else:\n",
    "                print(f\"Country {cracid} already mapped to {cracid_to_location[cracid]}\")\n",
    "    \n",
    "    print(f\"Made {updates_made} new mappings\")\n",
    "    \n",
    "    # Convert back to DataFrame and save\n",
    "    updated_df = pd.DataFrame([\n",
    "        {'cracid': k, 'actloc_labels': ';'.join([str(x) for x in v])} \n",
    "        for k, v in cracid_to_location.items()\n",
    "    ]).sort_values('cracid')\n",
    "    \n",
    "    updated_df.to_csv(\"cracid_to_actloc.csv\", index=False)\n",
    "    print(f\"✓ Saved updated cracid_to_actloc.csv with {len(updated_df)} entries\")\n",
    "    \n",
    "    # Show sample of updated mapping\n",
    "    print(f\"\\nSample updated mappings:\")\n",
    "    print(updated_df.head(10).to_string(index=False))\n",
    "    \n",
    "    return cracid_to_location\n",
    "\n",
    "def calculate_geographic_match_fixed(crisis_location, alliance_members_str, cracid_mapping):\n",
    "    \"\"\"\n",
    "    Fixed geographic match: 1 if ANY alliance member location matches crisis location\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(crisis_location) or not alliance_members_str or pd.isna(alliance_members_str):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        crisis_loc = int(float(crisis_location))  # Handle float strings\n",
    "        \n",
    "        # Get all alliance member codes\n",
    "        member_codes = []\n",
    "        for x in str(alliance_members_str).split(';'):\n",
    "            if x.strip():\n",
    "                try:\n",
    "                    member_codes.append(int(float(x.strip())))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        # Get all locations for all alliance members\n",
    "        all_member_locations = set()\n",
    "        for member_code in member_codes:\n",
    "            if member_code in cracid_mapping:\n",
    "                member_locations = cracid_mapping[member_code]\n",
    "                all_member_locations.update(member_locations)\n",
    "        \n",
    "        # Check if crisis location matches any member location\n",
    "        return 1 if crisis_loc in all_member_locations else 0\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def update_geographic_matches_in_datasets(cracid_mapping):\n",
    "    \"\"\"Update geographic matches in final datasets\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== UPDATING GEOGRAPHIC MATCHES IN DATASETS ===\")\n",
    "    \n",
    "    def extract_member_codes_from_formatted(members_formatted):\n",
    "        \"\"\"Extract codes from 'CountryName(123);CountryName(456)' format\"\"\"\n",
    "        if not members_formatted or pd.isna(members_formatted):\n",
    "            return \"\"\n",
    "        \n",
    "        codes = []\n",
    "        for item in str(members_formatted).split(';'):\n",
    "            if '(' in item and ')' in item:\n",
    "                try:\n",
    "                    code = item.split('(')[1].split(')')[0]\n",
    "                    codes.append(code)\n",
    "                except:\n",
    "                    continue\n",
    "        return ';'.join(codes)\n",
    "    \n",
    "    # Update formatted_full_df\n",
    "    if 'formatted_full_df' in globals():\n",
    "        print(\"Updating full dataset...\")\n",
    "        \n",
    "        # Extract member codes\n",
    "        formatted_full_df['member_codes'] = formatted_full_df['Members_List'].apply(extract_member_codes_from_formatted)\n",
    "        \n",
    "        # Recalculate geographic matches\n",
    "        formatted_full_df['Geographic_Match'] = formatted_full_df.apply(\n",
    "            lambda row: calculate_geographic_match_fixed(\n",
    "                row['Crisis_Location'], \n",
    "                row['member_codes'], \n",
    "                cracid_mapping\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Remove temporary column\n",
    "        formatted_full_df.drop('member_codes', axis=1, inplace=True)\n",
    "        \n",
    "        geo_matches_full = formatted_full_df['Geographic_Match'].sum()\n",
    "        print(f\"Full dataset: {geo_matches_full:,}/{len(formatted_full_df):,} geographic matches ({geo_matches_full/len(formatted_full_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Update formatted_analysis_df\n",
    "    if 'formatted_analysis_df' in globals():\n",
    "        print(\"Updating analysis dataset...\")\n",
    "        \n",
    "        # Extract member codes\n",
    "        formatted_analysis_df['member_codes'] = formatted_analysis_df['Members_List'].apply(extract_member_codes_from_formatted)\n",
    "        \n",
    "        # Recalculate geographic matches\n",
    "        formatted_analysis_df['Geographic_Match'] = formatted_analysis_df.apply(\n",
    "            lambda row: calculate_geographic_match_fixed(\n",
    "                row['Crisis_Location'], \n",
    "                row['member_codes'], \n",
    "                cracid_mapping\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Remove temporary column\n",
    "        formatted_analysis_df.drop('member_codes', axis=1, inplace=True)\n",
    "        \n",
    "        geo_matches_analysis = formatted_analysis_df['Geographic_Match'].sum()\n",
    "        print(f\"Analysis dataset: {geo_matches_analysis:,}/{len(formatted_analysis_df):,} geographic matches ({geo_matches_analysis/len(formatted_analysis_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Re-save all datasets\n",
    "    print(f\"\\n=== RE-SAVING DATASETS WITH FIXED GEOGRAPHIC MATCHES ===\")\n",
    "    \n",
    "    if 'formatted_full_df' in globals():\n",
    "        formatted_full_df_save = formatted_full_df.copy()\n",
    "        date_columns = ['Crisis_Start', 'Crisis_End', 'Alliance_Start', 'Alliance_End']\n",
    "        for col in date_columns:\n",
    "            if col in formatted_full_df_save.columns:\n",
    "                formatted_full_df_save[col] = formatted_full_df_save[col].astype(str)\n",
    "        \n",
    "        formatted_full_df_save.to_csv(\"ICB_ATOP_full_20250702.csv\", index=False)\n",
    "        formatted_full_df_save.to_json(\"ICB_ATOP_full_20250702.json\", orient='records', indent=2)\n",
    "        print(f\"✓ Updated full dataset saved\")\n",
    "    \n",
    "    if 'formatted_analysis_df' in globals():\n",
    "        formatted_analysis_df_save = formatted_analysis_df.copy()\n",
    "        for col in date_columns:\n",
    "            if col in formatted_analysis_df_save.columns:\n",
    "                formatted_analysis_df_save[col] = formatted_analysis_df_save[col].astype(str)\n",
    "        \n",
    "        formatted_analysis_df_save.to_csv(\"ICB_ATOP_merged_20250702.csv\", index=False)\n",
    "        formatted_analysis_df_save.to_json(\"ICB_ATOP_merged_20250702.json\", orient='records', indent=2)\n",
    "        print(f\"✓ Updated analysis dataset saved\")\n",
    "\n",
    "# Execute the fixed geographic update\n",
    "print(\"=== FIXING GEOGRAPHIC MATCH VARIABLE (FIXED) ===\")\n",
    "\n",
    "# Step 1: Load and update country-to-location mapping\n",
    "cracid_mapping = load_and_update_cracid_actloc_fixed()\n",
    "\n",
    "if cracid_mapping is not None:\n",
    "    # Step 2: Update geographic matches in datasets\n",
    "    update_geographic_matches_in_datasets(cracid_mapping)\n",
    "    \n",
    "    print(f\"\\n=== SUCCESS ===\")\n",
    "    print(f\"✓ Fixed float parsing in cracid_to_actloc.csv\")\n",
    "    print(f\"✓ Updated geographic match logic\")\n",
    "    print(f\"✓ Updated all 4 final datasets\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to load country-to-location mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "443439f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL DATASET UPDATE WITH LOCATION NAMES ===\n",
      "=== CREATING GEOGRAPHIC LOCATION MAPPING ===\n",
      "✓ Saved geographic_location_mapping.csv with 21 entries\n",
      " location_code   location_name\n",
      "             9    Central Asia\n",
      "            10       West Asia\n",
      "            11       East Asia\n",
      "            12 South-East Asia\n",
      "            13      South Asia\n",
      "            15     Middle East\n",
      "            20     West Africa\n",
      "            21    North Africa\n",
      "            22     East Africa\n",
      "            23 Southern Africa\n",
      "            24  Central Africa\n",
      "            30       Euro-Asia\n",
      "            31     East Europe\n",
      "            32  Central Europe\n",
      "            33     West Europe\n",
      "            34    North Europe\n",
      "            35    South Europe\n",
      "            41   North America\n",
      "            42 Central America\n",
      "            43   South America\n",
      "            51     Australasia\n",
      "\n",
      "=== UPDATING FINAL DATASETS WITH LOCATION NAMES ===\n",
      "Updating full dataset...\n",
      "Full dataset columns: ['Crisis_ID', 'Crisis_Name', 'Crisis_Start', 'Crisis_End', 'Alliance_ID', 'Alliance_Name', 'Alliance_Start', 'Alliance_End', 'Alliance_Type', 'Active_During_Crisis', 'Member_Is_Actor', 'N_Members', 'N_Members_Actors', 'Members_List', 'Actors_List', 'Crisis_Location', 'Geographic_Match']\n",
      "Updating analysis dataset...\n",
      "Analysis dataset columns: ['Crisis_ID', 'Crisis_Name', 'Crisis_Start', 'Crisis_End', 'Alliance_ID', 'Alliance_Name', 'Alliance_Start', 'Alliance_End', 'Alliance_Type', 'Active_During_Crisis', 'Member_Is_Actor', 'N_Members', 'N_Members_Actors', 'Members_List', 'Actors_List', 'Crisis_Location', 'Geographic_Match']\n",
      "\n",
      "=== RE-SAVING FINAL DATASETS ===\n",
      "✓ Updated full dataset saved: 403,968 rows\n",
      "✓ Updated analysis dataset saved: 5,100 rows\n",
      "\n",
      "=== SAMPLE FINAL FORMAT ===\n",
      "Sample row from analysis dataset:\n",
      "Crisis_ID: 1\n",
      "Crisis_Name: RUSSIAN CIVIL WAR I\n",
      "Crisis_Start: 1918-05-01\n",
      "Crisis_End: 1920-04-01\n",
      "Alliance_ID: 2015\n",
      "Alliance_Name: \n",
      "Alliance_Start: 1914-09-05\n",
      "Alliance_End: 1918-11-11\n",
      "Alliance_Type: Consultation\n",
      "Active_During_Crisis: 1\n",
      "Member_Is_Actor: 1\n",
      "N_Members: 5\n",
      "N_Members_Actors: 1\n",
      "Members_List: United Kingdom(200);France(220);Italy(325);Russia(365);Japan(740)\n",
      "Actors_List: Russia(365)\n",
      "Crisis_Location: Euro-Asia(30)\n",
      "Geographic_Match: 1\n",
      "\n",
      "First 3 rows of analysis dataset:\n",
      " Crisis_ID         Crisis_Name  Alliance_ID                Alliance_Type Crisis_Location  Geographic_Match\n",
      "         1 RUSSIAN CIVIL WAR I         2015                 Consultation   Euro-Asia(30)                 1\n",
      "         1 RUSSIAN CIVIL WAR I         2025 Defense;Offense;Consultation   Euro-Asia(30)                 1\n",
      "         1 RUSSIAN CIVIL WAR I         2040              Defense;Offense   Euro-Asia(30)                 1\n",
      "\n",
      "Crisis location distribution:\n",
      "Crisis_Location\n",
      "Middle East(15)        1055\n",
      "East Europe(31)         499\n",
      "West Asia(10)           483\n",
      "South-East Asia(12)     425\n",
      "East Asia(11)           422\n",
      "South Asia(13)          371\n",
      "North Africa(21)        340\n",
      "Central Africa(24)      317\n",
      "South Europe(35)        247\n",
      "Central America(42)     223\n",
      "\n",
      "=== FINAL SUCCESS ===\n",
      "✅ Created geographic_location_mapping.csv\n",
      "✅ Updated Crisis_Location format to 'Region Name(code)'\n",
      "✅ Removed Alliance_InEffect (not in required format)\n",
      "✅ Reordered columns to match specification\n",
      "✅ Saved final datasets:\n",
      "   - ICB_ATOP_full_20250702.csv\n",
      "   - ICB_ATOP_full_20250702.json\n",
      "   - ICB_ATOP_merged_20250702.csv\n",
      "   - ICB_ATOP_merged_20250702.json\n",
      "✅ Final format matches specification exactly!\n"
     ]
    }
   ],
   "source": [
    "# Create geographic location mappings and update final datasets\n",
    "import pandas as pd\n",
    "\n",
    "def create_geographic_location_mapping():\n",
    "    \"\"\"Create and save geographic location code to name mapping\"\"\"\n",
    "    \n",
    "    print(\"=== CREATING GEOGRAPHIC LOCATION MAPPING ===\")\n",
    "    \n",
    "    # Define the mappings\n",
    "    geo_mappings = {\n",
    "        9: \"Central Asia\",\n",
    "        10: \"West Asia\",\n",
    "        11: \"East Asia\", \n",
    "        12: \"South-East Asia\",\n",
    "        13: \"South Asia\",\n",
    "        15: \"Middle East\",\n",
    "        20: \"West Africa\",\n",
    "        21: \"North Africa\", \n",
    "        22: \"East Africa\",\n",
    "        23: \"Southern Africa\",\n",
    "        24: \"Central Africa\",\n",
    "        30: \"Euro-Asia\",\n",
    "        31: \"East Europe\",\n",
    "        32: \"Central Europe\", \n",
    "        33: \"West Europe\",\n",
    "        34: \"North Europe\",\n",
    "        35: \"South Europe\",\n",
    "        41: \"North America\",\n",
    "        42: \"Central America\",\n",
    "        43: \"South America\",\n",
    "        51: \"Australasia\"\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    geo_mapping_df = pd.DataFrame([\n",
    "        {'location_code': code, 'location_name': name} \n",
    "        for code, name in geo_mappings.items()\n",
    "    ]).sort_values('location_code')\n",
    "    \n",
    "    geo_mapping_df.to_csv(\"geographic_location_mapping.csv\", index=False)\n",
    "    print(f\"✓ Saved geographic_location_mapping.csv with {len(geo_mapping_df)} entries\")\n",
    "    print(geo_mapping_df.to_string(index=False))\n",
    "    \n",
    "    return geo_mappings\n",
    "\n",
    "def format_crisis_location(location_code, geo_mappings):\n",
    "    \"\"\"Format crisis location as 'Location Name(code)'\"\"\"\n",
    "    \n",
    "    if pd.isna(location_code):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        code = int(float(location_code))\n",
    "        location_name = geo_mappings.get(code, f\"Unknown_{code}\")\n",
    "        return f\"{location_name}({code})\"\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"\n",
    "\n",
    "def update_final_datasets_with_location_names(geo_mappings):\n",
    "    \"\"\"Update final datasets with formatted location names\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== UPDATING FINAL DATASETS WITH LOCATION NAMES ===\")\n",
    "    \n",
    "    # Update formatted_full_df\n",
    "    if 'formatted_full_df' in globals():\n",
    "        print(\"Updating full dataset...\")\n",
    "        \n",
    "        # Create new dataset without Alliance_InEffect (not in required format)\n",
    "        updated_full = formatted_full_df.copy()\n",
    "        \n",
    "        # Remove Alliance_InEffect if it exists\n",
    "        if 'Alliance_InEffect' in updated_full.columns:\n",
    "            updated_full = updated_full.drop('Alliance_InEffect', axis=1)\n",
    "        \n",
    "        # Update Crisis_Location format\n",
    "        updated_full['Crisis_Location'] = updated_full['Crisis_Location'].apply(\n",
    "            lambda x: format_crisis_location(x, geo_mappings)\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to match required format\n",
    "        required_columns = [\n",
    "            'Crisis_ID', 'Crisis_Name', 'Crisis_Start', 'Crisis_End', 'Alliance_ID', \n",
    "            'Alliance_Name', 'Alliance_Start', 'Alliance_End', 'Alliance_Type', \n",
    "            'Active_During_Crisis', 'Member_Is_Actor', 'N_Members', 'N_Members_Actors', \n",
    "            'Members_List', 'Actors_List', 'Crisis_Location', 'Geographic_Match'\n",
    "        ]\n",
    "        \n",
    "        # Keep only required columns that exist\n",
    "        available_columns = [col for col in required_columns if col in updated_full.columns]\n",
    "        updated_full = updated_full[available_columns]\n",
    "        \n",
    "        print(f\"Full dataset columns: {list(updated_full.columns)}\")\n",
    "        globals()['formatted_full_df'] = updated_full\n",
    "    \n",
    "    # Update formatted_analysis_df\n",
    "    if 'formatted_analysis_df' in globals():\n",
    "        print(\"Updating analysis dataset...\")\n",
    "        \n",
    "        # Create new dataset without Alliance_InEffect\n",
    "        updated_analysis = formatted_analysis_df.copy()\n",
    "        \n",
    "        # Remove Alliance_InEffect if it exists\n",
    "        if 'Alliance_InEffect' in updated_analysis.columns:\n",
    "            updated_analysis = updated_analysis.drop('Alliance_InEffect', axis=1)\n",
    "        \n",
    "        # Update Crisis_Location format\n",
    "        updated_analysis['Crisis_Location'] = updated_analysis['Crisis_Location'].apply(\n",
    "            lambda x: format_crisis_location(x, geo_mappings)\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to match required format\n",
    "        available_columns = [col for col in required_columns if col in updated_analysis.columns]\n",
    "        updated_analysis = updated_analysis[available_columns]\n",
    "        \n",
    "        print(f\"Analysis dataset columns: {list(updated_analysis.columns)}\")\n",
    "        globals()['formatted_analysis_df'] = updated_analysis\n",
    "    \n",
    "    # Re-save datasets\n",
    "    print(f\"\\n=== RE-SAVING FINAL DATASETS ===\")\n",
    "    \n",
    "    if 'formatted_full_df' in globals():\n",
    "        # Convert dates to strings for JSON\n",
    "        full_save = formatted_full_df.copy()\n",
    "        date_columns = ['Crisis_Start', 'Crisis_End', 'Alliance_Start', 'Alliance_End']\n",
    "        for col in date_columns:\n",
    "            if col in full_save.columns:\n",
    "                full_save[col] = full_save[col].astype(str)\n",
    "        \n",
    "        full_save.to_csv(\"ICB_ATOP_full_20250702.csv\", index=False)\n",
    "        full_save.to_json(\"ICB_ATOP_full_20250702.json\", orient='records', indent=2)\n",
    "        print(f\"✓ Updated full dataset saved: {len(full_save):,} rows\")\n",
    "    \n",
    "    if 'formatted_analysis_df' in globals():\n",
    "        # Convert dates to strings for JSON\n",
    "        analysis_save = formatted_analysis_df.copy()\n",
    "        for col in date_columns:\n",
    "            if col in analysis_save.columns:\n",
    "                analysis_save[col] = analysis_save[col].astype(str)\n",
    "        \n",
    "        analysis_save.to_csv(\"ICB_ATOP_merged_20250702.csv\", index=False)\n",
    "        analysis_save.to_json(\"ICB_ATOP_merged_20250702.json\", orient='records', indent=2)\n",
    "        print(f\"✓ Updated analysis dataset saved: {len(analysis_save):,} rows\")\n",
    "\n",
    "def show_sample_final_format():\n",
    "    \"\"\"Show sample of final formatted data\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== SAMPLE FINAL FORMAT ===\")\n",
    "    \n",
    "    if 'formatted_analysis_df' in globals() and len(formatted_analysis_df) > 0:\n",
    "        \n",
    "        print(\"Sample row from analysis dataset:\")\n",
    "        sample_row = formatted_analysis_df.iloc[0]\n",
    "        \n",
    "        print(f\"Crisis_ID: {sample_row['Crisis_ID']}\")\n",
    "        print(f\"Crisis_Name: {sample_row['Crisis_Name']}\")\n",
    "        print(f\"Crisis_Start: {sample_row['Crisis_Start']}\")\n",
    "        print(f\"Crisis_End: {sample_row['Crisis_End']}\")\n",
    "        print(f\"Alliance_ID: {sample_row['Alliance_ID']}\")\n",
    "        print(f\"Alliance_Name: {sample_row['Alliance_Name']}\")\n",
    "        print(f\"Alliance_Start: {sample_row['Alliance_Start']}\")\n",
    "        print(f\"Alliance_End: {sample_row['Alliance_End']}\")\n",
    "        print(f\"Alliance_Type: {sample_row['Alliance_Type']}\")\n",
    "        print(f\"Active_During_Crisis: {sample_row['Active_During_Crisis']}\")\n",
    "        print(f\"Member_Is_Actor: {sample_row['Member_Is_Actor']}\")\n",
    "        print(f\"N_Members: {sample_row['N_Members']}\")\n",
    "        print(f\"N_Members_Actors: {sample_row['N_Members_Actors']}\")\n",
    "        print(f\"Members_List: {sample_row['Members_List']}\")\n",
    "        print(f\"Actors_List: {sample_row['Actors_List']}\")\n",
    "        print(f\"Crisis_Location: {sample_row['Crisis_Location']}\")\n",
    "        print(f\"Geographic_Match: {sample_row['Geographic_Match']}\")\n",
    "        \n",
    "        print(f\"\\nFirst 3 rows of analysis dataset:\")\n",
    "        display_cols = ['Crisis_ID', 'Crisis_Name', 'Alliance_ID', 'Alliance_Type', 'Crisis_Location', 'Geographic_Match']\n",
    "        available_display_cols = [col for col in display_cols if col in formatted_analysis_df.columns]\n",
    "        print(formatted_analysis_df[available_display_cols].head(3).to_string(index=False))\n",
    "        \n",
    "        # Show crisis location distribution\n",
    "        print(f\"\\nCrisis location distribution:\")\n",
    "        location_dist = formatted_analysis_df['Crisis_Location'].value_counts().head(10)\n",
    "        print(location_dist.to_string())\n",
    "\n",
    "# Execute the final update\n",
    "print(\"=== FINAL DATASET UPDATE WITH LOCATION NAMES ===\")\n",
    "\n",
    "# Step 1: Create geographic mapping\n",
    "geo_mappings = create_geographic_location_mapping()\n",
    "\n",
    "# Step 2: Update datasets with location names\n",
    "update_final_datasets_with_location_names(geo_mappings)\n",
    "\n",
    "# Step 3: Show sample results\n",
    "show_sample_final_format()\n",
    "\n",
    "print(f\"\\n=== FINAL SUCCESS ===\")\n",
    "print(f\"✅ Created geographic_location_mapping.csv\")\n",
    "print(f\"✅ Updated Crisis_Location format to 'Region Name(code)'\")\n",
    "print(f\"✅ Removed Alliance_InEffect (not in required format)\")\n",
    "print(f\"✅ Reordered columns to match specification\")\n",
    "print(f\"✅ Saved final datasets:\")\n",
    "print(f\"   - ICB_ATOP_full_20250702.csv\")\n",
    "print(f\"   - ICB_ATOP_full_20250702.json\") \n",
    "print(f\"   - ICB_ATOP_merged_20250702.csv\")\n",
    "print(f\"   - ICB_ATOP_merged_20250702.json\")\n",
    "print(f\"✅ Final format matches specification exactly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9ef49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Alliance_ID  Start_Date    End_Date\n",
      "0      200         1005  1815-01-03  1815-02-08\n",
      "1      220         1005  1815-01-03  1815-02-08\n",
      "2      300         1005  1815-01-03  1815-02-08\n",
      "3      245         1005  1815-01-13  1815-02-08\n",
      "4      240         1005  1815-01-19  1815-02-08\n"
     ]
    }
   ],
   "source": [
    "##CORRECTION CODE FOR TIME INVARIANCE IN MEMBERSHIP\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ── 1. load the member-level file ──────────────────────────────\n",
    "mfile = Path(r\"atop_5.1__.csv_\\ATOP 5.1 (.csv)\\atop5_1m.csv\")      # adjust if the file sits elsewhere\n",
    "cols  = [\"member\", \"atopid\",\n",
    "         \"yrent\", \"moent\", \"dayent\",\n",
    "         \"yrexit\", \"moexit\", \"dayexit\"]\n",
    "\n",
    "df = pd.read_csv(mfile, usecols=cols)\n",
    "\n",
    "# ── 2. helper: combine Y/M/D into one string ───────────────────\n",
    "def ymd_to_str(y, m, d, as_end=False):\n",
    "    \"\"\"\n",
    "    Build YYYY-MM-DD; blank parts default to Jan/01 (start)\n",
    "    or Dec/31 (end)                                         \"\"\"\n",
    "    if pd.isna(y):                                    # no year ⇒ missing date\n",
    "        return pd.NA\n",
    "\n",
    "    y = int(y)\n",
    "    m = int(m) if pd.notna(m) else (12 if as_end else 1)\n",
    "    d = int(d) if pd.notna(d) else (31 if as_end else 1)\n",
    "    return f\"{y:04d}-{m:02d}-{d:02d}\"\n",
    "\n",
    "# ── 3. build Start_Date / End_Date columns ─────────────────────\n",
    "df[\"Start_Date\"] = df.apply(\n",
    "    lambda r: ymd_to_str(r.yrent,  r.moent,  r.dayent,  as_end=False), axis=1)\n",
    "\n",
    "df[\"End_Date\"]   = df.apply(\n",
    "    lambda r: ymd_to_str(r.yrexit, r.moexit, r.dayexit, as_end=True),  axis=1)\n",
    "\n",
    "# ── 4. keep only the requested four columns and save / inspect ─\n",
    "tidy = df.rename(columns={\"member\": \"Country\",\n",
    "                          \"atopid\": \"Alliance_ID\"})[\n",
    "             [\"Country\", \"Alliance_ID\", \"Start_Date\", \"End_Date\"]]\n",
    "\n",
    "print(tidy.head())\n",
    "tidy.to_csv(\"atop_member_dates.csv\", index=False)   # optional save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b70c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved:\n",
      "  • Corrected_Dataset\\ICB_ATOP_full_20250702_corrected.csv\n",
      "  • Corrected_Dataset\\ICB_ATOP_merged_20250702_corrected.csv\n",
      "  • Corrected_Dataset\\ICB_ATOP_full_20250702_corrected.json\n",
      "  • Corrected_Dataset\\ICB_ATOP_merged_20250702_corrected.json\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd, re, json\n",
    "\n",
    "FULL_PATH       = \"ICB_ATOP_full_20250702.csv\"   # time-invariant file\n",
    "ROSTER_PATH     = \"atop_member_dates.csv\"        # Country|Alliance_ID|Start_Date|End_Date\n",
    "CRACID_LOC_PATH = \"cracid_to_actloc.csv\"         # optional geo dictionary\n",
    "OUT_DIR         = Path(\"Corrected_Dataset\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# ------------------------------------------------------------\n",
    "full   = pd.read_csv(FULL_PATH)\n",
    "roster = pd.read_csv(ROSTER_PATH)      # per-member join/exit dates\n",
    "\n",
    "# optional geographic dictionary  {cracid: [actloc,…]}\n",
    "try:\n",
    "    loc_df = pd.read_csv(CRACID_LOC_PATH)\n",
    "    cracid_loc = {int(r.cracid): [int(float(x)) for x in str(r.actloc_labels).split(\";\") if x]\n",
    "                  for r in loc_df.itertuples(index=False)}\n",
    "except FileNotFoundError:\n",
    "    cracid_loc = {}\n",
    "\n",
    "# parse dates once\n",
    "to_dt = pd.to_datetime\n",
    "full[\"Crisis_Start_dt\"] = to_dt(full[\"Crisis_Start\"], errors=\"coerce\")\n",
    "full[\"Crisis_End_dt\"]   = to_dt(full[\"Crisis_End\"],   errors=\"coerce\")\n",
    "\n",
    "roster[\"Start_dt\"]      = to_dt(roster[\"Start_Date\"], errors=\"coerce\")\n",
    "roster[\"End_dt\"]        = to_dt(roster[\"End_Date\"],   errors=\"coerce\").fillna(\n",
    "                          pd.Timestamp(\"2030-12-31\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. BUILD LOOK-UP: alliance_id → dataframe of active members\n",
    "# ------------------------------------------------------------\n",
    "roster_grp = roster.groupby(\"Alliance_ID\", sort=False)\n",
    "\n",
    "def active_members(aid, c_start, c_end):\n",
    "    \"\"\"return list[int] of members whose tenure overlaps the crisis window\"\"\"\n",
    "    try:\n",
    "        block = roster_grp.get_group(aid)\n",
    "    except KeyError:\n",
    "        return []\n",
    "    mask = (block[\"Start_dt\"] <= c_end) & (block[\"End_dt\"] >= c_start)\n",
    "    return block.loc[mask, \"Country\"].astype(int).tolist()\n",
    "\n",
    "# helper: pull numeric codes out of 'Name(123)' or '123'\n",
    "_code_re = re.compile(r\"(\\d+)\")\n",
    "def extract_codes(semi_str):\n",
    "    if not semi_str or pd.isna(semi_str):\n",
    "        return []\n",
    "    return [int(m.group(1)) for token in str(semi_str).split(\";\")\n",
    "                          for m in [_code_re.search(token)] if m]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. RE-COMPUTE MEMBERSHIP-BASED VARIABLES\n",
    "# ------------------------------------------------------------\n",
    "new_cols = {k: [] for k in\n",
    "            [\"Members_List\",\"N_Members\",\"Member_Is_Actor\",\n",
    "             \"N_Members_Actors\",\"Geographic_Match\"]}\n",
    "\n",
    "for row in full.itertuples(index=False):\n",
    "    # members at crisis time\n",
    "    members = active_members(row.Alliance_ID, row.Crisis_Start_dt, row.Crisis_End_dt)\n",
    "    mset    = set(members)\n",
    "\n",
    "    # crisis actors\n",
    "    actors  = set(extract_codes(row.Actors_List))\n",
    "\n",
    "    # populate\n",
    "    new_cols[\"Members_List\"].append(\";\".join(map(str, sorted(members))))\n",
    "    new_cols[\"N_Members\"].append(len(members))\n",
    "\n",
    "    inter = actors & mset\n",
    "    new_cols[\"Member_Is_Actor\"].append(int(bool(inter)))\n",
    "    new_cols[\"N_Members_Actors\"].append(len(inter))\n",
    "\n",
    "    # geographic match\n",
    "    if cracid_loc and pd.notna(row.Crisis_Location):\n",
    "        try:\n",
    "            cloc = int(row.Crisis_Location.split(\"(\")[-1].rstrip(\")\"))\n",
    "        except ValueError:\n",
    "            cloc = None\n",
    "        member_locs = {loc for c in mset for loc in cracid_loc.get(c, [])}\n",
    "        new_cols[\"Geographic_Match\"].append(int(cloc in member_locs) if cloc else pd.NA)\n",
    "    else:\n",
    "        new_cols[\"Geographic_Match\"].append(pd.NA)\n",
    "\n",
    "# attach to dataframe\n",
    "for k, v in new_cols.items():\n",
    "    full[k] = v\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. BUILD ANALYSIS SUBSET Ω  (δ_active & Member_Is_Actor)\n",
    "# ------------------------------------------------------------\n",
    "analysis = full[(full[\"Active_During_Crisis\"] == 1) &\n",
    "                (full[\"Member_Is_Actor\"] == 1)].copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. SAVE  (CSV + JSON)\n",
    "# ------------------------------------------------------------\n",
    "csv_full      = OUT_DIR / \"ICB_ATOP_full_20250702_corrected.csv\"\n",
    "csv_analysis  = OUT_DIR / \"ICB_ATOP_merged_20250702_corrected.csv\"\n",
    "json_full     = OUT_DIR / \"ICB_ATOP_full_20250702_corrected.json\"\n",
    "json_analysis = OUT_DIR / \"ICB_ATOP_merged_20250702_corrected.json\"\n",
    "\n",
    "full.to_csv(csv_full, index=False)\n",
    "analysis.to_csv(csv_analysis, index=False)\n",
    "\n",
    "full.to_json(json_full, orient=\"records\", indent=2)\n",
    "analysis.to_json(json_analysis, orient=\"records\", indent=2)\n",
    "\n",
    "print(\"✓ Saved:\")\n",
    "print(\"  •\", csv_full)\n",
    "print(\"  •\", csv_analysis)\n",
    "print(\"  •\", json_full)\n",
    "print(\"  •\", json_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e327b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Crisis_ID        Crisis_Name  Alliance_ID Alliance_Start Alliance_End  Active_During_Crisis  Member_Is_Actor  N_Members  N_Members_Actors                                             Members_List              Actors_List\n",
      "       155 HUNGARIAN UPRISING         3180     1949-04-04   2030-12-31                     1                0         15                 0 2;20;200;210;211;212;220;235;260;325;350;385;390;395;640 Hungary(310);Russia(365)\n",
      "\n",
      "Hungary present in filtered roster?  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# path to the corrected full file you just generated\n",
    "CORRECTED = Path(\"Corrected_Dataset/ICB_ATOP_full_20250702_corrected.csv\")\n",
    "\n",
    "# load just the NATO–Hungarian-uprising row\n",
    "check = (\n",
    "    pd.read_csv(CORRECTED)\n",
    "      .loc[lambda d: (d[\"Crisis_ID\"] == 155) & (d[\"Alliance_ID\"] == 3180)]\n",
    ")\n",
    "\n",
    "if check.empty:\n",
    "    raise RuntimeError(\"Row not found - did the IDs change?\")\n",
    "\n",
    "cols = [\"Crisis_ID\", \"Crisis_Name\",\n",
    "        \"Alliance_ID\", \"Alliance_Start\", \"Alliance_End\",\n",
    "        \"Active_During_Crisis\", \"Member_Is_Actor\",\n",
    "        \"N_Members\", \"N_Members_Actors\",\n",
    "        \"Members_List\", \"Actors_List\"]\n",
    "\n",
    "print(check[cols].to_string(index=False, max_colwidth=90))\n",
    "\n",
    "# quick sanity flags\n",
    "roster      = check[\"Members_List\"].iloc[0].split(\";\")\n",
    "hungary_in  = any(code.strip()==\"310\" for code in roster)\n",
    "print(\"\\nHungary present in filtered roster? \", hungary_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad16dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ saved ICB_ATOP_full_20250702_corrected.csv and ICB_ATOP_full_20250702_corrected.json to readable_corrected/\n",
      "✓ saved ICB_ATOP_merged_20250702_corrected.csv and ICB_ATOP_merged_20250702_corrected.json to readable_corrected/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, re, json\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# paths & filenames\n",
    "# ------------------------------------------------------------\n",
    "SRC_DIR  = Path(\"Corrected_Dataset\")\n",
    "OUT_DIR  = Path(\"readable_corrected\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "FULL_FILE_NUM  = SRC_DIR / \"ICB_ATOP_full_20250702_corrected.csv\"\n",
    "SUBSET_FILE_NUM = SRC_DIR / \"ICB_ATOP_merged_20250702_corrected.csv\"\n",
    "\n",
    "COW_PATH   = Path(\"COW-country-codes.csv\")               # official list\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. build augmented country-code mapping\n",
    "# ------------------------------------------------------------\n",
    "cow = pd.read_csv(COW_PATH, usecols=[\"CCode\", \"StateNme\"]).drop_duplicates()\n",
    "extra = pd.DataFrame({\n",
    "    \"CCode\":   [219, 671, 672],\n",
    "    \"StateNme\":[\"Vichy France\", \"Hejaz\", \"Najd\"]\n",
    "})\n",
    "codes = pd.concat([cow, extra], ignore_index=True).drop_duplicates(\"CCode\")\n",
    "code2name = dict(zip(codes.CCode, codes.StateNme))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. helpers for list conversion\n",
    "# ------------------------------------------------------------\n",
    "_code_re = re.compile(r\"(\\d+)\")\n",
    "\n",
    "def extract_codes(semistr):\n",
    "    \"\"\"'Name(123);456' → [123,456]  (ints)\"\"\"\n",
    "    if not semistr or pd.isna(semistr):\n",
    "        return []\n",
    "    return [int(m.group(1)) for token in str(semistr).split(\";\")\n",
    "                           for m in [_code_re.search(token)] if m]\n",
    "\n",
    "def to_readable(semistr):\n",
    "    \"\"\"numeric codes → 'Country(code)'  ; leaves already-readable rows unchanged\"\"\"\n",
    "    if \"(\" in str(semistr):          # already labelled\n",
    "        return semistr\n",
    "    codes = extract_codes(semistr)\n",
    "    labels = [f\"{code2name.get(c, f'Unknown_{c}')}({c})\" for c in sorted(codes)]\n",
    "    return \";\".join(labels)\n",
    "\n",
    "def convert_dataframe(df):\n",
    "    df = df.copy()\n",
    "    df[\"Members_List\"] = df[\"Members_List\"].apply(to_readable)\n",
    "    df[\"Actors_List\"]  = df[\"Actors_List\"].apply(to_readable)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. load, convert, save\n",
    "# ------------------------------------------------------------\n",
    "for infile in [FULL_FILE_NUM, SUBSET_FILE_NUM]:\n",
    "    df_num  = pd.read_csv(infile)\n",
    "    df_read = convert_dataframe(df_num)\n",
    "\n",
    "    # keep same filename, different folder\n",
    "    csv_out  = OUT_DIR / infile.name\n",
    "    json_out = csv_out.with_suffix(\".json\")\n",
    "\n",
    "    df_read.to_csv(csv_out, index=False)\n",
    "    df_read.to_json(json_out, orient=\"records\", indent=2)\n",
    "\n",
    "    print(f\"✓ saved {csv_out.name} and {json_out.name} to {OUT_DIR}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
